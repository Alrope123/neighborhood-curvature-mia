{"---\nabstract: 'The purpose of this article is to study the problem of finding sharp lower bounds for the norm of the product of polynomials in the ultraproducts of Banach spaces $(X_i)_{\\mathfrak U}$. We show that, under certain hypotheses, there is a strong relation between this problem and the same problem for the spaces $X_i$.'\naddress: 'IMAS-CONICET'\nauthor:\n- Jorge Tom\u00e1s Rodr\u00edguez\ntitle: On the norm of products of polynomials on ultraproducts of Banach spaces\n---\n\nIntroduction\n============\n\nIn this article we study the factor problem in the context of ultraproducts of Banach spaces. This problem can be stated as follows: for a Banach space $X$ over a field ${\\mathbb K}$ (with ${\\mathbb K}={\\mathbb R}$ or ${\\mathbb K}={\\mathbb C}$) and natural numbers $k_1,\\cdots, k_n$ find the optimal constant $M$ such that, given any set of continuous scalar polynomials $P_1,\\cdots,P_n:X\\rightarrow {\\mathbb K}$, of degrees $k_1,\\cdots,k_n$; the inequality $$\\label{problema}\nM \\Vert P_1 \\cdots P_n\\Vert \\ge  \\, \\Vert P_1 \\Vert \\cdots \\Vert P_n \\Vert$$ holds, where $\\Vert P \\Vert = \\sup_{\\Vert x \\Vert_X=1} \\vert P(x)\\vert$. We also study a variant of the problem in which we require the polynomials to be homogeneous.\n\nRecall that a function $P:X\\rightarrow {\\mathbb K}$ is a continuous $k-$homogeneous polynomial if there is a continuous $k-$linear function $T:X^k\\rightarrow {\\mathbb K}$ for which $P(x)=T(x,\\cdots,x)$. A function $Q:X\\rightarrow {\\mathbb K}$ is a continuous polynomial of degree $k$ if $Q=\\sum_{l=0}^k Q_l$ with $Q_0$ a constant, $Q_l$ ($1\\leq l \\leq k$) an $l-$homogeneous polynomial and $Q_k \\neq 0$ .\n\nThe factor problem has been studied by several authors. In [@BST], C. Ben\u00edtez, Y. Sarantopoulos and A. Tonge proved that, for continuous polynomials, inequality (\\[problema\\]) holds with constant $$M=\\frac{(k_1+\\cdots + k_n)^{(k_1+\\cdots +k_n)}}{k_1^{k_1} \\cdots k_n^{k_n}}$$ for any complex Banach space. The authors also showed that this is the best universal constant, since there are polynomials on $\\ell_1$ for which equality prevails. For complex Hilbert spaces and homogeneous polynomials, D. Pinasco proved in [@P] that the optimal constant is $$\\nonumber\nM=\\sqrt{\\frac{(k_1+\\cdots + k_n)^{(k_1+\\cdots +k_n)}}{k_1^{k_1} \\cdots k_n^{k_n}}}.$$ This is a generalization of the result for linear functions obtained by Arias-de-Reyna in [@A]. In [@CPR], also for homogeneous polynomials, D. Carando, D. Pinasco and the author proved that for any complex $L_p(\\mu)$ space, with $dim(L_p(\\mu))\\geq n$ and $1<p<2$, the optimal constant is $$\\nonumber\nM=\\sqrt[p]{\\frac{(k_1+\\cdots + k_n)^{(k_1+\\cdots +k_n)}}{k_1^{k_1} \\cdots k_n^{k_n}}}.$$\n\nThis article is partially motivated by the work of M. Lindstr\u00f6m and R. A. Ryan in [@LR]. In that article they studied, among other things, a problem similar to (\\[problema\\]): finding the so called polarization constant of a Banach space. They found a relation between the polarization constant of the ultraproduct $(X_i)_{\\mathfrak U}$ and the polarization constant of each of the spaces $X_i$. Our objective is to do an analogous analysis for our problem (\\[problema\\]). That is, to find a relation between the factor problem for the space $(X_i)_{\\mathfrak U}$ and the factor problem for the spaces $X_i$.\n\nIn Section 2 we give some basic definitions and results of ultraproducts needed for our discussion. In Section 3 we state and prove the main result of this paper, involving ultraproducts, and a similar result on biduals.\n\nUltraproducts\n=============\n\nWe begin with some definitions, notations and basic results on filters, ultrafilters and ultraproducts. Most of the content presented in this section, as well as an exhaustive exposition on ultraproducts, can be found in Heinrich\u2019s article [@H].\n\nA filter ${\\mathfrak U}$ on a family $I$ is a collection of non empty subsets of $I$ closed by finite intersections and inclusions. An ultrafilter is maximal filter.\n\nIn order to define the ultraproduct of Banach spaces, we are going to need some topological results first.\n\nLet ${\\mathfrak U}$ be an ultrafilter on $I$ and $X$ a topological space. We say that the limit of $(x_i)_{i\\in I} \\subseteq X$ respect of ${\\mathfrak U}$ is $x$ if for every open neighborhood $U$ of $x$ the set $\\{i\\in I: x_i \\in U\\}$ is an element of ${\\mathfrak U}$. We denote $$\\displaystyle\\lim_{i,{\\mathfrak U}} x_i = x.$$\n\nThe following is Proposition 1.5 from [@H].\n\n\\[buenadef\\] Let ${\\mathfrak U}$ be an ultrafilter on $I$, $X$ a compact Hausdorff space and $(x_i)_{i\\in I} \\subseteq X$. Then, the limit of $(x_i)_{i\\in I}$ respect of ${\\mathfrak U}$ exists and is unique.\n\nLater on, we are going to need the next basic Lemma about limits of ultraproducts, whose proof is an easy exercise of basic topology and ultrafilters.\n\n\\[lemlimit\\] Let ${\\mathfrak U}$ be an ultrafilter on $I$ and $\\{x_i\\}_{i\\in I}$ a family of real numbers. Assume that the limit of $(x_i)_{i\\in I} \\subseteq {\\mathbb R}$ respect of ${\\mathfrak U}$ exists and let $r$ be a real number such that there is a subset $U$ of $\\{i: r<x_i\\}$ with $U\\in {\\mathfrak U}$. Then $$r \\leq \\displaystyle\\lim_{i,{\\mathfrak U}} x_i.$$\n\nWe are now able to define the ultraproduct of Banach spaces. Given an ultrafilter ${\\mathfrak U}$ on $I$ and a family of Banach spaces $(X_i)_{i\\in I}$, take the Banach space $\\ell_\\infty(I,X_i)$ of norm bounded families $(x_i)_{i\\in I}$ with $x_i \\in X_i$ and norm $$\\Vert (x_i)_{i\\in I} \\Vert = \\sup_{i\\in I} \\Vert x_i \\Vert.$$ The ultraproduct $(X_i)_{\\mathfrak U}$ is defined as the quotient space $\\ell_\\infty(I,X_i)/ \\sim $ where $$(x_i)_{i\\in I}\\sim (y_i)_{i\\in I} \\Leftrightarrow \\displaystyle\\lim_{i,{\\mathfrak U}} \\Vert x_i - y_i \\Vert = 0.$$\n\nObserve that Proposition \\[buenadef\\] assures us that this limit exists for every pair $(x_i)_{i\\in I}, (y_i)_{i\\in I}\\in \\ell_\\infty(I,X_i)$. We denote the class of $(x_i)_{i\\in I}$ in $(X_i)_{\\mathfrak U}$ by $(x_i)_{\\mathfrak U}$.\n\nThe following result is the polynomial version of Definition 2.2 from [@H] (see also Proposition 2.3 from [@LR]). The reasoning behind is almost the same.\n\n\\[pollim\\] Given two ultraproducts $(X_i)_{\\mathfrak U}$, $(Y_i)_{\\mathfrak U}$ and a family of continuous homogeneous polynomials $\\{P_i\\}_{i\\in I}$ of degree $k$ with $$\\displaystyle\\sup_{i\\in I} \\Vert P_i \\Vert < \\infty,$$ the map $P:(X_i)_{\\mathfrak U}\\longrightarrow (Y_i)_{\\mathfrak U}$ defined by $P((x_i)_{\\mathfrak U})=(P_i(x_i))_{\\mathfrak U}$ is a continuous homogeneous polynomial of degree $k$. Moreover $\\Vert P \\Vert = \\displaystyle\\lim_{i,{\\mathfrak U}} \\Vert P_i \\Vert$.\n\nIf ${\\mathbb K}={\\mathbb C}$, the hypothesis of homogeneity can be omitted, but in this case the degree of $P$ can be lower than $k$.\n\nLet us start with the homogeneous case. Write $P_i(x)=T_i(x,\\cdots,x)$ with $T_i$ a $k-$linear continuous function. Define $T:(X_i)_{\\mathfrak U}^k \\longrightarrow (Y_i)_{\\mathfrak U}$ by $$T((x^1_i)_{\\mathfrak U},\\cdots,(x^k_i)_{\\mathfrak U})=(T_i(x^1_i,\\cdots ,x^k_i))_{\\mathfrak U}.$$ $T$ is well defined since, by the polarization formula, $ \\displaystyle\\sup_{i\\in I} \\Vert T_i \\Vert \\leq   \\displaystyle\\sup_{i\\in I} \\frac{k^k}{k!}\\Vert P_i \\Vert< \\infty$.\n\nSeeing that for each coordinate the maps $T_i$ are linear, the map $T$ is linear in each coordinate, and thus it is a $k-$linear function. Given that $$P((x_i)_{\\mathfrak U})=(P_i(x_i))_{\\mathfrak U}=(T_i(x_i,\\cdots,x_i))_{\\mathfrak U}=T((x_i)_{\\mathfrak U},\\cdots,(x_i)_{\\mathfrak U})$$ we conclude that $P$ is a $k-$homogeneous polynomial.\n\nTo see the equality of the norms for every $i$ choose a norm $1$ element $x_i\\in X_i$ where $P_i$ almost attains its norm, and from there is easy to deduce that $\\Vert P \\Vert \\geq \\displaystyle\\lim_{i,{\\mathfrak U}} \\Vert P_i \\Vert$. For the other inequality we use that $$|P((x_i)_{\\mathfrak U})|= \\displaystyle\\lim_{i,{\\mathfrak U}}|P_i(x_i)| \\leq \\displaystyle\\lim_{i,{\\mathfrak U}}\\Vert P_i \\Vert \\Vert x_i \\Vert^k = \\left(\\displaystyle\\lim_{i,{\\mathfrak U}}\\Vert P_i \\Vert \\right)\\Vert (x_i)_{\\mathfrak U}\\Vert^k .$$\n\nNow we treat the non homogeneous case. For each $i\\in I$ we write $P_i=\\sum_{l=0}^kP_{i,l}$, with $P_{i,0}$ a constant and $P_{i,l}$ ($1\\leq l \\leq k$) an $l-$homogeneous polynomial. Take the direct sum $X_i \\oplus_\\infty {\\mathbb C}$ of $X_i$ and ${\\mathbb C}$, endowed with the norm $\\Vert (x,\\lambda) \\Vert =\\max \\{ \\Vert x \\Vert, | \\lambda| \\}$. Consider the polynomial $\\tilde{P_i}:X_i \\oplus_\\infty {\\mathbb C}\\rightarrow Y_i$ defined by $\\tilde{P}_i(x,\\lambda)=\\sum_{l=0}^k P_{i,l}(x)\\lambda^{k-l}$. The polynomial $\\tilde{P}_i$ is an homogeneous polynomial of degree $k$ and, using the maximum modulus principle, it is easy to see that $\\Vert P_i \\Vert = \\Vert \\tilde{P_i} \\Vert $. Then, by the homogeneous case, we have that the polynomial $\\tilde{P}:(X_i \\oplus_\\infty {\\mathbb C})_{\\mathfrak U}\\rightarrow (Y_i)_{\\mathfrak U}$ defined as $\\tilde{P}((x_i,\\lambda_i)_{\\mathfrak U})=(\\tilde{P}_i(x_i,\\lambda_i))_{\\mathfrak U}$ is a continuous homogeneous polynomial of degree $k$ and $\\Vert \\tilde{P} \\Vert =\\displaystyle\\lim_{i,{\\mathfrak U}} \\Vert \\tilde{P}_i \\Vert =\\displaystyle\\lim_{i,{\\mathfrak U}} \\Vert P_i \\Vert$.\n\nVia the identification $(X_i \\oplus_\\infty {\\mathbb C})_{\\mathfrak U}=(X_i)_{\\mathfrak U}\\oplus_\\infty {\\mathbb C}$ given by $(x_i,\\lambda_i)_{\\mathfrak U}=((x_i)_{\\mathfrak U},\\displaystyle\\lim_{i,{\\mathfrak U}} \\lambda_i)$ we have that the polynomial $Q:(X_i)_{\\mathfrak U}\\oplus_\\infty {\\mathbb C}\\rightarrow {\\mathbb C}$ defined as $Q((x_i)_{\\mathfrak U},\\lambda)=\\tilde{P}((x_i,\\lambda)_{\\mathfrak U})$ is a continuous homogeneous polynomial of degree $k$ and $\\Vert Q\\Vert =\\Vert \\tilde{P}\\Vert$. Then, the polynomial $P((x_i)_{\\mathfrak U})=Q((x_i)_{\\mathfrak U},1)$ is a continuous polynomial of degree at most $k$ and $\\Vert P\\Vert =\\Vert Q\\Vert =\\displaystyle\\lim_{i,{\\mathfrak U}} \\Vert P_i \\Vert$. If $\\displaystyle\\lim_{i,{\\mathfrak U}} \\Vert P_{i,k} \\Vert =0 $ then the degree of $P$ is lower than $k$.\n\nNote that, in the last proof, we can take the same approach used for non homogeneous polynomials in the real case, but we would not have the same control over the norms.\n\n Main result \n=============\n\nThis section contains our main result. As mentioned above, this result is partially motivated by Theorem 3.2 from [@LR]. We follow similar ideas for the proof. First, let us fix some notation that will be used throughout this section.\n\nIn this section, all polynomials considered are continuous scalar polynomials. Given a Banach space $X$, $B_X$ and $S_X$ denote the unit ball and the unit sphere of $X$ respectively, and $X^*$ is the dual of $X$. Given a polynomial $P$ on $X$, $deg(P)$ stands for the degree of $P$.\n\nFor a Banach space $X$ let $D(X,k_1,\\cdots,k_n)$ denote the smallest constant that satisfies (\\[problema\\]) for polynomials of degree $k_1,\\cdots,k_n$. We also define $C(X,k_1,\\cdots,k_n)$ as the smallest constant that satisfies (\\[problema\\]) for homogeneous polynomials of degree $k_1,\\cdots,k_n$.\n\nThroughout this section most of the results will have two parts. The first involving the constant $C(X,k_1,\\cdots,k_n)$ for homogeneous polynomials and the second involving the constant $D(X,k_1,\\cdots,k_n)$ for arbitrary polynomials. Given that the proof of both parts are almost equal, we will limit to prove only the second part of the results.\n\nRecall that a space $X$ has the $1 +$ uniform approximation property if for all $n\\in {\\mathbb N}$, exists $m=m(n)$ such that for every subspace $M\\subset X$ with $dim(M)=n$ and every $\\varepsilon > 0$ there is an operator $T\\in \\mathcal{L}(X,X)$ with $T|_M=id$, $rg(T)\\leq m$ and $\\Vert T\\Vert  \\leq 1 + \\varepsilon$ (i.e. for every $\\varepsilon > 0$ $X$ has the $1+\\varepsilon$ uniform approximation property).\n\n\\[main thm\\] If ${\\mathfrak U}$ is an ultrafilter on a family $I$ and $(X_i)_{\\mathfrak U}$ is an ultraproduct of complex Banach spaces then\n\n1.  $C((X_i)_{\\mathfrak U},k_1,\\cdots,k_n) \\geq \\displaystyle\\lim_{i,{\\mathfrak U}}(C(X_i,k_1,\\cdots,k_n)).$\n\n2.  $D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n) \\geq \\displaystyle\\lim_{i,{\\mathfrak U}}(D(X_i,k_1,\\cdots,k_n)).$\n\nMoreover, if each $X_i$ has the $1+$ uniform approximation property, equality holds in both cases.\n\nIn order to prove this Theorem some auxiliary lemmas are going to be needed. The first one is due to Heinrich [@H].\n\n\\[aprox\\] Given an ultraproduct of Banach spaces $(X_i)_{\\mathfrak U}$, if each $X_i$ has the $1+$ uniform approximation property then $(X_i)_{\\mathfrak U}$ has the metric approximation property.\n\nWhen working with the constants $C(X,k_1,\\cdots,k_n)$ and $D(X,k_1,\\cdots,k_n)$, the following characterization may result handy.\n\n\\[alternat\\] a) The constant $C(X,k_1,\\cdots,k_n)$ is the biggest constant $M$ such that given any $\\varepsilon >0$ there exist a set of homogeneous continuous polynomials $\\{P_j\\}_{j=1}^n$ with $deg(P_j)\\leq k_j$ such that\n\n$$\\label{condition} M\\left \\Vert \\prod_{j=1}^{n} P_j \\right \\Vert \\leq  (1+\\varepsilon) \\prod_{j=1}^{n} \\Vert P_j \\Vert.$$\n\nb\\) The constant $D(X,k_1,\\cdots,k_n)$ is the biggest constant satisfying the same for arbitrary polynomials.\n\nTo prove this Lemma it is enough to see that $D(X,k_1,\\cdots,k_n)$ is decreasing as a function of the degrees $k_1,\\cdots, k_n$ and use that the infimum is the greatest lower bound.\n\n\\[rmkalternat\\] It is clear that in Lemma \\[alternat\\] we can take the polynomials $\\{P_j\\}_{j=1}^n$ with $deg(P_j)= k_j$ instead of $deg(P_j)\\leq k_j$. Later on we will use both versions of the Lemma.\n\nOne last lemma is needed for the proof of the Main Theorem.\n\n\\[normas\\] Let $P$ be a (not necessarily homogeneous) polynomial on a complex Banach space $X$ with $deg(P)=k$. For any point $x\\in X$ $$|P(x)|\\leq  \\max\\{\\Vert x \\Vert, 1\\}^k \\Vert P\\Vert  . \\nonumber$$\n\nIf $P$ is homogeneous the result is rather obvious since we have the inequality $$|P(x)|\\leq \\Vert x \\Vert^k \\Vert P\\Vert . \\nonumber$$ Suppose that $P=\\sum_{l=0}^k P_l$ with $P_l$ an $l-$homogeneous polynomial. Consider the space $X \\oplus_\\infty {\\mathbb C}$ and the polynomial $\\tilde{P}:X \\oplus_\\infty {\\mathbb C}\\rightarrow {\\mathbb C}$ defined by $\\tilde{P}(x,\\lambda)=\\sum_{l=0}^k P_l(x)\\lambda^{k-l}$. The polynomial $\\tilde{P}$ is homogeneous of degree $k$ and $\\Vert P \\Vert = \\Vert \\tilde{P} \\Vert $. Then, using that $\\tilde{P}$ is homogeneous we have $$|P(x)|=|\\tilde{P} (x,1)| \\leq \\Vert (x,1) \\Vert^k \\Vert \\tilde{P} \\Vert = \\max\\{\\Vert x \\Vert, 1\\}^k \\Vert P\\Vert . \\nonumber$$\n\nWe are now able to prove our main result.\n\nThroughout this proof we regard the space $({\\mathbb C})_{\\mathfrak U}$ as ${\\mathbb C}$ via the identification $(\\lambda_i)_{\\mathfrak U}=\\displaystyle\\lim_{i,{\\mathfrak U}} \\lambda_i$.\n\nFirst, we are going to see that $D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n) \\geq \\displaystyle\\lim_{i,{\\mathfrak U}}(D(X_i,k_1,\\cdots,k_n))$. To do this we only need to prove that $\\displaystyle\\lim_{i,{\\mathfrak U}}(D(X_i,k_1,\\cdots,k_n))$ satisfies (\\[condition\\]). Given $\\varepsilon >0$ we need to find a set of polynomials $\\{P_{j}\\}_{j=1}^n$ on $(X_i)_{\\mathfrak U}$ with $deg(P_{j})\\leq k_j$ such that $$\\displaystyle\\lim_{i,{\\mathfrak U}}(D(X_i,k_1,\\cdots,k_n)) \\left \\Vert \\prod_{j=1}^{n} P_j \\right \\Vert \\leq (1+\\varepsilon) \\prod_{j=1}^{n} \\left \\Vert P_j \\right \\Vert .$$\n\nBy Remark \\[rmkalternat\\] we know that for each $i\\in I$ there is a set of polynomials $\\{P_{i,j}\\}_{j=1}^n$ on $X_i$ with $deg(P_{i,j})=k_j$ such that $$D(X_i,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} P_{i,j} \\right \\Vert \\leq (1 +\\varepsilon)\\prod_{j=1}^{n} \\left \\Vert P_{i,j} \\right \\Vert.$$ Replacing $P_{i,j}$ with $P_{i,j}/\\Vert P_{i,j} \\Vert$ we may assume that $\\Vert P_{i,j} \\Vert =1$. Define the polynomials $\\{P_j\\}_{j=1}^n$ on $(X_i)_{\\mathfrak U}$ by $P_j((x_i)_{\\mathfrak U})=(P_{i,j}(x_i))_{\\mathfrak U}$. Then, by Proposition \\[pollim\\], $deg(P_j)\\leq k_j$ and $$\\begin{aligned}\n \\displaystyle\\lim_{i,{\\mathfrak U}}(D(X_i,k_1,\\cdots,k_n)) \\left \\Vert \\prod_{j=1}^{n} P_{j} \\right \\Vert &=& \\displaystyle\\lim_{i,{\\mathfrak U}} \\left(D(X_i,k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} P_{i,j} \\right \\Vert \\right)  \\nonumber \\\\\n&\\leq& \\displaystyle\\lim_{i,{\\mathfrak U}}\\left((1+\\varepsilon)\\prod_{j=1}^{n}\\Vert  P_{i,j}  \\Vert \\right)\\nonumber \\\\\n&=& (1+\\varepsilon)\\prod_{j=1}^{n} \\Vert P_{j}  \\Vert \\nonumber \n \\nonumber \\end{aligned}$$ as desired.\n\nTo prove that $D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n) \\leq \\displaystyle\\lim_{i,{\\mathfrak U}}(D(X_i,k_1,\\cdots,k_n))$ if each $X_i$ has the $1+$ uniform approximation property is not as straightforward. Given $\\varepsilon >0$, let $\\{P_j\\}_{j=1}^n$ be a set of polynomials on $(X_i)_{\\mathfrak U}$ with $deg(P_j)=k_j$ such that $$D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} P_j \\right \\Vert \\leq  (1+\\varepsilon)\\prod_{j=1}^{n} \\Vert P_j \\Vert .$$\n\nLet $K\\subseteq B_{(X_i)_{\\mathfrak U}}$ be the finite set $K=\\{x_1,\\cdots, x_n\\}$ where $ x_j$ is such that $$|P_j(x_j)| > \\Vert P_j\\Vert (1- \\varepsilon) \\mbox{ for }j=1,\\cdots, n.$$ Being that each $X_i$ has the $1+$ uniform approximation property, then, by Lemma \\[aprox\\], $(X_i)_{\\mathfrak U}$ has the metric approximation property. Therefore, exist a finite rank operator $S:(X_i)_{\\mathfrak U}\\rightarrow (X_i)_{\\mathfrak U}$ such that $\\Vert S\\Vert \\leq 1 $ and $$\\Vert P_j - P_j \\circ S \\Vert_K< |P_j(x_j)|\\varepsilon \\mbox{ for }j=1,\\cdots, n.$$\n\nNow, define the polynomials $Q_1,\\cdots, Q_n$ on $(X_i)_{\\mathfrak U}$ as $Q_j=P_j\\circ S$. Then $$\\left\\Vert \\prod_{j=1}^n Q_j \\right\\Vert \\leq \\left\\Vert \\prod_{j=1}^n P_j \\right\\Vert$$ $$\\Vert Q_j\\Vert_K > | P_j(x_j)|-\\varepsilon | P_j(x_j)| =| P_j(x_j)| (1-\\varepsilon) \\geq \\Vert P_j \\Vert(1-\\varepsilon)^2.$$ The construction of this polynomials is a slight variation of Lemma 3.1 from [@LR]. We have the next inequality for the product of the polynomials $\\{Q_j\\}_{j=1}^n$ $$\\begin{aligned}\n D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} Q_{j} \\right \\Vert &\\leq& D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} P_{j} \\right \\Vert \\nonumber \\\\\n&\\leq&  (1+\\varepsilon) \\prod_{j=1}^{n}  \\left \\Vert P_{j} \\right \\Vert . \\label{desq}\\end{aligned}$$\n\nSince $S$ is a finite rank operator, the polynomials $\\{ Q_j\\}_{j=1}^n$ have the advantage that are finite type polynomials. This will allow us to construct polynomials on $(X_i)_{\\mathfrak U}$ which are limit of polynomials on the spaces $X_i$. For each $j$ write $Q_j=\\sum_{t=1}^{m_j}(\\psi_{j,t})^{r_{j,t}}$ with $\\psi_{j,t}\\in (X_i)_{\\mathfrak U}^*$, and consider the spaces $N=\\rm{span}  \\{x_1,\\cdots,x_n\\}\\subset (X_i)_{\\mathfrak U}$ and $M=\\rm{span} \\{\\psi_{j,t} \\}\\subset (X_i)_{\\mathfrak U}^*$. By the local duality of ultraproducts (see Theorem 7.3 from [@H]) exist $T:M\\rightarrow (X_i^*)_{\\mathfrak U}$ an $(1+\\varepsilon)-$isomorphism such that $$JT(\\psi)(x)=\\psi(x) \\mbox{ } \\forall x\\in N, \\mbox{ } \\forall \\psi\\in M$$ where $J:(X_i^*)_{\\mathfrak U}\\rightarrow (X_i)_{\\mathfrak U}^*$ is the canonical embedding. Let $\\phi_{j,t}=JT(\\psi_{j,t})$ and consider the polynomials $\\bar{Q}_1,\\cdots, \\bar{Q}_n$ on $(X_i)_{\\mathfrak U}$ with $\\bar{Q}_j=\\sum_{t=1}^{m_j}(\\phi_{j,t})^{r_{j,t}}$. Clearly $\\bar{Q}_j$ is equal to $Q_j$ in $N$ and $K\\subseteq N$, therefore we have the following lower bound for the norm of each polynomial $$\\Vert \\bar{Q}_j \\Vert \\geq \\Vert \\bar{Q}_j \\Vert_K = \\Vert Q_j \\Vert_K >\\Vert P_j \\Vert(1-\\varepsilon)^2 \\label{desbarq}$$\n\nNow, let us find an upper bound for the norm of the product $\\Vert \\prod_{j=1}^n \\bar{Q}_j \\Vert$. Let $x=(x_i)_{\\mathfrak U}$ be any point in $B_{(X_i)_{\\mathfrak U}}$. Then, we have $$\\begin{aligned}\n \\left|\\prod_{j=1}^n \\bar{Q}_j(x)\\right| &=& \\left|\\prod_{j=1}^n \\sum_{t=1}^{m_j}(\\phi_{j,t} (x))^{r_{j,t}}\\right|=\\left|\\prod_{j=1}^n \\sum_{t=1}^{m_j} (JT\\psi_{j,t}(x))^{r_{j,t}} \\right| \\nonumber \\\\\n&=& \\left|\\prod_{j=1}^n \\sum_{t=1}^{m_j}((JT)^*\\hat{x}(\\psi_{j,t}))^{r_{j,t}}\\right|\\nonumber\\end{aligned}$$\n\nSince $(JT)^*\\hat{x}\\in M^*$, $\\Vert (JT)^*\\hat{x}\\Vert =\\Vert JT \\Vert \\Vert x \\Vert \\leq \\Vert J \\Vert \\Vert T \\Vert \\Vert x \\Vert< 1 + \\varepsilon$ and $M^*=\\frac{(X_i)_{\\mathfrak U}^{**}}{M^{\\bot}}$, we can chose $z^{**}\\in (X_i)_{\\mathfrak U}^{**}$ with $\\Vert z^{**} \\Vert < \\Vert (JT)^*\\hat{x}\\Vert+\\varepsilon < 1+2\\varepsilon$, such that $\\prod_{j=1}^n \\sum_{t=1}^{m_j} ((JT)^*\\hat{x}(\\psi_{j,t}))^{r_{j,t}}= \\prod_{j=1}^n \\sum_{t=1}^{m_j} (z^{**}(\\psi_{j,t}))^{r_{j,t}}$. By Goldstine\u2019s Theorem exist a net $\\{z_\\alpha\\} \\subseteq (X_i)_{\\mathfrak U}$ $w^*-$convergent to $z$ in $(X_i)_{\\mathfrak U}^{**}$ with $\\Vert z_\\alpha \\Vert = \\Vert z^{**}\\Vert$. In particular, $ \\psi_{j,t}(z_\\alpha)$ converges to $z^{**}(\\psi_{j,t})$. If we call ${\\mathbf k}= \\sum k_j$, since $\\Vert z_\\alpha \\Vert< (1+2\\varepsilon)$, by Lemma \\[normas\\], we have $$\\left \\Vert \\prod_{j=1}^{n} Q_j \\right \\Vert (1+2\\varepsilon)^{\\mathbf k}\\geq \\left|\\prod_{j=1}^n Q_j(z_\\alpha)\\right| = \\left|\\prod_{j=1}^n \\sum_{t=1}^{m_j} ((\\psi_{j,t})(z_\\alpha))^{r_{j,t}}\\right| .      \\label{usecomplex}$$ Combining this with the fact that $$\\begin{aligned}\n \\left|\\prod_{j=1}^{n} \\sum_{t=1}^{m_j} ((\\psi_{j,t})(z_\\alpha))^{r_{j,t}}\\right| &\\longrightarrow&  \\left|\\prod_{j=1}^{n} \\sum_{t=1}^{m_j} (z^{**}(\\psi_{j,t}))^{r_{j,t}}\\right|\\nonumber\\\\\n &=& \\left|\\prod_{j=1}^{n} \\sum_{t=1}^{m_j} ((JT)^*\\hat{x}(\\psi_{j,t}))^{r_{j,t}}\\right| = \\left|\\prod_{j=1}^{n} \\bar{Q}_j(x)\\right|\\nonumber\\end{aligned}$$ we conclude that $\\left \\Vert \\prod_{j=1}^{n} Q_j \\right \\Vert (1+2\\varepsilon)^{\\mathbf k}\\geq |\\prod_{j=1}^{n} \\bar{Q}_j(x)|$.\n\nSince the choice of $x$ was arbitrary we arrive to the next inequality $$\\begin{aligned}\n D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_j \\right \\Vert &\\leq& (1+2\\varepsilon)^{\\mathbf k}D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n)   \\left \\Vert \\prod_{j=1}^{n} Q_j \\right \\Vert   \\nonumber \\\\\n&\\leq&  (1+2\\varepsilon)^{\\mathbf k}(1+\\varepsilon) \\prod_{j=1}^{n}  \\left \\Vert P_{j} \\right \\Vert \\label{desbarq2} \\\\\n&<& (1+2\\varepsilon)^{\\mathbf k}(1+\\varepsilon) \\frac{\\prod_{j=1}^{n} \\Vert \\bar{Q}_j \\Vert }{(1-\\varepsilon)^{2n}} .\\label{desbarq3}  \\\\end{aligned}$$ In (\\[desbarq2\\]) and (\\[desbarq3\\]) we use (\\[desq\\]) and (\\[desbarq\\]) respectively. The polynomials $\\bar{Q}_j$ are not only of finite type, these polynomials are also generated by elements of $(X_i^*)_{\\mathfrak U}$. This will allow us to write them as limits of polynomials in $X_i$. For any $i$, consider the polynomials $\\bar{Q}_{i,1},\\cdots,\\bar{Q}_{i,n}$ on $X_i$ defined by $\\bar{Q}_{i,j}= \\displaystyle\\sum_{t=1}^{m_j} (\\phi_{i,j,t})^{r_{j,t}}$, where the functionals $\\phi_{i,j,t}\\in X_i^*$ are such that $(\\phi_{i,j,t})_{\\mathfrak U}=\\phi_{j,t}$. Then $\\bar{Q}_j(x)=\\displaystyle\\lim_{i,{\\mathfrak U}} \\bar{Q}_{i,j}(x)$ $\\forall x \\in (X_i)_{\\mathfrak U}$ and, by Proposition \\[pollim\\], $\\Vert \\bar{Q}_j \\Vert = \\displaystyle\\lim_{i,{\\mathfrak U}} \\Vert \\bar{Q}_{i,j} \\Vert$. Therefore $$\\begin{aligned}\nD((X_i)_{\\mathfrak U},k_1,\\cdots,k_n) \\displaystyle\\lim_{i,{\\mathfrak U}} \\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_{i,j} \\right \\Vert &=& D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_{j} \\right \\Vert \\nonumber \\\\\n&<& \\frac{(1+\\varepsilon)(1+2\\varepsilon)^{\\mathbf k}}{(1-\\varepsilon)^{2n}} \\prod_{j=1}^{n} \\Vert \\bar{Q}_{j}  \\Vert \\nonumber \\\\\n&=& \\frac{(1+\\varepsilon)(1+2\\varepsilon)^{\\mathbf k}}{(1-\\varepsilon)^{2n}} \\prod_{j=1}^{n} \\displaystyle\\lim_{i,{\\mathfrak U}} \\Vert \\bar{Q}_{i,j}  \\Vert .\n \\nonumber \\end{aligned}$$\n\nTo simplify the notation let us call $\\lambda = \\frac{(1+\\varepsilon)(1+2\\varepsilon)^{\\mathbf k}}{(1-\\varepsilon)^{2n}} $. Take $L>0$ such that\n\n$$D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n) \\displaystyle\\lim_{i,{\\mathfrak U}} \\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_{i,j} \\right \\Vert < L < \\lambda \\prod_{j=1}^{n} \\displaystyle\\lim_{i,{\\mathfrak U}} \\Vert \\bar{Q}_{i,j}  \\Vert . \\nonumber$$\n\nSince $(-\\infty, \\frac{L}{D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n)})$ and $(\\frac{L}{\\lambda},+\\infty)$ are neighborhoods of $\\displaystyle\\lim_{i,{\\mathfrak U}} \\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_{i,j} \\right \\Vert$ and $\\prod_{j=1}^{n} \\displaystyle\\lim_{i,{\\mathfrak U}} \\Vert \\bar{Q}_{i,j}  \\Vert$ respectively, and $\\prod_{j=1}^{n} \\displaystyle\\lim_{i,{\\mathfrak U}} \\Vert \\bar{Q}_{i,j}  \\Vert= \\displaystyle\\lim_{i,{\\mathfrak U}} \\prod_{j=1}^{n} \\Vert \\bar{Q}_{i,j}  \\Vert$, by definition of $\\displaystyle\\lim_{i,{\\mathfrak U}}$, the sets $$A=\\{i_0: D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_{i_0,j} \\right \\Vert <L\\} \\mbox{ and }B=\\{i_0: \\lambda \\prod_{j=1}^{n}  \\Vert \\bar{Q}_{i_0,j}  \\Vert > L \\}$$ are elements of ${\\mathfrak U}$. Since ${\\mathfrak U}$ is closed by finite intersections $A\\cap B\\in {\\mathfrak U}$. If we take any element $i_0 \\in A\\cap B$ then, for any $\\delta >0$, we have that $$D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_{i_0,j} \\right \\Vert \\frac{1}{\\lambda}\\leq \\frac{L}{\\lambda} \\leq \\prod_{j=1}^{n}  \\Vert \\bar{Q}_{i_0,j}  \\Vert < (1+ \\delta)\\prod_{j=  1}^{n}  \\Vert \\bar{Q}_{i_0,j} \\Vert \\nonumber$$ Then, since $\\delta$ is arbitrary, the constant $D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n)\\frac{1}{\\lambda}$ satisfy (\\[condition\\]) for the space $X_{i_0}$ and therefore, by Lemma \\[alternat\\], $$\\frac{1}{\\lambda}D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n) \\leq  D(X_{i_0},k_1,\\cdots,k_n).  \\nonumber$$\n\nThis holds true for any $i_0$ in $A\\cap B$. Since $A\\cap B \\in {\\mathfrak U}$, by Lemma \\[lemlimit\\], $\\frac{1}{\\lambda}D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n)\\leq \\displaystyle\\lim_{i,{\\mathfrak U}}  D(X_i,k_1,\\cdots,k_n) $. Using that $\\lambda \\rightarrow 1$ when $\\varepsilon \\rightarrow 0$ we conclude that $D((X_i)_{\\mathfrak U},k_1,\\cdots,k_n)\\leq \\displaystyle\\lim_{i,{\\mathfrak U}}  D(X_i,k_1,\\cdots,k_n).$\n\nSimilar to Corollary 3.3 from [@LR], a straightforward corollary of our main result is that for any complex Banach space $X$ with $1+$ uniform approximation property $C(X,k_1,\\cdots,k_n)=C(X^{**},k_1,\\cdots,k_n)$ and $D(X,k_1,\\cdots,k_n)=D(X^{**},k_1,\\cdots,k_n)$ . Using that $X^{**}$ is $1-$complemented in some adequate ultrafilter $(X)_{{\\mathfrak U}}$ the result is rather obvious. For a construction of the adequate ultrafilter see [@LR].\n\nBut following the previous proof, and using the principle of local reflexivity applied to $X^*$ instead of the local duality of ultraproducts, we can prove the next stronger result.\n\nLet $X$ be a complex Banach space. Then\n\n1.  $C(X^{**},k_1,\\cdots,k_n)\\geq C(X,k_1,\\cdots,k_n).$\n\n2.  $D(X^{**},k_1,\\cdots,k_n \\geq D(X,k_1,\\cdots,k_n)).$\n\nMoreover, if $X^{**}$ has the metric approximation property, equality holds in both cases.\n\nThe inequality $D(X^{**},k_1,\\cdots,k_n) \\geq D(X,k_1,\\cdots,k_n)$ is a corollary of Theorem \\[main thm\\] (using the adequate ultrafilter mentioned above).\n\nLet us prove that if $X^{**}$ has the metric approximation property then $D((X^{**},k_1,\\cdots,k_n)\\geq D(X,k_1,\\cdots,k_n)$. Given $\\varepsilon >0$, let $\\{P_j\\}_{j=1}^n$ be a set of polynomials on $X^{**}$ with $deg(P_j)=k_j$ such that $$D(X^{**},k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} P_{j} \\right \\Vert \\leq (1+\\varepsilon)\\prod_{j=1}^{n} \\left \\Vert P_{j} \\right \\Vert .\\nonumber$$\n\nAnalogous to the proof of Theorem \\[main thm\\], since $X^{**}$ has the metric approximation, we can construct finite type polynomials $Q_1,\\cdots,Q_n$ on $X^{**}$ with $deg(Q_j)=k_j$, $\\Vert Q_j \\Vert_K \\geq \\Vert P_j \\Vert (1-\\varepsilon)^2$ for some finite set $K\\subseteq B_{X^{**}}$ and that $$D(X^{**},k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} Q_{j} \\right \\Vert < (1+\\varepsilon)\\prod_{j=1}^{n} \\left \\Vert P_{j} \\right  \\Vert . \\nonumber$$\n\nSuppose that $Q_j=\\sum_{t=1}^{m_j}(\\psi_{j,t})^{r_{j,t}}$ and consider the spaces $N=\\rm{span} \\{K\\}$ and $M=\\rm{span} \\{\\psi_{j,t} \\}$. By the principle of local reflexivity (see [@D]), applied to $X^*$ (thinking $N$ as a subspaces of $(X^*)^*$ and $M$ as a subspaces of $(X^*)^{**}$), there is an $(1+\\varepsilon)-$isomorphism $T:M\\rightarrow X^*$ such that $$JT(\\psi)(x)=\\psi(x) \\mbox{ } \\forall x\\in N, \\mbox{ } \\forall \\psi\\in M\\cap X^*=M,$$ where $J:X^*\\rightarrow X^{***}$ is the canonical embedding.\n\nLet $\\phi_{j,t}=JT(\\psi_{j,t})$ and consider the polynomials $\\bar{Q}_1,\\cdots, \\bar{Q}_n$ on $X^{**}$ defined by $\\bar{Q}_j=\\sum_{t=1}^{m_j}(\\phi_{j,t})^{r_{j,t}}$. Following the proof of the Main Theorem, one arrives to the inequation $$D(X^{**},k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} \\bar{Q_j} \\right \\Vert < (1+ \\delta) \\frac{(1+\\varepsilon)(1+2\\varepsilon)^{\\mathbf k}}{(1-\\varepsilon)^{2n}} \\prod_{j=1}^{n} \\Vert \\bar{Q_j}  \\Vert \\nonumber$$ for every $\\delta >0$. Since each $\\bar{Q}_j$ is generated by elements of $J(X^*)$, by Goldstine\u2019s Theorem, the restriction of $\\bar{Q}_j$ to $X$ has the same norm and the same is true for $\\prod_{j=1}^{n} \\bar{Q_j}$. Then $$D(X^{**},k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} \\left.\\bar{Q_j}\\right|_X \\right \\Vert < (1+ \\delta) \\frac{(1+\\varepsilon)(1+2\\varepsilon)^{\\mathbf k}}{(1-\\varepsilon)^{2n}} \\prod_{j=1}^{n} \\Vert \\left.\\bar{Q_j}\\right|_X  \\Vert \\nonumber$$ By Lemma \\[alternat\\] we conclude that $$\\frac{(1-\\varepsilon)^{2n}}{(1+\\varepsilon)(1+2\\varepsilon)^{\\mathbf k}}D(X^{**},k_1,\\cdots,k_n)\\leq D(X,k_1,\\cdots,k_n).$$ Given that the choice of $\\varepsilon$ is arbitrary and that $\\frac{(1-\\varepsilon)^{2n}}{(1+\\varepsilon)(1+2\\varepsilon)^{\\mathbf k}} $ tends to $1$ when $\\varepsilon$ tends to $0$ we conclude that $D(X^{**},k_1,\\cdots,k_n)\\leq D(X,k_1,\\cdots,k_n)$.\n\nNote that in the proof of the Main Theorem the only parts where we need the spaces to be complex Banach spaces are at the beginning, where we use Proposition \\[pollim\\], and in the inequality (\\[usecomplex\\]), where we use Lemma \\[normas\\]. But both results holds true for homogeneous polynomials on a real Banach space. Then, copying the proof of the Main Theorem we obtain the following result for real spaces.\n\nIf ${\\mathfrak U}$ is an ultrafilter on a family $I$ and $(X_i)_{\\mathfrak U}$ is an ultraproduct of real Banach spaces then $$C((X_i)_{\\mathfrak U},k_1,\\cdots,k_n) \\geq \\displaystyle\\lim_{i,{\\mathfrak U}}(C(X_i,k_1,\\cdots,k_n)).$$\n\nIf in addition each $X_i$ has the $1+$ uniform approximation property, the equality holds.\n\nAlso we can get a similar result for the bidual of a real space.\n\nLet $X$ be a real Banach space. Then\n\n1.  $C(X^{**},k_1,\\cdots,k_n)\\geq C(X,k_1,\\cdots,k_n).$\n\n2.  $D(X^{**},k_1,\\cdots,k_n) \\geq D(X,k_1,\\cdots,k_n).$\n\nIf $X^{**}$ has the metric approximation property, equality holds in $(a)$.\n\nThe proof of item $(a)$ is the same that in the complex case, so we limit to prove $D(X^{**},k_1,\\cdots,k_n) \\geq D(X,k_1,\\cdots,k_n))$. To do this we will show that given an arbitrary $\\varepsilon >0$, there is a set of polynomials $\\{P_{j}\\}_{j=1}^n$ on $X^{**}$ with $deg(P_{j})\\leq k_j$ such that $$D(X,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} P_j \\right \\Vert \\leq (1+\\varepsilon) \\prod_{j=1}^{n} \\left \\Vert P_j \\right \\Vert .$$\n\nTake $\\{Q_{j}\\}_{j=1}^n$ a set of polynomials on $X$ with $deg(Q_j)=k_j$ such that $$D(X,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} Q_{j} \\right \\Vert \\leq (1 +\\varepsilon)\\prod_{j=1}^{n} \\left \\Vert Q_{j} \\right \\Vert.$$\n\nConsider now the polynomials $P_j=AB(Q_j)$, where $AB(Q_j)$ is the Aron Berner extension of $Q_j$ (for details on this extension see [@AB] or [@Z]). Since $AB\\left( \\prod_{j=1}^n P_j \\right)=\\prod_{j=1}^n AB(P_j)$, using that the Aror Berner extension preserves norm (see [@DG]) we have\n\n$$\\begin{aligned}\n D(X,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} P_{j} \\right \\Vert &=& D(X,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} Q_{j}  \\right \\Vert\\nonumber \\\\\n&\\leq& (1 +\\varepsilon)\\prod_{j=1}^{n} \\left\\Vert Q_{j} \\right\\Vert \\nonumber \\\\\n&=& (1 +\\varepsilon)\\prod_{j=1}^{n} \\left \\Vert P_{j} \\right  \\Vert \\nonumber \\end{aligned}$$\n\nas desired.\n\nAs a final remark, we mention two types of spaces for which the results on this section can be applied.\n\nCorollary 9.2 from [@H] states that any Orlicz space $L_\\Phi(\\mu)$, with $\\mu$ a finite measure and $\\Phi$ an Orlicz function with regular variation at $\\infty$, has the $1+$ uniform projection property, which is stronger than the $1+$ uniform approximation property.\n\nIn [@PeR] Section two, A. Pe\u0142czy\u0144ski and H. Rosenthal proved that any ${\\mathcal L}_{p,\\lambda}-$space ($1\\leq \\lambda < \\infty$) has the $1+\\varepsilon-$uniform projection property for every $\\varepsilon>0$ (which is stronger than the $1+\\varepsilon-$uniform approximation property), therefore, any ${\\mathcal L}_{p,\\lambda}-$space has the $1+$ uniform approximation property.\n\nAcknowledgment {#acknowledgment .unnumbered}\n==============\n\nI would like to thank Professor Daniel Carando for both encouraging me to write this article, and for his comments and remarks which improved its presentation and content.\n\n[HD]{}\n\nR. M. J. Arias-de-Reyna. *Gaussian variables, polynomials and permanents*. Linear Algebra Appl. 285 (1998), 107\u2013114.\n\nR. M. Aron and P. D. Berner. *A Hahn-Banach extension theorem for analytic mapping*. Bull. Soc. Math. France 106 (1978), 3\u201324.\n\nC. Ben\u00edtez, Y. Sarantopoulos and A. Tonge. *Lower bounds for norms of products of polynomials*. Math. Proc. Cambridge Philos. Soc. 124 (1998), 395\u2013408.\n\nD. Carando, D. Pinasco y J.T. Rodr\u00edguez. *Lower bounds for norms of products of polynomials on $L_p$ spaces*. Studia Math. 214 (2013), 157\u2013166.\n\nA. M. Davie and T. W. Gamelin. *A theorem on polynomial-star approximation*. Proc. Amer. Math. Soc. 106 (1989) 351\u2013356.\n\nD. W. Dean. *The equation $L(E,X^{**})=L(E,X)^{**}$ and the principle of local reflexivity*. Proceedings of the American Mathematical Society. 40 (1973), 146-148.\n\nS. Heinrich. *Ultraproducts in Banach space theory*. J. Reine Angew. Math. 313 (1980), 72\u2013104.\n\nM. Lindstr\u00f6m and R. A. Ryan. *Applications of ultraproducts to infinite dimensional holomorphy*. Math. Scand. 71 (1992), 229\u2013242.\n\nA. Pe\u0142czy\u0144ski and H. Rosenthal. *Localization techniques in $L_p$ spaces*. Studia Math. 52 (1975), 265\u2013289.\n\nD. Pinasco. *Lower bounds for norms of products of polynomials via Bombieri inequality*. Trans. Amer. Math. Soc. 364 (2012), 3993\u20134010.\n\nI. Zalduendo. *Extending polynomials on Banach Spaces - A survey*. Rev. Un. Mat. Argentina 46 (2005), 45\u201372.\n": [["\\section{Introduction}\n\nIn this article we study the factor problem in the context of ultraproducts of Banach spaces. This problem can be stated as follows: for a Banach space $X$ over a field $\\mathbb K$ (with $\\mathbb K=\\mathbb R$ or $\\mathbb K=\\mathbb C$) and natural numbers $k_1,\\cdots, k_n$ find the optimal constant $M$ such that, given any set of continuous scalar polynomials $P_1,\\cdots,P_n:X\\rightarrow \\mathbb K$, of degrees $k_1,\\cdots,k_n$; the inequality\n\\begin{equation}\\label{problema}\nM \\Vert P_1 \\cdots P_n\\Vert \\ge  \\, \\Vert P_1 \\Vert \\cdots \\Vert P_n \\Vert\n\\end{equation}\nholds, where $\\Vert P \\Vert = \\sup_{\\Vert x \\Vert_X=1} \\vert P(x)\\vert$. We also study a variant of the problem in which we require the polynomials to be homogeneous.\n\nRecall that a function $P:X\\rightarrow \\mathbb K$ is a continuous $k-$homogeneous polynomial if there is a continuous $k-$linear function $T:X^k\\rightarrow \\mathbb K$ for which $P(x)=T(x,\\cdots,x)$. A function $Q:X\\rightarrow \\mathbb K$ is a continuous polynomial of degree $k$ if $Q=\\sum_{l=0}^k Q_l$ with $Q_0$ a constant, $Q_l$ ($1\\leq l \\leq k$) an $l-$homogeneous polynomial and $Q_k \\neq 0$ . \n\nThe factor problem has been studied by several authors. In \\cite{BST}, C. Ben\\'{i}tez, Y. Sarantopoulos and A. Tonge proved that, for continuous polynomials, inequality (\\ref{problema}) holds with constant\n\\[\nM=\\frac{(k_1+\\cdots + k_n)^{(k_1+\\cdots +k_n)}}{k_1^{k_1} \\cdots k_n^{k_n}}\n\\]\nfor any complex Banach space. The authors also showed that this is the best universal constant, since there are polynomials on $\\ell_1$ for which equality prevails. \nFor complex Hilbert spaces and homogeneous polynomials, D. Pinasco proved in \\cite{P} that the optimal constant is\n\\begin{equation}\\nonumber\nM=\\sqrt{\\frac{(k_1+\\cdots + k_n)^{(k_1+\\cdots +k_n)}}{k_1^{k_1} \\cdots k_n^{k_n}}}.\n\\end{equation}\nThis is a generalization of the result for linear functions obtained by Arias-de-Reyna in \\cite{A}. In \\cite{CPR}, also for homogeneous polynomials, D. Carando, D. Pinasco and the author proved that for any complex $L_p(\\mu)$ space, with $dim(L_p(\\mu))\\geq n$ and $1<p<2$, the optimal constant is\n\\begin{equation}\\nonumber\nM=\\sqrt[p]{\\frac{(k_1+\\cdots + k_n)^{(k_1+\\cdots +k_n)}}{k_1^{k_1} \\cdots k_n^{k_n}}}.\n\\end{equation}\n\nThis article is partially motivated by the work of M. Lindstr\\\"{o}m and R. A. Ryan in \\cite{LR}. In that article they studied, among other things, a problem similar to (\\ref{problema}): finding the so called polarization constant of a Banach space. They found a relation between the polarization constant of the ultraproduct $(X_i)_\\mathfrak U$ and the polarization constant of each of the spaces $X_i$. Our objective is to do an analogous analysis for our problem (\\ref{problema}). That is, to find a relation between the factor problem for the space $(X_i)_\\mathfrak U$  and the factor problem for the spaces $X_i$.\n\n\nIn Section 2 we give some basic definitions and results of ultraproducts needed for our discussion. In Section 3 we state and prove the main result of this paper, involving ultraproducts, and a similar result on biduals. \n\n\n\\section{Ultraproducts}\n\nWe begin with some definitions, notations and basic results on filters, ultrafilters and ultraproducts. Most of the content presented in this section, as well as an exhaustive exposition on ultraproducts, can be found in Heinrich's article \\cite{H}.\n\n\nA filter $\\mathfrak U$ on a family $I$ is a collection of non empty subsets of $I$ closed by finite intersections and inclusions. An ultrafilter is maximal filter.\n\nIn order to define the ultraproduct of Banach spaces, we are going to need some topological results first. \n\n\\begin{defin} Let $\\mathfrak U$ be an ultrafilter on $I$ and $X$ a topological space. We say that the limit of $(x_i)_{i\\in I} \\subseteq X$ respect of $\\mathfrak U$ is $x$ if for every open neighborhood $U$ of $x$ the set $\\{i\\in I: x_i \\in U\\}$ is an element of $\\mathfrak U$. We denote\n$$ \\displaystyle\\lim_{i,\\mathfrak U} x_i = x.$$\n\n\\end{defin}\n\nThe following is Proposition 1.5 from \\cite{H}.\n\n\\begin{prop}\\label{buenadef} Let $\\mathfrak U$ be an ultrafilter on $I$, $X$ a compact Hausdorff space and $(x_i)_{i\\in I} \\subseteq X$. Then, the limit of $(x_i)_{i\\in I}$ respect of $\\mathfrak U$ exists and is unique.\n\\end{prop} \n\nLater on, we are going to need the next basic Lemma about limits of ultraproducts, whose proof is an easy exercise of basic topology and ultrafilters.\n\n\\begin{lem}\\label{lemlimit} Let $\\mathfrak U$ be an ultrafilter on $I$ and $\\{x_i\\}_{i\\in I}$ a family of real numbers. Assume that the limit of $(x_i)_{i\\in I} \\subseteq \\mathbb R$ respect of $\\mathfrak U$ exists and let $r$ be a real number such that there is a subset $U$ of $\\{i: r<x_i\\}$ with $U\\in \\mathfrak U$. Then\n$$ r \\leq \\displaystyle\\lim_{i,\\mathfrak U} x_i. $$\n\n\\end{lem}\n\n\nWe are now able to define the ultraproduct of Banach spaces. Given an ultrafilter $\\mathfrak U$ on $I$ and a family of Banach spaces $(X_i)_{i\\in I}$, take the Banach space $\\ell_\\infty(I,X_i)$ of norm bounded families $(x_i)_{i\\in I}$ with $x_i \\in X_i$ and norm\n$$\\Vert (x_i)_{i\\in I} \\Vert = \\sup_{i\\in I} \\Vert x_i \\Vert.$$ The ultraproduct $(X_i)_\\mathfrak U$ is defined as the quotient space $\\ell_\\infty(I,X_i)/ \\sim $ where\n$$ (x_i)_{i\\in I}\\sim (y_i)_{i\\in I} \\Leftrightarrow \\displaystyle\\lim_{i,\\mathfrak U} \\Vert x_i - y_i \\Vert = 0.$$\n\nObserve that Proposition \\ref{buenadef} assures us that this limit exists for every pair $(x_i)_{i\\in I}, (y_i)_{i\\in I}\\in \\ell_\\infty(I,X_i)$. We denote the class of $(x_i)_{i\\in I}$ in $(X_i)_\\mathfrak U$ by $(x_i)_\\mathfrak U$.\n\nThe following result is the polynomial version of Definition 2.2 from \\cite{H} (see also Proposition 2.3 from \\cite{LR}). The reasoning behind is almost the same.\n\n\\begin{prop}\\label{pollim} Given two ultraproducts $(X_i)_\\mathfrak U$, $(Y_i)_\\mathfrak U$ and a family of continuous homogeneous polynomials $\\{P_i\\}_{i\\in I}$ of degree $k$ with \n$$ \\displaystyle\\sup_{i\\in I} \\Vert P_i \\Vert < \\infty,$$\nthe map $P:(X_i)_\\mathfrak U \\longrightarrow (Y_i)_\\mathfrak U$ defined by $P((x_i)_\\mathfrak U)=(P_i(x_i))_\\mathfrak U$ is a continuous homogeneous polynomial of degree $k$. Moreover $\\Vert P \\Vert = \\displaystyle\\lim_{i,\\mathfrak U} \\Vert P_i \\Vert$.\n\nIf $\\mathbb K=\\mathbb C$, the hypothesis of homogeneity can be omitted, but in this case the degree of $P$ can be lower than $k$.\n\\end{prop}\n\n\\begin{proof} Let us start with the homogeneous case. Write $P_i(x)=T_i(x,\\cdots,x)$ with $T_i$ a $k-$linear continuous function. Define $T:(X_i)_\\mathfrak U^k \\longrightarrow (Y_i)_\\mathfrak U$ by \n$$T((x^1_i)_\\mathfrak U,\\cdots,(x^k_i)_\\mathfrak U)=(T_i(x^1_i,\\cdots ,x^k_i))_\\mathfrak U.$$\n$T$ is well defined since, by the polarization formula, $ \\displaystyle\\sup_{i\\in I} \\Vert T_i \\Vert \\leq   \\displaystyle\\sup_{i\\in I} \\frac{k^k}{k!}\\Vert P_i \\Vert< \\infty$.\n\nSeeing that for each coordinate the maps $T_i$ are linear, the map $T$ is linear in each coordinate, and thus it is a $k-$linear function. Given that \n$$P((x_i)_\\mathfrak U)=(P_i(x_i))_\\mathfrak U=(T_i(x_i,\\cdots,x_i))_\\mathfrak U=T((x_i)_\\mathfrak U,\\cdots,(x_i)_\\mathfrak U)$$\nwe conclude that $P$ is a $k-$homogeneous polynomial.\n\nTo see the equality of the norms for every $i$ choose a norm $1$ element $x_i\\in X_i$ where $P_i$ almost attains its norm, and from there  is easy to deduce that $\\Vert P \\Vert \\geq \\displaystyle\\lim_{i,\\mathfrak U} \\Vert P_i \\Vert$. For the other inequality we use that $$ |P((x_i)_\\mathfrak U)|= \\displaystyle\\lim_{i,\\mathfrak U}|P_i(x_i)| \\leq \\displaystyle\\lim_{i,\\mathfrak U}\\Vert P_i \\Vert \\Vert x_i \\Vert^k = \\left(\\displaystyle\\lim_{i,\\mathfrak U}\\Vert P_i \\Vert \\right)\\Vert (x_i)_\\mathfrak U \\Vert^k .$$\n\nNow we treat the non homogeneous case. For each $i\\in I$ we write $P_i=\\sum_{l=0}^kP_{i,l}$, with $P_{i,0}$ a constant and $P_{i,l}$ ($1\\leq l \\leq k$) an $l-$homogeneous polynomial. Take the direct sum $X_i \\oplus_\\infty \\mathbb C$ of $X_i$ and $\\mathbb C$, endowed with the norm $\\Vert (x,\\lambda) \\Vert =\\max \\{ \\Vert x \\Vert, | \\lambda| \\}$. Consider the polynomial $\\tilde{P_i}:X_i \\oplus_\\infty \\mathbb C\\rightarrow Y_i$ defined by $\\tilde{P}_i(x,\\lambda)=\\sum_{l=0}^k P_{i,l}(x)\\lambda^{k-l}$. The polynomial $\\tilde{P}_i$ is an homogeneous polynomial of degree $k$ and, using the maximum modulus principle, it is easy to see that $\\Vert P_i \\Vert = \\Vert \\tilde{P_i} \\Vert $. Then, by the homogeneous case, we have that the polynomial $\\tilde{P}:(X_i \\oplus_\\infty \\mathbb C)_\\mathfrak U \\rightarrow (Y_i)_\\mathfrak U$ defined as $\\tilde{P}((x_i,\\lambda_i)_\\mathfrak U)=(\\tilde{P}_i(x_i,\\lambda_i))_\\mathfrak U$ is a continuous homogeneous polynomial of degree $k$  and $\\Vert \\tilde{P} \\Vert =\\displaystyle\\lim_{i,\\mathfrak U} \\Vert \\tilde{P}_i \\Vert =\\displaystyle\\lim_{i,\\mathfrak U} \\Vert P_i \\Vert$.\n\nVia the identification $(X_i \\oplus_\\infty \\mathbb C)_\\mathfrak U=(X_i)_\\mathfrak U \\oplus_\\infty \\mathbb C$ given by $(x_i,\\lambda_i)_\\mathfrak U=((x_i)_\\mathfrak U,\\displaystyle\\lim_{i,\\mathfrak U} \\lambda_i)$ we have that the polynomial $Q:(X_i)_\\mathfrak U \\oplus_\\infty \\mathbb C\\rightarrow \\mathbb C$ defined as $Q((x_i)_\\mathfrak U,\\lambda)=\\tilde{P}((x_i,\\lambda)_\\mathfrak U)$ is a continuous homogeneous polynomial of degree $k$ and $\\Vert Q\\Vert =\\Vert \\tilde{P}\\Vert$. Then, the polynomial $P((x_i)_\\mathfrak U)=Q((x_i)_\\mathfrak U,1)$ is a continuous polynomial of degree at most $k$ and $\\Vert P\\Vert =\\Vert Q\\Vert =\\displaystyle\\lim_{i,\\mathfrak U} \\Vert P_i \\Vert$. If $\\displaystyle\\lim_{i,\\mathfrak U} \\Vert P_{i,k} \\Vert =0 $ then the degree of $P$ is lower than $k$.\n\n\n\\end{proof}\n\nNote that, in the last proof, we can take the same approach used for non homogeneous polynomials in the real case, but we would not have the same control over the norms.\n\n\n\\section{ Main result }\n\n\n\nThis section contains our main result. As mentioned above, this result is partially motivated by Theorem 3.2 from \\cite{LR}. We follow similar ideas for the proof. First, let us fix some notation that will be used throughout this section.\n\nIn this section, all polynomials considered are continuous scalar polynomials. Given a Banach space $X$, $B_X$ and $S_X$ denote the unit ball and the unit sphere of $X$ respectively, and $X^*$ is the dual of $X$. Given a polynomial $P$ on $X$, $deg(P)$ stands for the degree of $P$. \n\n\\begin{defin} For a Banach space $X$ let $D(X,k_1,\\cdots,k_n)$ denote the smallest constant that satisfies (\\ref{problema}) for polynomials of degree $k_1,\\cdots,k_n$. We also define $C(X,k_1,\\cdots,k_n)$ as the smallest constant that satisfies (\\ref{problema}) for homogeneous polynomials of degree $k_1,\\cdots,k_n$.\n\\end{defin}\n\nThroughout this section most of the results will have two parts. The first involving the constant $C(X,k_1,\\cdots,k_n)$ for homogeneous polynomials and the second involving the constant $D(X,k_1,\\cdots,k_n)$ for arbitrary polynomials. Given that the proof of both parts are almost equal, we will limit to prove only the second part of the results.\n\n\nRecall that a space $X$ has the $1 +$ uniform approximation property if for all $n\\in \\mathbb N$, exists $m=m(n)$ such that for every subspace $M\\subset X$ with $dim(M)=n$ and every $\\varepsilon > 0$ there is an operator $T\\in \\mathcal{L}(X,X)$ with $T|_M=id$, $rg(T)\\leq m$ and $\\Vert T\\Vert  \\leq 1 + \\varepsilon$ (i.e. for every $\\varepsilon > 0$ $X$ has the $1+\\varepsilon$ uniform approximation property).\n\n\n\\begin{mainthm}\\label{main thm} If $\\mathfrak U$ is an ultrafilter on a family $I$ and $(X_i)_\\mathfrak U$ is an ultraproduct of complex Banach spaces then\n\n\\begin{enumerate}\n\\item[(a)] $C((X_i)_\\mathfrak U,k_1,\\cdots,k_n) \\geq \\displaystyle\\lim_{i,\\mathfrak U}(C(X_i,k_1,\\cdots,k_n)).$\n\n\\item[(b)] $D((X_i)_\\mathfrak U,k_1,\\cdots,k_n) \\geq \\displaystyle\\lim_{i,\\mathfrak U}(D(X_i,k_1,\\cdots,k_n)).$\n\\end{enumerate}\nMoreover, if each $X_i$ has the $1+$ uniform approximation property, equality holds in both cases.\n\\end{mainthm}\n\n\nIn order to prove this Theorem some auxiliary lemmas are going to be needed. The first one is due to Heinrich \\cite{H}.\n\n\\begin{lem}\\label{aprox} Given an ultraproduct of Banach spaces $(X_i)_\\mathfrak U$, if each $X_i$ has the $1+$ uniform approximation property then $(X_i)_\\mathfrak U$ has the metric approximation property.\n\\end{lem}\n\n\n\n\n\nWhen working with the constants $C(X,k_1,\\cdots,k_n)$ and $D(X,k_1,\\cdots,k_n)$, the following characterization may result handy.\n\n\\begin{lem}\\label{alternat} a) The constant $C(X,k_1,\\cdots,k_n)$ is the biggest constant $M$ such that given any $\\varepsilon >0$ there exist a set of homogeneous continuous polynomials $\\{P_j\\}_{j=1}^n$ with $deg(P_j)\\leq k_j$ such that\n\n\n\\begin{equation} \\label{condition} M\\left \\Vert \\prod_{j=1}^{n} P_j \\right \\Vert \\leq  (1+\\varepsilon) \\prod_{j=1}^{n} \\Vert P_j \\Vert. \\end{equation}\n\n\\begin{flushleft}\nb) The constant $D(X,k_1,\\cdots,k_n)$ is the biggest constant satisfying the same for arbitrary polynomials.\n\\end{flushleft}\n\n\n\n\n\\end{lem}\n\n\nTo prove this Lemma it is enough to see that $D(X,k_1,\\cdots,k_n)$ is decreasing as a function of the degrees $k_1,\\cdots, k_n$ and use that the infimum is the greatest lower bound.\n\n\\begin{rem}\\label{rmkalternat} It is clear that in Lemma \\ref{alternat} we can take the polynomials $\\{P_j\\}_{j=1}^n$ with $deg(P_j)= k_j$ instead of $deg(P_j)\\leq k_j$. Later on we will use both versions of the Lemma. \n\\end{rem}\n\nOne last lemma is needed for the proof of the Main Theorem.\n\n\\begin{lem}\\label{normas} Let $P$ be a (not necessarily homogeneous) polynomial on a complex Banach space $X$ with $deg(P)=k$. For any point $x\\in X$ \n\\begin{equation} |P(x)|\\leq  \\max\\{\\Vert x \\Vert, 1\\}^k \\Vert P\\Vert  . \\nonumber \\end{equation}\n\\end{lem}\n\n\\begin{proof} If $P$ is homogeneous the result is rather obvious since we have the inequality\n\\begin{equation} |P(x)|\\leq \\Vert x \\Vert^k \\Vert P\\Vert . \\nonumber \\end{equation}\nSuppose that $P=\\sum_{l=0}^k P_l$ with $P_l$ an  $l-$homogeneous polynomial. Consider the space $X \\oplus_\\infty \\mathbb C$ and the polynomial $\\tilde{P}:X \\oplus_\\infty \\mathbb C\\rightarrow \\mathbb C$ defined by $\\tilde{P}(x,\\lambda)=\\sum_{l=0}^k P_l(x)\\lambda^{k-l}$. The polynomial $\\tilde{P}$ is homogeneous of degree $k$ and $\\Vert P \\Vert = \\Vert \\tilde{P} \\Vert $. Then, using that $\\tilde{P}$ is homogeneous we have\n\\begin{equation} |P(x)|=|\\tilde{P} (x,1)| \\leq \\Vert (x,1) \\Vert^k \\Vert \\tilde{P} \\Vert = \\max\\{\\Vert x \\Vert, 1\\}^k \\Vert P\\Vert . \\nonumber \\end{equation}\n\\end{proof}\n\nWe are now able  to prove our main result.\n\n\\begin{proof}[Proof of Main Theorem] \nThroughout this proof we regard the space $(\\mathbb C)_\\mathfrak U$ as $\\mathbb C$ via the identification $(\\lambda_i)_\\mathfrak U=\\displaystyle\\lim_{i,\\mathfrak U} \\lambda_i$.\n\nFirst, we are going to see that $D((X_i)_\\mathfrak U,k_1,\\cdots,k_n) \\geq \\displaystyle\\lim_{i,\\mathfrak U}(D(X_i,k_1,\\cdots,k_n))$. To do this we only need to prove that $\\displaystyle\\lim_{i,\\mathfrak U}(D(X_i,k_1,\\cdots,k_n))$ satisfies (\\ref{condition}). Given $\\varepsilon >0$ we need to find a set of polynomials $\\{P_{j}\\}_{j=1}^n$ on $(X_i)_\\mathfrak U$ with $deg(P_{j})\\leq k_j$  such that\n$$ \\displaystyle\\lim_{i,\\mathfrak U}(D(X_i,k_1,\\cdots,k_n)) \\left \\Vert \\prod_{j=1}^{n} P_j \\right \\Vert \\leq (1+\\varepsilon) \\prod_{j=1}^{n} \\left \\Vert P_j \\right \\Vert .$$\n\n\nBy Remark \\ref{rmkalternat} we know that for each $i\\in I$ there is a set of polynomials $\\{P_{i,j}\\}_{j=1}^n$ on $X_i$ with $deg(P_{i,j})=k_j$ such that\n$$ D(X_i,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} P_{i,j} \\right \\Vert \\leq (1 +\\varepsilon)\\prod_{j=1}^{n} \\left \\Vert P_{i,j} \\right \\Vert.$$\nReplacing $P_{i,j}$ with $P_{i,j}/\\Vert P_{i,j} \\Vert$ we may assume that $\\Vert P_{i,j} \\Vert =1$. Define the polynomials $\\{P_j\\}_{j=1}^n$ on $(X_i)_\\mathfrak U$ by $P_j((x_i)_\\mathfrak U)=(P_{i,j}(x_i))_\\mathfrak U$. Then, by Proposition \\ref{pollim}, $deg(P_j)\\leq k_j$ and\n\\begin{eqnarray} \\displaystyle\\lim_{i,\\mathfrak U}(D(X_i,k_1,\\cdots,k_n)) \\left \\Vert \\prod_{j=1}^{n} P_{j} \\right \\Vert &=& \\displaystyle\\lim_{i,\\mathfrak U} \\left(D(X_i,k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} P_{i,j} \\right \\Vert \\right)  \\nonumber \\\\\n&\\leq& \\displaystyle\\lim_{i,\\mathfrak U}\\left((1+\\varepsilon)\\prod_{j=1}^{n}\\Vert  P_{i,j}  \\Vert \\right)\\nonumber \\\\\n&=& (1+\\varepsilon)\\prod_{j=1}^{n} \\Vert P_{j}  \\Vert \\nonumber \n \\nonumber \n\\end{eqnarray}\nas desired.\n\n\nTo prove that $D((X_i)_\\mathfrak U,k_1,\\cdots,k_n) \\leq \\displaystyle\\lim_{i,\\mathfrak U}(D(X_i,k_1,\\cdots,k_n))$ if each $X_i$ has the $1+$ uniform approximation property is not as straightforward. Given $\\varepsilon >0$, let $\\{P_j\\}_{j=1}^n$ be a set of polynomials on $(X_i)_\\mathfrak U$ with $deg(P_j)=k_j$ such that\n$$ D((X_i)_\\mathfrak U,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} P_j \\right \\Vert \\leq  (1+\\varepsilon)\\prod_{j=1}^{n} \\Vert P_j \\Vert . $$\n\nLet $K\\subseteq B_{(X_i)_\\mathfrak U}$ be the finite set $K=\\{x_1,\\cdots, x_n\\}$ where $ x_j$ is such that \n$$|P_j(x_j)| > \\Vert P_j\\Vert (1- \\varepsilon) \\mbox{ for }j=1,\\cdots, n.$$ \nBeing that each $X_i$ has the $1+$ uniform approximation property, then, by Lemma \\ref{aprox}, $(X_i)_\\mathfrak U$ has the metric approximation property. Therefore, exist a finite rank operator $S:(X_i)_\\mathfrak U\\rightarrow (X_i)_\\mathfrak U$ such that $\\Vert S\\Vert \\leq 1 $ and \n$$\\Vert P_j - P_j \\circ S \\Vert_K< |P_j(x_j)|\\varepsilon \\mbox{ for }j=1,\\cdots, n.$$ \n\n\nNow, define the polynomials $Q_1,\\cdots, Q_n$ on $(X_i)_\\mathfrak U$ as $Q_j=P_j\\circ S$. Then \n$$\\left\\Vert \\prod_{j=1}^n Q_j \\right\\Vert \\leq \\left\\Vert \\prod_{j=1}^n P_j \\right\\Vert $$\n$$\\Vert Q_j\\Vert_K > | P_j(x_j)|-\\varepsilon | P_j(x_j)| =| P_j(x_j)| (1-\\varepsilon) \\geq \\Vert P_j \\Vert(1-\\varepsilon)^2.$$\nThe construction of this polynomials is a slight variation of Lemma 3.1 from \\cite{LR}. We have the next inequality for the product of the polynomials $\\{Q_j\\}_{j=1}^n$\n\\begin{eqnarray} D((X_i)_\\mathfrak U,k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} Q_{j} \\right \\Vert &\\leq& D((X_i)_\\mathfrak U,k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} P_{j} \\right \\Vert \\nonumber \\\\\n&\\leq&  (1+\\varepsilon) \\prod_{j=1}^{n}  \\left \\Vert P_{j} \\right \\Vert . \\label{desq}\\end{eqnarray}\n\n\nSince $S$ is a finite rank operator, the polynomials $\\{ Q_j\\}_{j=1}^n$ have the advantage that are finite type polynomials. This will allow us to construct polynomials on $(X_i)_\\mathfrak U$ which are limit of polynomials on the spaces $X_i$. For each $j$ write $Q_j=\\sum_{t=1}^{m_j}(\\psi_{j,t})^{r_{j,t}}$ with $\\psi_{j,t}\\in (X_i)_\\mathfrak U^*$, and consider the spaces $N=\\rm{span}  \\{x_1,\\cdots,x_n\\}\\subset (X_i)_\\mathfrak U$ and $M=\\rm{span} \\{\\psi_{j,t} \\}\\subset (X_i)_\\mathfrak U^*$. By the local duality of ultraproducts (see Theorem 7.3 from \\cite{H}) exist $T:M\\rightarrow (X_i^*)_\\mathfrak U$ an $(1+\\varepsilon)-$isomorphism such that\n$$JT(\\psi)(x)=\\psi(x) \\mbox{ } \\forall x\\in N, \\mbox{ } \\forall \\psi\\in M$$\nwhere $J:(X_i^*)_\\mathfrak U\\rightarrow (X_i)_\\mathfrak U^*$ is the canonical embedding. Let $\\phi_{j,t}=JT(\\psi_{j,t})$ and consider the polynomials $\\bar{Q}_1,\\cdots, \\bar{Q}_n$ on $(X_i)_\\mathfrak U$ with $\\bar{Q}_j=\\sum_{t=1}^{m_j}(\\phi_{j,t})^{r_{j,t}}$. Clearly $\\bar{Q}_j$ is equal to $Q_j$ in $N$ and $K\\subseteq N$, therefore we have the following lower bound for the norm of each polynomial\n\\begin{equation}\\Vert \\bar{Q}_j \\Vert \\geq \\Vert \\bar{Q}_j \\Vert_K = \\Vert Q_j \\Vert_K >\\Vert P_j \\Vert(1-\\varepsilon)^2 \\label{desbarq} \\end{equation}\n\nNow, let us find an upper bound for the norm of the product $\\Vert \\prod_{j=1}^n \\bar{Q}_j \\Vert$. Let $x=(x_i)_\\mathfrak U$ be any point in $B_{(X_i)_\\mathfrak U}$. Then, we have \n\\begin{eqnarray} \\left|\\prod_{j=1}^n \\bar{Q}_j(x)\\right| &=& \\left|\\prod_{j=1}^n \\sum_{t=1}^{m_j}(\\phi_{j,t} (x))^{r_{j,t}}\\right|=\\left|\\prod_{j=1}^n \\sum_{t=1}^{m_j} (JT\\psi_{j,t}(x))^{r_{j,t}} \\right| \\nonumber \\\\\n&=& \\left|\\prod_{j=1}^n \\sum_{t=1}^{m_j}((JT)^*\\hat{x}(\\psi_{j,t}))^{r_{j,t}}\\right|\\nonumber\\end{eqnarray}\n\nSince $(JT)^*\\hat{x}\\in M^*$, $\\Vert (JT)^*\\hat{x}\\Vert =\\Vert JT \\Vert \\Vert x \\Vert \\leq \\Vert J \\Vert \\Vert T \\Vert \\Vert x \\Vert< 1 + \\varepsilon$ and $M^*=\\frac{(X_i)_\\mathfrak U^{**}}{M^{\\bot}}$, we can chose $z^{**}\\in (X_i)_\\mathfrak U^{**}$ with $\\Vert z^{**} \\Vert < \\Vert (JT)^*\\hat{x}\\Vert+\\varepsilon < 1+2\\varepsilon$, such that $\\prod_{j=1}^n \\sum_{t=1}^{m_j} ((JT)^*\\hat{x}(\\psi_{j,t}))^{r_{j,t}}= \\prod_{j=1}^n \\sum_{t=1}^{m_j} (z^{**}(\\psi_{j,t}))^{r_{j,t}}$. By Goldstine's Theorem exist a net $\\{z_\\alpha\\} \\subseteq (X_i)_\\mathfrak U$ $w^*-$convergent to $z$ in $(X_i)_\\mathfrak U^{**}$ with $\\Vert z_\\alpha \\Vert = \\Vert z^{**}\\Vert$. In particular, $ \\psi_{j,t}(z_\\alpha)$ converges to $z^{**}(\\psi_{j,t})$. If we call $\\mathbf k = \\sum k_j$, since $\\Vert z_\\alpha \\Vert< (1+2\\varepsilon)$, by Lemma \\ref{normas}, we have\n\\begin{equation} \\left \\Vert \\prod_{j=1}^{n} Q_j \\right \\Vert (1+2\\varepsilon)^\\mathbf k \\geq \\left|\\prod_{j=1}^n Q_j(z_\\alpha)\\right| = \\left|\\prod_{j=1}^n \\sum_{t=1}^{m_j} ((\\psi_{j,t})(z_\\alpha))^{r_{j,t}}\\right| .      \\label{usecomplex} \n\\end{equation}\nCombining this with the fact that\n\\begin{eqnarray} \\left|\\prod_{j=1}^{n} \\sum_{t=1}^{m_j} ((\\psi_{j,t})(z_\\alpha))^{r_{j,t}}\\right| &\\longrightarrow&  \\left|\\prod_{j=1}^{n} \\sum_{t=1}^{m_j} (z^{**}(\\psi_{j,t}))^{r_{j,t}}\\right|\\nonumber\\\\\n &=& \\left|\\prod_{j=1}^{n} \\sum_{t=1}^{m_j} ((JT)^*\\hat{x}(\\psi_{j,t}))^{r_{j,t}}\\right| = \\left|\\prod_{j=1}^{n} \\bar{Q}_j(x)\\right|\\nonumber\n\\end{eqnarray}\nwe conclude that $\\left \\Vert \\prod_{j=1}^{n} Q_j \\right \\Vert (1+2\\varepsilon)^\\mathbf k \\geq |\\prod_{j=1}^{n} \\bar{Q}_j(x)|$. \n\n\nSince the choice of $x$ was arbitrary we arrive to the next inequality\n\\begin{eqnarray}\n D((X_i)_\\mathfrak U,k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_j \\right \\Vert &\\leq& (1+2\\varepsilon)^\\mathbf k D((X_i)_\\mathfrak U,k_1,\\cdots,k_n)   \\left \\Vert \\prod_{j=1}^{n} Q_j \\right \\Vert   \\nonumber \\\\\n&\\leq&  (1+2\\varepsilon)^\\mathbf k (1+\\varepsilon) \\prod_{j=1}^{n}  \\left \\Vert P_{j} \\right \\Vert \\label{desbarq2} \\\\\n&<& (1+2\\varepsilon)^\\mathbf k (1+\\varepsilon) \\frac{\\prod_{j=1}^{n} \\Vert \\bar{Q}_j \\Vert }{(1-\\varepsilon)^{2n}} .\\label{desbarq3}  \\\n\\end{eqnarray}\nIn (\\ref{desbarq2}) and (\\ref{desbarq3}) we use (\\ref{desq}) and (\\ref{desbarq}) respectively. The polynomials $\\bar{Q}_j$ are not only of finite type, these polynomials are also generated by elements of $(X_i^*)_\\mathfrak U$. This will allow us to write them as limits of polynomials in $X_i$. For any $i$, consider the polynomials $\\bar{Q}_{i,1},\\cdots,\\bar{Q}_{i,n}$ on $X_i$ defined by $\\bar{Q}_{i,j}= \\displaystyle\\sum_{t=1}^{m_j} (\\phi_{i,j,t})^{r_{j,t}}$, where the functionals $\\phi_{i,j,t}\\in X_i^*$ are such that $(\\phi_{i,j,t})_\\mathfrak U=\\phi_{j,t}$. Then $\\bar{Q}_j(x)=\\displaystyle\\lim_{i,\\mathfrak U} \\bar{Q}_{i,j}(x)$ $\\forall x \\in (X_i)_\\mathfrak U$ and, by Proposition \\ref{pollim}, $\\Vert \\bar{Q}_j \\Vert = \\displaystyle\\lim_{i,\\mathfrak U} \\Vert \\bar{Q}_{i,j} \\Vert$. Therefore \n\\begin{eqnarray}\nD((X_i)_\\mathfrak U,k_1,\\cdots,k_n) \\displaystyle\\lim_{i,\\mathfrak U} \\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_{i,j} \\right \\Vert &=& D((X_i)_\\mathfrak U,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_{j} \\right \\Vert \\nonumber \\\\\n&<& \\frac{(1+\\varepsilon)(1+2\\varepsilon)^\\mathbf k}{(1-\\varepsilon)^{2n}} \\prod_{j=1}^{n} \\Vert \\bar{Q}_{j}  \\Vert \\nonumber \\\\\n&=& \\frac{(1+\\varepsilon)(1+2\\varepsilon)^\\mathbf k\t}{(1-\\varepsilon)^{2n}} \\prod_{j=1}^{n} \\displaystyle\\lim_{i,\\mathfrak U} \\Vert \\bar{Q}_{i,j}  \\Vert .\n \\nonumber \n\\end{eqnarray}\n\nTo simplify the notation let us call $\\lambda = \\frac{(1+\\varepsilon)(1+2\\varepsilon)^\\mathbf k}{(1-\\varepsilon)^{2n}} $. Take $L>0$ such that \n\n\\begin{equation}D((X_i)_\\mathfrak U,k_1,\\cdots,k_n) \\displaystyle\\lim_{i,\\mathfrak U} \\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_{i,j} \\right \\Vert < L < \\lambda \\prod_{j=1}^{n} \\displaystyle\\lim_{i,\\mathfrak U} \\Vert \\bar{Q}_{i,j}  \\Vert . \\nonumber \\end{equation}\n\nSince $(-\\infty, \\frac{L}{D((X_i)_\\mathfrak U,k_1,\\cdots,k_n)})$ and $(\\frac{L}{\\lambda},+\\infty)$ are neighborhoods of $\\displaystyle\\lim_{i,\\mathfrak U} \\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_{i,j} \\right \\Vert$ and $\\prod_{j=1}^{n} \\displaystyle\\lim_{i,\\mathfrak U} \\Vert \\bar{Q}_{i,j}  \\Vert$ respectively, and $\\prod_{j=1}^{n} \\displaystyle\\lim_{i,\\mathfrak U} \\Vert \\bar{Q}_{i,j}  \\Vert= \\displaystyle\\lim_{i,\\mathfrak U} \\prod_{j=1}^{n} \\Vert \\bar{Q}_{i,j}  \\Vert$, by definition of $\\displaystyle\\lim_{i,\\mathfrak U}$, the sets\n$$A=\\{i_0: D((X_i)_\\mathfrak U,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_{i_0,j} \\right \\Vert <L\\} \\mbox{ and }B=\\{i_0: \\lambda \\prod_{j=1}^{n}  \\Vert \\bar{Q}_{i_0,j}  \\Vert > L \\}$$\nare elements of $\\mathfrak U$. Since $\\mathfrak U$ is closed by finite intersections $A\\cap B\\in \\mathfrak U$. If we take any element $i_0 \\in A\\cap B$ then, for any $\\delta >0$, we have that\n\\begin{equation}D((X_i)_\\mathfrak U,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} \\bar{Q}_{i_0,j} \\right \\Vert \\frac{1}{\\lambda}\\leq \\frac{L}{\\lambda} \\leq \\prod_{j=1}^{n}  \\Vert \\bar{Q}_{i_0,j}  \\Vert < (1+ \\delta)\\prod_{j=\t1}^{n}  \\Vert \\bar{Q}_{i_0,j} \\Vert \\nonumber \\end{equation}\nThen, since $\\delta$ is arbitrary, the constant $D((X_i)_\\mathfrak U,k_1,\\cdots,k_n)\\frac{1}{\\lambda}$ satisfy (\\ref{condition}) for the space $X_{i_0}$ and therefore, by Lemma \\ref{alternat},\n\\begin{equation} \\frac{1}{\\lambda}D((X_i)_\\mathfrak U,k_1,\\cdots,k_n) \\leq  D(X_{i_0},k_1,\\cdots,k_n). \t\\nonumber \\end{equation}\n\nThis holds true for any $i_0$ in $A\\cap B$. Since $A\\cap B \\in \\mathfrak U$, by Lemma \\ref{lemlimit}, $\\frac{1}{\\lambda}D((X_i)_\\mathfrak U,k_1,\\cdots,k_n)\\leq \\displaystyle\\lim_{i,\\mathfrak U}  D(X_i,k_1,\\cdots,k_n) $. Using that $\\lambda \\rightarrow 1$ when $\\varepsilon \\rightarrow 0$ we conclude that $D((X_i)_\\mathfrak U,k_1,\\cdots,k_n)\\leq \\displaystyle\\lim_{i,\\mathfrak U}  D(X_i,k_1,\\cdots,k_n).$\n\\end{proof}\n\nSimilar to Corollary 3.3 from \\cite{LR}, a straightforward corollary of our main result is that for any complex Banach space $X$ with $1+$ uniform approximation property $C(X,k_1,\\cdots,k_n)=C(X^{**},k_1,\\cdots,k_n)$ and $D(X,k_1,\\cdots,k_n)=D(X^{**},k_1,\\cdots,k_n)$ .  Using that $X^{**}$ is $1-$complemented in some adequate ultrafilter $(X)_{\\mathfrak U}$ the result is rather obvious. For a construction of the adequate ultrafilter see \\cite{LR}. \n\nBut following the previous proof, and using the principle of local reflexivity applied to $X^*$ instead of the local duality of ultraproducts, we can prove the next stronger result.\n\n\n\\begin{thm}  Let $X$ be a complex Banach space. Then\n\n\\begin{enumerate}\n\\item[(a)] $C(X^{**},k_1,\\cdots,k_n)\\geq C(X,k_1,\\cdots,k_n).$\n\n\\item[(b)] $D(X^{**},k_1,\\cdots,k_n \\geq D(X,k_1,\\cdots,k_n)).$\n\\end{enumerate}\nMoreover, if $X^{**}$ has the metric approximation property, equality holds in both cases.\n\\end{thm}\n\n\\begin{proof} The inequality $D(X^{**},k_1,\\cdots,k_n) \\geq D(X,k_1,\\cdots,k_n)$ is a corollary of Theorem \\ref{main thm} (using the adequate ultrafilter mentioned above).\n\n\nLet us prove that if $X^{**}$ has the metric approximation property then $D((X^{**},k_1,\\cdots,k_n)\\geq D(X,k_1,\\cdots,k_n)$. Given $\\varepsilon >0$, let $\\{P_j\\}_{j=1}^n$ be a set of polynomials on $X^{**}$ with $deg(P_j)=k_j$ such that\n\\begin{equation} D(X^{**},k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} P_{j} \\right \\Vert \\leq (1+\\varepsilon)\\prod_{j=1}^{n} \\left \\Vert P_{j} \\right \\Vert .\\nonumber \\end{equation}\n\nAnalogous to the proof of Theorem \\ref{main thm}, since $X^{**}$ has the metric approximation, we can construct finite type polynomials $Q_1,\\cdots,Q_n$ on $X^{**}$ with $deg(Q_j)=k_j$, $\\Vert Q_j \\Vert_K \\geq \\Vert P_j \\Vert (1-\\varepsilon)^2$ for some finite set $K\\subseteq B_{X^{**}}$ and that\n\\begin{equation}D(X^{**},k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} Q_{j} \\right \\Vert < (1+\\varepsilon)\\prod_{j=1}^{n} \\left \\Vert P_{j} \\right  \\Vert . \\nonumber \\end{equation}\n\nSuppose that $Q_j=\\sum_{t=1}^{m_j}(\\psi_{j,t})^{r_{j,t}}$ and consider the spaces $N=\\rm{span} \\{K\\}$ and $M=\\rm{span} \\{\\psi_{j,t} \\}$. By the principle of local reflexivity (see \\cite{D}), applied to $X^*$ (thinking $N$ as a subspaces of $(X^*)^*$ and $M$ as a subspaces of $(X^*)^{**}$), there is an  $(1+\\varepsilon)-$isomorphism $T:M\\rightarrow X^*$ such that\n$$JT(\\psi)(x)=\\psi(x) \\mbox{ } \\forall x\\in N, \\mbox{ } \\forall \\psi\\in M\\cap X^*=M,$$\nwhere $J:X^*\\rightarrow X^{***}$ is the canonical embedding. \n\nLet $\\phi_{j,t}=JT(\\psi_{j,t})$ and consider the polynomials $\\bar{Q}_1,\\cdots, \\bar{Q}_n$ on $X^{**}$ defined by $\\bar{Q}_j=\\sum_{t=1}^{m_j}(\\phi_{j,t})^{r_{j,t}}$. Following the proof of the Main Theorem, one arrives to the inequation\n\\begin{equation}D(X^{**},k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} \\bar{Q_j} \\right \\Vert < (1+ \\delta) \\frac{(1+\\varepsilon)(1+2\\varepsilon)^\\mathbf k}{(1-\\varepsilon)^{2n}} \\prod_{j=1}^{n} \\Vert \\bar{Q_j}  \\Vert \\nonumber \\end{equation}\nfor every $\\delta >0$. Since each $\\bar{Q}_j$ is generated by elements of $J(X^*)$, by Goldstine's Theorem, the restriction of $\\bar{Q}_j$ to $X$ has the same norm and the same is true for $\\prod_{j=1}^{n} \\bar{Q_j}$. Then\n\\begin{equation}D(X^{**},k_1,\\cdots,k_n)\\left \\Vert \\prod_{j=1}^{n} \\left.\\bar{Q_j}\\right|_X \\right \\Vert < (1+ \\delta) \\frac{(1+\\varepsilon)(1+2\\varepsilon)^\\mathbf k}{(1-\\varepsilon)^{2n}} \\prod_{j=1}^{n} \\Vert \\left.\\bar{Q_j}\\right|_X  \\Vert \\nonumber \\end{equation}\nBy Lemma \\ref{alternat} we conclude that \n$$\\frac{(1-\\varepsilon)^{2n}}{(1+\\varepsilon)(1+2\\varepsilon)^\\mathbf k}D(X^{**},k_1,\\cdots,k_n)\\leq D(X,k_1,\\cdots,k_n).$$ Given that the choice of $\\varepsilon$ is arbitrary and that $\\frac{(1-\\varepsilon)^{2n}}{(1+\\varepsilon)(1+2\\varepsilon)^\\mathbf k} $ tends to $1$ when $\\varepsilon$ tends to $0$ we conclude that $D(X^{**},k_1,\\cdots,k_n)\\leq D(X,k_1,\\cdots,k_n)$.\n\n\\end{proof}\n\nNote that in the proof of the Main Theorem the only parts where we need the spaces to be complex Banach spaces are at the beginning, where we use Proposition \\ref{pollim}, and in the inequality (\\ref{usecomplex}), where we use Lemma \\ref{normas}. But both results holds true for homogeneous polynomials on a real Banach space. Then, copying the proof of the Main Theorem we obtain the following result for real spaces.\n\n\\begin{thm} If $\\mathfrak U$ is an ultrafilter on a family $I$ and $(X_i)_\\mathfrak U$ is an ultraproduct of real Banach spaces then\n $$C((X_i)_\\mathfrak U,k_1,\\cdots,k_n) \\geq \\displaystyle\\lim_{i,\\mathfrak U}(C(X_i,k_1,\\cdots,k_n)).$$\n\n\nIf in addition each $X_i$ has the $1+$ uniform approximation property, the equality holds.\n\\end{thm}\n\nAlso we can get a similar result for the bidual of a real space.\n\n\\begin{thm}  Let $X$ be a real Banach space. Then\n\n\\begin{enumerate}\n\\item[(a)] $C(X^{**},k_1,\\cdots,k_n)\\geq C(X,k_1,\\cdots,k_n).$\n\n\\item[(b)] $D(X^{**},k_1,\\cdots,k_n) \\geq D(X,k_1,\\cdots,k_n).$\n\\end{enumerate}\nIf $X^{**}$ has the metric approximation property, equality holds in $(a)$.\n\\end{thm}\n\n\\begin{proof} The proof of item $(a)$ is the same that in the complex case, so we limit to prove $D(X^{**},k_1,\\cdots,k_n) \\geq D(X,k_1,\\cdots,k_n))$. To do this we will show that given an arbitrary $\\varepsilon >0$, there is a set of polynomials $\\{P_{j}\\}_{j=1}^n$ on $X^{**}$ with $deg(P_{j})\\leq k_j$  such that\n$$ D(X,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} P_j \\right \\Vert \\leq (1+\\varepsilon) \\prod_{j=1}^{n} \\left \\Vert P_j \\right \\Vert .$$\n\n\nTake $\\{Q_{j}\\}_{j=1}^n$ a set of polynomials  on $X$ with $deg(Q_j)=k_j$ such that\n$$ D(X,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} Q_{j} \\right \\Vert \\leq (1 +\\varepsilon)\\prod_{j=1}^{n} \\left \\Vert Q_{j} \\right \\Vert.$$\n\nConsider now the polynomials $P_j=AB(Q_j)$, where $AB(Q_j)$ is the Aron Berner extension of $Q_j$ (for details on this extension see \\cite{AB} or \\cite{Z}). Since $AB\\left( \\prod_{j=1}^n P_j \\right)=\\prod_{j=1}^n AB(P_j)$, using that the Aror Berner extension preserves norm (see \\cite{DG}) we have\n\n\\begin{eqnarray} D(X,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} P_{j} \\right \\Vert &=& D(X,k_1,\\cdots,k_n) \\left \\Vert \\prod_{j=1}^{n} Q_{j}  \\right \\Vert\\nonumber \\\\\n&\\leq& (1 +\\varepsilon)\\prod_{j=1}^{n} \\left\\Vert Q_{j} \\right\\Vert \\nonumber \\\\\n&=& (1 +\\varepsilon)\\prod_{j=1}^{n} \\left \\Vert P_{j} \\right  \\Vert \\nonumber \n\\end{eqnarray}\nas desired.\n\n\\end{proof}\n\n\nAs a final remark, we mention two types of spaces for which the results on this section can be applied.\n\n\nCorollary 9.2 from \\cite{H} states that any Orlicz space $L_\\Phi(\\mu)$, with $\\mu$ a finite measure and $\\Phi$ an Orlicz function with regular variation at $\\infty$, has the $1+$ uniform projection property, which is stronger than  the $1+$ uniform approximation property.\n\nIn \\cite{PeR} Section two, A. Pe\\l czy\\'nski and H. Rosenthal proved that any $\\mathcal L_{p,\\lambda}-$space ($1\\leq \\lambda < \\infty$) has the $1+\\varepsilon-$uniform projection property for every $\\varepsilon>0$ (which is stronger than the $1+\\varepsilon-$uniform approximation property), therefore, any $\\mathcal L_{p,\\lambda}-$space has the $1+$ uniform approximation property. \n\n\\section*{Acknowledgment}\nI would like to thank Professor Daniel Carando for both encouraging me to write this article, and for his comments and remarks which improved its presentation and content.\n\n\n", 0.9900941508273343], ["\\section{Introduction} \nRecently much work has been done to design structure preserving methods, but\nwhile the construction of such methods was found early on in two dimensions,\nthe three-dimensional case remained difficult and the introduction of the finite element exterior calculus brought a significant breakthrough.\nAn excellent review is given by V. John et al.\\ \\cite{John2017}.\nThe general idea taken from the finite element exterior calculus is to use a subcomplex of the De Rham complex.\nThere are well-known discrete counterparts of this complex with minimal regularity, however the discretization of smoother variants is still an active topic \nusually leading to shape functions of high degree, for example see \\cite{Neilan2015}.\nWe chose to use the complex with minimal regularity, as this is often done for electromagnetism or recently for magnetohydrodynamics (see \\cite{Hu2021}).\n\nThe main difference from usual schemes lies in the regularity of the velocity field\nsince we only require it to be in $H(\\mathrm{div})$ and in the discrete adjoint of $H(\\mathrm{curl})$.\nAlthough the continuous space regularity is the same as the usual one, since the adjoint of $(\\mathrm{curl},H(\\mathrm{curl}))$ is $(\\mathrm{curl},H_0(\\mathrm{curl}))$, \nand the velocity is sought in $H(\\mathrm{div}) \\cap H_0(\\mathrm{curl}) \\subset H^1$ (for a smooth enough domain, it is discussed in part 3.2 of \\cite{GiraultRaviart}).\nThis does not hold (in general) in the discrete case since for $V_h \\subset H(\\mathrm{curl})$ and $(\\mathrm{curl}^*,V_h^*)$\nthe adjoint of $(\\mathrm{curl},V_h)$ we no longer have $V_h^* \\subset H(\\mathrm{curl})$.\nThis has a fundamental impact both from the philosophical and practical point of view.\nIn practice $v \\in H(\\mathrm{div})$ (resp. $v \\in H(\\mathrm{curl})$) does not impose continuity of the tangential (resp. normal) components on faces.\nThis suggests that \nwe will not have any degree of freedom corresponding to these\nand lack any way to set them in a Dirichlet boundary condition.\nThis means that the normal and the tangential part of the boundary condition must be treated in two different ways.\nIt makes more sense in the exterior algebra and means that the fluid velocity is really sought as a $2$-form \n(mostly defined by its flux across cell boundaries) which happens to be regular enough to also be in the domain of the exterior derivative adjoint.\n\nLet us summarize the main idea of the algorithm. In order to preserve the free divergence constraint, we have to consider $u$ as a $2$-form,\nwhich can be discretized by face elements. \nThen it is not straightforward to discretize the Laplacian in the usual way $\\langle \\nabla u, \\nabla v \\rangle$ because $\\nabla u$ is not a natural quantity for a $2$-form.\nOur simple trick is to use $\\nabla \\cdot u = 0$ to rewrite the Laplacian: \n\\begin{equation} \\label{eq:laplace}\n\\begin{aligned}\n\\Delta u =& \\nabla (\\nabla \\cdot u) - \\nabla \\times (\\nabla \\times u) \\\\\n=& - \\nabla \\times (\\nabla \\times u).\n\\end{aligned}\n\\end{equation}\n\nLet $\\Omega$ be a bounded domain of $\\mathbb{R}^3$ and $T > 0$, we recall the Navier-Stokes equations:\n\\begin{equation}\n\\begin{aligned}\nu_t + (u \\cdot \\nabla) u - \\nu \\Delta u + \\nabla p =&\\ f \\text{ on } \\Omega \\times (0,T),\\\\\n\\nabla \\cdot u =&\\ 0 \\text{ on } \\Omega \\times (0,T)\n\\end{aligned}\n\\end{equation}\ntogether with some boundary and initial conditions,\nwhere $u$ is the velocity of the fluid,\n$p$ the pressure, $\\nu$ the kinematic viscosity and $f$ an external force.\\\\\nUsing the Lamb identity $(u \\cdot \\nabla) u = (\\nabla \\times u) \\times u + \\frac{1}{2} \\nabla (u \\cdot u)$ and Equation \\eqref{eq:laplace}, we get the following formulation\n\\begin{equation}\n\\begin{aligned}\nu_t + (\\nabla \\times u) \\times u + \\nu \\nabla \\times (\\nabla \\times u) + \\nabla P =&\\ f \\text{ on } \\Omega \\times (0,T),\\\\\n\\nabla \\cdot u =&\\ 0 \\text{ on } \\Omega \\times (0,T).\n\\end{aligned}\n\\end{equation}\nwhere $P = p + \\frac{1}{2} u \\cdot u$ is the Bernoulli pressure.\n\nSince $u$ is a $2$-form it is not natural to take $\\nabla \\times u$ (and it is unadvisable for reasons detailed in Remark \\ref{rem:removedstar}).\nTherefore, we introduce an auxiliary variable $\\omega = \\nabla \\times u$ (namely the vorticity) and work with a mixed problem.\nThis is known as the vorticity-velocity-pressure formulation and was considered by many others (see \\cite{ARNOLD2012,Amoura,Kreeft,Dubois,Anaya}).\nThe finite element exterior calculus framework is very flexible because it\nallows us to work in abstract  spaces which can be discretized easily provided that some exactness properties are fulfilled.\nTherefore we shall use a generic name for our spaces, here $V^1 \\times V^2 \\times V^3$ for the continuous spaces and $V_h^1 \\times V^2_h \\times V^3_h$ for the discrete spaces (indexed by the mesh size $h$).\nThe exact requirements for these spaces require introducing some concepts and notations, \nhence for the sake of readability we postpone the definition to Section \\ref{Spaces}.\nTypically a valid choice is to take $V^1 = H(\\text{curl},\\Omega)$, $V^2 = H(\\text{div},\\Omega)$ and $V^3 = L^2(\\Omega)$.\nWe also need another space $\\mathfrak{H}^3 \\subset V^3$. This is a vector space that does not only depend on $V^3$ but on the couple $V^2 \\times V^3$\nand which is typically of small dimension, i.e.\\ of dimension $0$ or $1$ in the case of interest to us.\nFrom the point of view of exterior calculus $\\mathfrak{H}^3$ is the set of harmonic $3$-forms, \nfor a more practical point of view $\\mathfrak{H}^3$ can be understood as a set of Lagrange multipliers.\n\\begin{remark}\nThe choice of boundary conditions is encoded in the choice of $V^1 \\times V^2 \\times V^3$.\nMore details are given later in Section \\ref{Boundaryconditions}.\n\\end{remark}\n\nAn example of discrete in time, mixed and linearized weak formulation is:\\\\\nGiven $f^n \\in L^2(\\Omega)$, find $(\\omega^n,u^n,p^n,\\phi^n) \\in V^1 \\times V^2 \\times V^3 \\times \\mathfrak{H}^3$ such that \n$\\forall (\\tau,v,q,\\chi) \\in V^1 \\times V^2 \\times V^3 \\times \\mathfrak{H}^3$,\n\n\\begin{subequations} \\label{eq:NS_Eulercont}\n\\begin{align}\n\\langle \\omega^n,\\tau \\rangle - \\langle u^n,\\nabla \\times \\tau \\rangle =&\\ 0, \\label{eq:weakl1}\\\\\n\\langle \\frac{1}{\\delta t} u^n, v \\rangle + \\langle \\nu \\nabla \\times \\omega^n + \\theta \\omega^n \\times u^{n-1} + (1 - \\theta) \\omega^{n-1} \\times u^n,v \\rangle& \\nonumber \\\\\n- \\langle p^n, \\nabla \\cdot v \\rangle\n=&\\ \\langle \\frac{1}{\\delta t} u^{n-1}, v \\rangle + \\langle f^n,v \\rangle, \\label{eq:weakl2}\\\\\n\\langle \\nabla \\cdot u^n + \\phi^n,q \\rangle =&\\ 0 \\label{eq:weakl3}\\\\\n\\langle p^n,\\chi \\rangle =&\\ 0. \\label{eq:weakl4}\n\\end{align}\n\\end{subequations}\nHere we used an implicit Euler discretization in time and consider a sequence of solutions at each time step indexed by an integer $n \\geq 1$.\nThe real number $\\theta \\in [0,1]$ is an arbitrary parameter. \nIn the following we take $\\theta = \\frac{1}{2}$.\nLet us look at a specific example in order to give some explanation on Equation \\eqref{eq:NS_Eulercont}:\nIf we take $V^1 = H(\\text{curl},\\Omega)$, $V^2 = H(\\text{div},\\Omega)$ and $V^3 = L^2(\\Omega)$ \nthen we must have $\\mathfrak{H}^3 = \\lbrace 0 \\rbrace$.\nEquation \\eqref{eq:weakl1} is equivalent to $\\omega^n = \\nabla \\times u^n$ and $u^n \\in H_0(\\mathrm{curl}, \\Omega)$,\n\\eqref{eq:weakl2} is equivalent to \n$\\frac{u^n - u^{n-1}}{\\delta t} + \\nu \\nabla \\times (\\nabla \\times u^n) + \\frac{1}{2} (\\omega^n \\times u^{n-1} + \\omega^{n-1} \\times u^n) + \\nabla p^n = f^n$ and $p^n \\in H_0^1$,\n\\eqref{eq:weakl3} is equivalent to $\\nabla \\cdot u^n = 0$,\nand \\eqref{eq:weakl4} is trivial here (since $\\mathfrak{H}^3 = \\lbrace 0 \\rbrace$).\nThis formulation is similar to the one studied by Anaya et al.\\ \\cite{Anaya}. \nThe main difference is that our formulation is studied in the framework of finite element exterior calculus and for arbitrary low order perturbations (see Equation \\eqref{eq:introformulation} below). \nIn particular the abstraction made on discrete spaces allows using any discrete subcomplex (as defined in Section \\ref{Notations}). \nTwo families are given as example in Section \\ref{Spaces} but more exist on different kind of meshes (e.g.\\ the cubical elements \\cite[Chapter~7.7]{feec-cbms}).\nThe construction of such families is still an active topic \nand the independence over the choice of discrete subcomplex is a great feature of finite element exterior calculus \nallowing to choose any family without modification to the proofs.\n\n\\begin{remark}\nEquations \\eqref{eq:weakl3} and \\eqref{eq:weakl4} might seem odd but in fact are quite natural.\nFrom the exterior calculus point of view $\\phi$ and $\\chi$ are harmonic forms of $V^3$, \nin practice they can be viewed as Lagrange multipliers.\nFor the given example $\\mathfrak{H}^3$ is trivial but if instead we consider \n$V^1 = H_0(\\text{curl},\\Omega)$, $V^2 = H_0(\\text{div},\\Omega)$ and $V^3 = L^2(\\Omega)$ \nthen we must have $\\mathfrak{H}^3 \\approx \\mathbb{R}$ (the $1$-dimensional vector space of constant functions).\nEquation \\eqref{eq:weakl3} ensure that the system is onto, although here the right-hand side is null so $\\phi^n$ will always be zero.\nEquation \\eqref{eq:weakl4} ensure that $p^n$ is orthogonal to the space of harmonics $3$-forms (i.e.\\ here that $\\int_\\Omega p^n = 0$).\n\\end{remark}\n\nAbstracting the linearization and time discretization scheme we simply consider two linear maps: \n$l_3$ and $l_5$ defined on $L^2(\\Omega) \\rightarrow L^2(\\Omega)$ \n(the name of which follows the convention of Arnold \\cite{Arnold2016}).\nAnd we define the problem:\\\\\nGiven $f_2, f_3 \\in L^2(\\Omega)$, find $(\\omega,u,p,\\phi) \\in V^1 \\times V^2 \\times V^3 \\times \\mathfrak{H}^3$ such that \n$\\forall (\\tau,v,q,\\chi) \\in V^1 \\times V^2 \\times V^3 \\times \\mathfrak{H}^3$:\n\\begin{equation} \\label{eq:introformulation} \n\\begin{aligned}\n\\langle \\omega,\\tau \\rangle - \\langle u,\\nabla \\times \\tau \\rangle =&\\ 0, \\\\\n\\langle \\nu \\nabla \\times \\omega + l_3 \\omega + l_5 u,v \\rangle - \\langle p, \\nabla \\cdot v \\rangle\n=&\\ \\langle f_2,v \\rangle, \\\\\n\\langle \\nabla \\cdot u,q \\rangle + \\langle \\phi,q \\rangle =&\\ \\langle f_3, q \\rangle,\\\\\n\\langle p,\\chi \\rangle =&\\ 0.\n\\end{aligned}\n\\end{equation}\nWe easily see that a suitable choice of $l_3$ and $l_5$ (namely $l_3 = (v \\rightarrow \\frac{1}{2} v \\times u^{n-1})$ and $l_5 = (v \\rightarrow \\frac{1}{\\delta t} v + \\frac{1}{2} \\omega^{n-1} \\times v$)\nallows recovering \\eqref{eq:NS_Eulercont}.\nWe redefine the problem in the framework of exterior calculus in \\eqref{eq:defmixedcontinuous}. We also define a discrete counterpart to this problem in \\eqref{eq:defmixeddiscrete}.\n\nUnder mild assumptions on $l_3$, $l_5$ and $\\Omega$ detailed in \\eqref{eq:regularity35} we prove the well-posedness of the problem \\eqref{eq:defmixedcontinuous} (or equivalently \\eqref{eq:introformulation}) and of its discrete counterpart \\eqref{eq:defmixeddiscrete}.\nIf we write $(\\omega,u,p,\\phi)$ (resp. $(\\omega_h,u_h,p_h,\\phi_h)$) the solution of \\eqref{eq:defmixedcontinuous} (resp. \\eqref{eq:defmixeddiscrete}) then we derive an optimal a priori error estimate on the energy norm \nproportional to the approximation properties of the discrete spaces used.\nThe result is stated in Corollary \\ref{corollary:pertubedestimate}.\nWhen $f_3 = 0$ (which is the case for \\eqref{eq:NS_Eulercont}) we show that the velocity is exactly divergence free even at the discrete level, i.e.\\ that\n$ \\nabla \\cdot u = \\nabla \\cdot u_h = 0 $ holds pointwise. \nWe also show that the scheme is pressure-robust (see Section \\ref{Pressurerobustness}). \nThis allows to derive an error estimate for the vorticity and velocity independent of the pressure in Theorem \\ref{th:pressurerobustestimate}.\n\nThe remaining of this paper is divided as follows:\nWe define the notations used in the paper and discuss some applications of the scheme in Section \\ref{Setting}.\nWe introduce an intermediary problem akin to a Stokes problem in Section \\ref{Unpertubedproblem}.\nWe show well-posedness and derive improved error estimates for this intermediary problem that will be useful in Section \\ref{Pertubedproblem}.\nSection \\ref{Pertubedproblem} is dedicated to the analysis of problem \\eqref{eq:introformulation}.\nThis is the most technical section.\nWe derive some additional results in Section \\ref{Conservedquantities},\nand finally present a variety of numerical simulations done with our scheme in Section \\ref{Numericalsimulations} that validate our results and give some perspectives.\nThe exterior calculus formalism is introduced in Section \\ref{Setting} and heavily used in Section \\ref{Unpertubedproblem} and \\ref{Pertubedproblem}. \nWe assume that the reader has some familiarity with exterior calculus in those two sections (\\ref{Unpertubedproblem} and \\ref{Pertubedproblem}).\nHowever, no prior knowledge of exterior calculus are expected in Section \\ref{Conservedquantities} and \\ref{Numericalsimulations}.\n\n\\section{Setting} \\label{Setting}\nThe exterior calculus framework allows getting a uniform vision on many objects. \nAs such it may appear abstract and confusing at first.\nTherefore, we will start by giving some explicit examples of spaces to fix the ideas.\nWe will also discuss how to deal with boundary conditions and how to use the scheme in a $2$-dimensional space.\nOnly then we will introduce the full notations and specifications of exterior calculus which will be used in the remaining of the paper.\n\n\\subsection{Function spaces} \\label{Spaces}\nOur scheme does not rely on a particular choice of discrete spaces, instead we make some assumptions (given in Section \\ref{Notations}) on them and any space fulfilling these assumptions can be used.\nAdequate spaces are readily available on simplicial and cubic meshes (they are given e.g.\\ in the \nperiodic table of finite elements \\cite{periodictable}).\nWe illustrate here an example of sensible choice. \nSince the discrete spaces depend on the continuous one (through boundary conditions) we begin by setting the continuous spaces for this example.\nLet $V^1 = H_0(\\mathrm{curl},\\Omega)$, \n$V^2 = H_0(\\mathrm{div},\\Omega)$, $V^3 = L^2(\\Omega)$ and $\\mathfrak{H}^3 = \\mathbb{R} \\subset L^2(\\Omega)$.\nThe discrete spaces depend on a parameter $r \\in \\mathbb{N}$ which is a polynomial degree.\nLet $\\mathbf{T}_h$ be a simplicial triangulation of $\\Omega$, we choose the following discretization:\n\\begin{itemize} \\label{defaultspaces}\n\\item To define the curl space $V^1_h$ we use Nedelec's edge elements of the first kind of degree $r$ (or $P^-_r\\Lambda^1$ in the periodic table),\n$V_h^1 = \\lbrace \\omega \\in H_0(\\mathrm{curl},\\Omega);\\; \\omega_{\\vert T} \\in P_r^-\\Lambda^1(T), \\forall T \\in \\mathbf{T}_h \\rbrace$.\n\\item To define the velocity space $V^2_h$ we use Nedelec's face elements of the first kind of degree $r$ (or $P^-_r\\Lambda^2$),\n$V_h^2 = \\lbrace u \\in H_0(\\mathrm{div},\\Omega);\\; u_{\\vert T} \\in P_r^-\\Lambda^2(T), \\forall T \\in \\mathbf{T}_h \\rbrace$.\n\\item To define the pressure space $V^3_h$ we use discontinuous Galerkin elements of degree $r-1$ (or $P^-_r\\Lambda^3$),\n$V_h^3 = \\lbrace p \\in L^2(\\Omega);\\; p_{\\vert T} \\in P_r^-\\Lambda^3(T), \\forall T \\in \\mathbf{T}_h \\rbrace$.\n\\item The space of discrete harmonic forms $\\mathfrak{H}^3_h = \\mathbb{R} \\subset L^2(\\Omega)$ \nis the $L^2$-orthogonal complement of $\\mathrm{div}(V^2_h)$ in $V^3_h$.\n\\end{itemize}\nWe can substitute the first kind by the second, at the expense of increasing the polynomial degrees.\nNedelec elements of first and second kind are respectively the $3$-dimensional equivalent of Raviart-Thomas and of Brezzi-Douglas-Marini elements.\nThe space $\\mathfrak{H}^3_h$ is just the natural way of fixing the pressure (usually only defined up to an arbitrary constant when it does not appear in boundary conditions).\nFigure \\ref{fig:elements} shows the degrees of freedom of this choice of finite elements for $r = 1$ and $r = 2$.\nThe first element shown on the left corresponds to the space $V^0_h$ that plays no role and will not appear in this paper.\n\n\\begin{remark}\nIn full generality $\\mathfrak{H}^3_h$ is simply the space of the discrete harmonic 3-forms.\nIn this case the space of discrete harmonic $3$-forms is the same as the space of continuous harmonic $3$-forms.\nIf we were to take $V^1 = H_0(\\mathrm{curl},\\Omega)$, \n$V^2 = H_0(\\mathrm{div},\\Omega)$ and $V^3 = L^2(\\Omega)$ instead then we would have $\\mathfrak{H}^3 = \\mathfrak{H}^3_h = \\lbrace 0 \\rbrace$.\n\\end{remark}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.75\\textwidth]{elements}\n\\caption{Degrees of freedom on reference elements for the two lowest polynomial degrees $r = 1$ and $r = 2$.}\n\\label{fig:elements}\n\\end{figure}\n\nFor this choice we can make explicit the approximation properties in terms of the size of the mesh $h$ and of the polynomial degree $r$.\nFor $(\\omega,u,p,\\phi)$ (resp. $(\\omega_h,u_h,p_h,\\phi_h)$) the solution of \\eqref{eq:defmixedcontinuous} \n(resp. \\eqref{eq:defmixeddiscrete}) the estimate of Corollary \\ref{corollary:pertubedestimate} reads:\n\\[\n\\Vert \\omega - \\omega_h \\Vert_{H(\\text{curl})} + \\Vert u - u_h \\Vert_{H(\\text{div})} + \\Vert p - p_h\\Vert_{L^2} + \\Vert \\phi - \\phi_h \\Vert_{L^2} = \\mathcal{O}(h^{r}) .\n\\]\n\n\\subsection{Boundary conditions} \\label{Boundaryconditions}\nWe give a quick review of the boundary conditions readily available.\nLet $n$ be the unit outward normal to the boundary and\n$g$ and $h$ arbitrary functions in a suitable space.\nEqualities below are always understood $\\text{on } \\partial \\Omega$ and\nany combination of the following conditions can be used:\n\\begin{align}\n\\omega \\times n &=\\ g,\\  u \\cdot n =\\ h. \\label{eq:bc1}\u00a0\\\\\n\\omega \\times n &=\\ g,\\  p =\\ h. \\label{eq:bc2} \\\\\nu &=\\ g. \\label{eq:bc3} \\\\\nu \\times n &=\\ g,\\  p =\\ h. \\label{eq:bc4}\n\\end{align}\nThe first two conditions are uncommon since they prescribe the tangential trace of the vorticity.\nThe third is the most commonly used since it allows enforcing the no slip condition \nwith $u \\cdot n = 0$, $u \\times n = 0$.\n\nIn order to implement them, we use essential Dirichlet boundary conditions for: \n\\begin{itemize}\n\\item $\\omega \\times n = g$ and take test functions in $H_0(\\mathrm{curl})$.\n\\item $u \\cdot n = g$ and take test functions in $H_0(\\mathrm{div})$.\n\\end{itemize}\nAnd natural conditions for the other two:\n\\begin{itemize}\n\\item $u \\times n = g$ by adding $- \\int_{\\partial \\Omega} (g \\times \\tau) \\cdot n ds$ to the left-hand side of equations \\eqref{eq:NS_Eulercont}.\n\\item $p = h$ by adding $\\int_{\\partial \\Omega} h v \\cdot n ds$ to the left-hand side of equations \\eqref{eq:NS_Eulercont}.\n\\end{itemize}\n\n\\begin{remark}\nThese are conditions that can be easily implemented. They have the particularity of treating independently the tangential and normal components of the functions.\nAlthough they can all be imposed, only condition \\eqref{eq:bc1} and \\eqref{eq:bc4} gives a complex.\nThe following proofs are based on the use of complexes, so we assume that one of these two conditions is chosen.\nWhen we do not use a complex we still have the well-posedness, however the rate of convergence can be impacted.\nThe case of condition \\eqref{eq:bc3} which is used to impose the no slip condition is studied by Arnold et al.\\ \\cite{ARNOLD2012,Chen2014}. \nHe shows that the rate of convergence on the vorticity (for the norm $H(\\text{curl})$) \nand on the pressure (for the norm $L^2$) is slightly degraded.\nHowever, for a polynomial degree of at least $2$ the rate of convergence of the velocity (for the norm $H(\\text{div})$) is not impacted.\n\\end{remark}\n\n\\subsection{Two-dimensional formulation}\nWhile everything is expressed in $\\mathbb{R}^3$ it is perfectly possible to apply this to a $2$-dimensional problem.\nOne shall take the Raviart-Thomas's face elements (and not edge since we wish to preserve a pointwise zero divergence) or Brezzi-Douglas-Marini's face elements for $V^2_h$.\nThe meaning of $\\omega$ in \\eqref{eq:weakl1} is then $\\omega = \\nabla \\times u = \\partial_1 u_2 - \\partial_2 u_1$ and the expressions appearing in \\eqref{eq:weakl2} become:\n\\[\n\\nabla \\times \\omega = \\begin{bmatrix} \\partial_2 \\omega \\\\ - \\partial_1 \\omega \\end{bmatrix} \\text{and } \\omega \\times u = \\omega u^\\perp = \\omega \n\\begin{bmatrix} - u_2 \\\\ u_1 \\end{bmatrix}.\n\\]\n\nEverything said and proved below also works in two dimensions.\nThe independence w.r.t.\\ dimension is a major advantage of exterior calculus formalism.\n\n\\subsection{Notations} \\label{Notations}\u00a0\nFinally we state the full framework of exterior calculus which will be used.\nWe make extensive use of the following notations introduced by D. Arnold \\cite{feec-cbms}. \nThey are explained in greater detail in the original reference.\n\n\\begin{itemize}\n\\item $d$ is the exterior derivative, $d^*$ its adjoint or codifferential,\n\\item $W^0 \\rightarrow W^1 \\rightarrow W^2 \\rightarrow W^3$ is the $L^2$ de Rham complex of a bounded domain of $\\mathbb{R}^3$.\nWe shall only use the last three of them,\n\\item $V^0 \\rightarrow V^1 \\rightarrow V^2 \\rightarrow V^3$ is a dense subcomplex on which the exterior derivative is defined \n(and not just densely defined). Here we have $H^1(\\Omega) \\rightarrow H(\\mathrm{curl},\\Omega) \\rightarrow H(\\mathrm{div},\\Omega) \\rightarrow L^2(\\Omega)$.\n\\item $V^*_k$ is the domain of the adjoint of $(d,V^k)$,\n\\item $\\Vert \\cdot \\Vert$ is the $L^2$-norm for scalar or vector valued functions,\n\\item $\\Vert \\cdot \\Vert_V$ is the $V$ norm, defined by $\\Vert \\cdot \\Vert_V = \\Vert \\cdot \\Vert + \\Vert d \\cdot \\Vert$,\n\\item Sometimes we take the norm on $V \\cap V^*$, it is given by $\\Vert \\cdot \\Vert_V + \\Vert d^* \\cdot \\Vert$,\n\\item $V^0_h \\rightarrow V^1_h \\rightarrow V^2_h \\rightarrow V^3_h$ is a discrete subcomplex parametrized by $h$,\n\\item $P_h$ is the $L^2$-orthogonal projection on the discrete subcomplex and in general $P_A$ is the $L^2$-orthogonal projection on $A$.\n\\end{itemize}\n\nWe assume that our complexes have the compactness property, \nwhich means that the inclusion $V^k \\cap V_k^* \\subset W^k$ is compact for each $k$.\\\\\nWe also assume that there exists a cochain projection $\\pi_h^k: V^k \\rightarrow V^k_h$, bounded for the $W$-norm, uniformly in $h$.\\\\\nEach space has the Hodge-decomposition $W^k = \\mathfrak{B}^k \\mathbin{\\raisebox{-1pt}{\\operpsymbol}} \\mathfrak{H}^k \\mathbin{\\raisebox{-1pt}{\\operpsymbol}}\u00a0\\mathfrak{B}^*_k$,\nwhere $\\mathfrak{B}^k = d (V^{k-1})$, $\\mathfrak{B}^*_k = d^*(V^*_{k+1})$ \nand $\\mathfrak{H}^k = \\mathfrak{Z}^k \\cap (\\mathfrak{B}^k)^\\perp$, $\\mathfrak{Z}^k$ being the kernel of $d: V^k \\rightarrow V^{k+1}$.\n\nIn order to measure the approximation properties of the discrete subspaces we introduce the notation $E = E^k$,\ndefined by:\n\\begin{equation} \\label{eq:approxprop}\n\\forall k, \\forall \\sigma \\in V^k, E^k(\\sigma) := \\inf_{\\tau \\in V^k_h} \\Vert \\sigma - \\tau \\Vert\\ .\n\\end{equation}\n\nThe choice of spaces introduced in Section \\ref{defaultspaces} does indeed verify all these properties\nwith $E = \\mathcal{O}(h^r)$ on a dense subset (see \\cite{Arnold_2010}).\nEach space has a Poincar\u00e9 inequality (see \\cite[Theorem~4.6]{feec-cbms}). \nThat is to say, for each $k$, $\\exists c_p^k > 0$ such that $\\forall z \\in \\mathfrak{Z}^{k \\perp_{V^k}}$, $\\Vert z \\Vert_{V^k} \\leq c_p^k \\Vert d z \\Vert_{L^2}$.\nMoreover this also holds for the discrete subcomplex with constants bounded by $c_p^k\\, \\Vert \\pi_h^k \\Vert_{W^k}$ (see \\cite[Theorem~5.3]{feec-cbms}).\nWe set $c_p = \\max_{k,h} c_p^k\\, \\Vert \\pi_h^k \\Vert_{W^k}$, so that the Poincar\u00e9 inequality holds for $c_p$ regardless of the space.\n\nWhen it is obvious from the context we shall drop the exponent.\nHence, when applied to a $1$-form $v_1 \\in V^1$ we mean $d v_1 := \\text{curl} (v_1)$,\nand when applied to a $2$-form $v_2 \\in V^2$ we mean $d v_2 := \\text{div} (v_2)$.\nWhen we apply an operator (such as $\\pi_h$, $d$ or $d^*$) \nto a product space, we mean to apply it to each component (i.e.\\ $d (v_1,v_2,v_3) := (d v_1, d v_2, 0)$).\nWhen we add a suffix $_h$ such as $d_h$ instead of $d$ we refer \nto the discrete counterpart of the object.\\\\\nWe will often add a numerical suffix such as $z_2$ for $z \\in V^2 \\times V^3$,\nthis means we take the $V^2$ component of $z$ and will be clear from the context.\n\nUsually when dealing with a primal formulation\nwe will use the variable $(u,p)$, and they can indeed be seen as the velocity and pressure.\nHowever, when we deal with mixed formulations we will frequently write $(u_1,u_2,u_3)$. \nHere $1$,$2$ and $3$ refer to 1-form, 2-form and 3-form and have nothing to do with \ncomponents in a frame of the velocity field.\nSpecifically we will have the identification $u_1 = \\omega$, $u_2 = u$ and $u_3 = p$.\n\nThe symbol $A \\lesssim B$ means that there exists a constant $C \\in \\mathbb{R}_+^*$ independent of $A$ and $B$ \n(depending solely on few specified parameters) such that $A \\leq C B$.\n\n\\section{Linear steady problem} \\label{Unpertubedproblem}\nWe first study a simpler problem, analogous to a Stokes problem, which is also \nclosely related to the Hodge-Laplace problem (see \\cite{feec-cbms}).\nGiven $f = (f_2,f_3) \\in W^2 \\times W^3$, the problem is:\\\\\nFind $(u,p)$ such that \n\\begin{equation} \\label{eq:linearsteadypnotations}\n\\begin{aligned}\n\\nu \\nabla \\times (\\nabla \\times u) + \\nabla p &= f_2 \\text{ on } \\Omega,\\\\\n\\nabla \\cdot u &= f_3 \\text{ on } \\Omega .\n\\end{aligned}\n\\end{equation}\n\\subsection{Primal formulation}\nWe give the primal formulation in order to explain more clearly what we seek in the mixed formulation,\nand because we shall need this operator in Section \\ref{Pertubedproblem}.\nSince we have no way to reach the harmonic part of $f_2$, we must include a second harmonic space (this time of $2$-forms).\n\n\\begin{definition}\nLet $D_0 := \\lbrace (u,p) \\in (V^2 \\cap V^*_2) \\times V^*_3 \\vert\\  d^*u \\in V^1 \\rbrace$\n, $f \\in W^2 \\times W^3$ and\n\\begin{equation} \\label{eq:L0}\nL_0 := \\begin{bmatrix}\n\\nu d d^* & - d^*\\\\\n d & 0 \n\\end{bmatrix} =\n\\begin{bmatrix}\n\\nu \\nabla \\times \\nabla \\times &  \\nabla\\\\\n\\nabla \\cdot & 0\n\\end{bmatrix}.\n\\end{equation}\n\\end{definition}\nThe problem is to find $(u,p) \\in P_{\\mathfrak{H}^\\perp} D_0$, such that\n\\begin{equation} \n\\forall (v,q) \\in P_{\\mathfrak{H}^\\perp} D_0, \\langle L_0 (u,p),(v,q) \\rangle = \\langle f,(v,q)\\rangle. \n\\end{equation}\n\n\\begin{remark}\nSince $d: V^3 \\rightarrow 0$, the Hodge decomposition on $W^3$ reads\n$W^3 = \\mathfrak{B}^3 \\mathbin{\\raisebox{-1pt}{\\operpsymbol}} \\mathfrak{H}^3$.\nWe also have the equality between the $V$-norm and the $W$-norm.\n\\end{remark}\n\\subsection{Continuous Well-posedness}\nNow we introduce the mixed formulation and switch to the exterior calculus notation.\nRecall that $u_2$ (resp. $u_3$) defined below corresponds to $u$ (resp. $p$) in \\eqref{eq:linearsteadypnotations}.\nThe mixed formulation is characterized by the introduction of the auxiliary variable $u_1$ corresponding to $\\nabla \\times u_2$ or $d^* u_2$.\nThe problem reads: \\\\\nGiven $(f_2,f_3) \\in W^2 \\times W^3$,\nfind $(u_1,u_2,u_3,\\phi_2,\\phi_3) \\in V^1 \\times V^2 \\times V^3 \\times \\mathfrak{H}^2 \\times \\mathfrak{H}^3 $ such that $\\forall (v_1,v_2,v_3,\\chi_2,\\chi_3) \\in V^1 \\times V^2 \\times V^3 \\times \\mathfrak{H}^2 \\times \\mathfrak{H}^3$,\n\n\\begin{equation} \\label{eq:B0}\n\\begin{aligned}\n\\langle u_1,v_1\\rangle  - \\langle u_2,d v_1\\rangle  =&\\ 0,\\\\\n\\nu \\langle  d u_1,v_2\\rangle  - \\langle u_3, d v_2\\rangle + \\langle \\phi_2,v_2\\rangle =&\\ \\langle f_2,v_2\\rangle, \\\\\n\\langle d u_2,v_3\\rangle+ \\langle \\phi_3,v_3\\rangle =&\\ \\langle f_3,v_3 \\rangle, \\\\\n\\langle u_2,\\chi_2\\rangle  + \\langle u_3,\\chi_3\\rangle  =&\\ 0.\\\\\n\\end{aligned}\n\\end{equation}\nThe associated bilinear form is noted $B_0$.\n\n\\begin{remark} \\label{rem:removedstar}\nThere are two important reasons to remove $d^*$ from the formulation.\nThey become clear when we will consider the discrete counterpart of the formulation. \nThe first one is that $d^*$ and $d_h^*$ are barely related, and it is much harder to analyze the difference between them than between $d$ and $d_h$.\nThe second one is because while $d_h$ is a local operator, $d_h^*$ is in general a global operator. \nThis greatly deteriorates the sparsity pattern of the system.\n\\end{remark}\n\n\\begin{lemma} \\label{lemma:discretewp1}\nThere is $\\alpha > 0$, such that $\\forall (u_1,u_2,u_3,\\phi_2,\\phi_3) \\in V^1 \\times V^2 \\times V^3 \\times \\mathfrak{H}^2 \\times \\mathfrak{H}^3$ we have:\n\\[\n\\begin{aligned}\n\\sup_{(v_1,v_2,v_3,\\chi_2,\\chi_3)} &\\frac{B_0((u_1,u_2,u_3,\\phi_2,\\phi_3),(v_1,v_2,v_3,\\chi_2,\\chi_3))}{\\Vert (v_1,v_2,v_3,\\chi_2,\\chi_3) \\Vert_V} \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\geq \\alpha \\Vert (u_1,u_2,u_3,\\phi_2,\\phi_3) \\Vert_V.\n\\end{aligned}\n\\]\n\\end{lemma}\n\n\\begin{proof}\nFor any $(u_1,u_2,u_3,\\phi_2,\\phi_3) \\in V^1 \\times V^2 \\times V^3 \\times \\mathfrak{H}^2 \\times \\mathfrak{H}^3$, \nlet $\\rho_1 \\in \\mathfrak{B}^{*}_1 \\cap V^1$ be such that $d \\rho_1 = P_{\\mathfrak{B}} u_2$,\n$\\rho_2 \\in \\mathfrak{B}^{*}_2 \\cap V^2$ be such that $d \\rho_2 = P_{\\mathfrak{B}} u_3 + d u_2$.\nAnd take $v_1 = \\nu u_1 - \\frac{\\nu}{c_p^2} \\rho_1$,\n$v_2 = P_{\\mathfrak{B}} u_2 + d u_1 - \\rho_2 + \\phi_2$,\n$v_3 = d u_2 - P_{\\mathfrak{B}} u_3 + \\phi_3$,\n$\\chi_2 = P_{\\mathfrak{H}} u_2$,\n$\\chi_3 = P_{\\mathfrak{H}} u_3$.\nThe Poincar\u00e9 inequality gives:\n\\begin{align}\n\\Vert \\rho_1 \\Vert_{V^1} \\leq& c_p \\Vert P_{\\mathfrak{B}} u_2 \\Vert, \\label{eq:Poincare_rho1} \\\\\n\\Vert \\rho_2 \\Vert_{V^2} \\leq& c_p \\Vert P_{\\mathfrak{B}} u_3 \\Vert + c_p \\Vert d u_2 \\Vert. \\label{eq:Poincare_rho2}\n\\end{align}\nAnd we easily see that:\n\\[\n\\Vert v_1 \\Vert_{V^1} + \\Vert v_2 \\Vert_{V^2} + \\Vert v_3 \\Vert_{V^3} + \\Vert \\chi_2 \\Vert + \\Vert \\chi_3 \\Vert \\lesssim\n\\Vert u_1 \\Vert_{V^1} + \\Vert u_2 \\Vert_{V^2} + \\Vert u_3 \\Vert_{V^3} + \\Vert \\phi_2 \\Vert + \\Vert \\phi_3 \\Vert,\n\\]\nwhere the hidden constant only depends on $\\nu$ and $c_p$.\nUsing orthogonality of the Hodge decomposition we get:\n\\begin{equation} \\label{eq:B0_wp}\n\\begin{aligned}\nB_0((u_1,u_2&,u_3,\\phi_2,\\phi_3),(v_1,v_2,v_3,\\chi_2,\\chi_3))\\\\\n&=\\langle u_1,\\nu u_1 - \\frac{\\nu}{c_p^2} \\rho_1\\rangle  - \\langle u_2,d (\\nu u_1 - \\frac{\\nu}{c_p^2} \\rho_1)\\rangle  \\\\\n&+ \\nu \\langle  d u_1,P_{\\mathfrak{B}} u_2 + d u_1 - \\rho_2 + \\phi_2\\rangle  - \\langle u_3, d (P_{\\mathfrak{B}} u_2 + d u_1 - \\rho_2 + \\phi_2)\\rangle  \\\\\n&+ \\langle d u_2,d u_2 - P_{\\mathfrak{B}} u_3 + \\phi_3\\rangle + \\langle u_2,P_{\\mathfrak{H}} u_2\\rangle  + \\langle u_3,P_{\\mathfrak{H}} u_3\\rangle  \\\\\n&+ \\langle \\phi_2,P_{\\mathfrak{B}} u_2 + d u_1 - \\rho_2 + \\phi_2\\rangle  + \\langle \\phi_3,d u_2 - P_{\\mathfrak{B}} u_3 + \\phi_3\\rangle\\\\\n&= \\nu \\langle u_1, u_1\\rangle  - \\frac{\\nu}{c_p^2} \\langle u_1, \\rho_1\\rangle  - \\nu \\langle u_2,d u_1\\rangle  + \\frac{\\nu}{c_p^2}\\langle u_2, P_{\\mathfrak{B}} u_2\\rangle  \\\\\n&+ \\nu \\langle  d u_1,u_2\\rangle  + \\nu \\langle d u_1,d u_1\\rangle  + \\langle u_3,P_{\\mathfrak{B}} u_3\\rangle + \\langle u_3, d u_2 \\rangle  \\\\\n&+ \\langle d u_2,d u_2 \\rangle - \\langle d u_2, u_3 \\rangle + \\langle u_2,P_{\\mathfrak{H}} u_2\\rangle  + \\langle u_3,P_{\\mathfrak{H}} u_3\\rangle  \\\\\n&+ \\langle \\phi_2, \\phi_2\\rangle  + \\langle \\phi_3,\\phi_3\\rangle \\\\\n&= \\nu \\Vert u_1 \\Vert ^2 - \\frac{\\nu}{c_p^2} \\langle u_1, \\rho_1\\rangle \n+ \\frac{\\nu}{c_p^2} \\Vert P_{\\mathfrak{B}} u_2 \\Vert ^2\n+ \\nu \\Vert d u_1 \\Vert ^2 + \\Vert P_{\\mathfrak{B}} u_3 \\Vert ^2\\\\\n&+ \\Vert d u_2 \\Vert ^2\n+ \\Vert P_{\\mathfrak{H}} u_2 \\Vert^2 + \\Vert P_{\\mathfrak{H}} u_3 \\Vert ^2\n+ \\Vert \\phi_2 \\Vert ^2 + \\Vert \\phi_3 \\Vert ^2 . \n\\end{aligned}\n\\end{equation}\n\nNow since $\\langle u_1,\\rho_1\\rangle  \\leq \\Vert u_1 \\Vert \\Vert \\rho_1 \\Vert \\leq \\frac{c_p^2}{2} \\Vert u_1 \\Vert ^2 + \\frac{1}{2 c_p^2} \\Vert \\rho_1 \\Vert ^2$,\nusing \\eqref{eq:Poincare_rho1} and injecting in the expression \\eqref{eq:B0_wp} we get:\n\\\n\\begin{aligned}\nB_0((u_1,&u_2,u_3,\\phi_2,\\phi_3),(v_1,v_2,v_3,\\chi_2,\\chi_3)) \\geq \\\\\n&\\frac{\\nu}{2} \\Vert u_1 \\Vert ^2 + \\frac{\\nu}{2 c_p^2} \\Vert P_{\\mathfrak{B}} u_2 \\Vert ^2\n+ \\nu \\Vert d u_1 \\Vert ^2 + \\Vert u_3 \\Vert ^2\n+ \\Vert d u_2 \\Vert ^2\\\\\n&+ \\Vert P_{\\mathfrak{H}} u_2 \\Vert^2 + \\Vert P_{\\mathfrak{H}} u_3 \\Vert ^2\n+ \\Vert \\phi_2 \\Vert ^2 + \\Vert \\phi_3 \\Vert ^2 . \n\\end{aligned}\n\\\n\nFinally, since $d u_3 = 0$, $d \\phi_2 = d \\phi_3 = 0$ and $\\Vert P_{\\mathfrak{B}^*} u_2 \\Vert \\leq c_p \\Vert d P_{\\mathfrak{B}^*} u_2 \\Vert = \\Vert d u_2 \\Vert $ we have:\n\\[B_0((u_1,u_2,u_3,\\phi_2,\\phi_3),(v_1,v_2,v_3,\\chi_2,\\chi_3)) \\gtrsim\n\\Vert (u_1,u_2,u_3,\\phi_2,\\phi_3) \\Vert ^2 _V, \\]\nwhere the hidden constant depends only on $c_p$ and $\\nu$.\n\\end{proof}\n\n\\begin{lemma} \\label{lemma:discretewp2}\nGiven any $(v_1,v_2,v_3,\\chi_2,\\chi_3) \\in V^1 \\times V^2 \\times V^3 \\times \\mathfrak{H}^2 \\times \\mathfrak{H}^3$ there is \n$(u_1,u_2,u_3,\\phi_2,\\phi_3) \\in V^1 \\times V^2 \\times V^3 \\times \\mathfrak{H}^2 \\times \\mathfrak{H}^3$ such that \n\\[B_0((u_1,u_2,u_3,\\phi_2,\\phi_3),(v_1,v_2,v_3,\\chi_2,\\chi_3)) > 0\\ .\n\\]\n\\end{lemma}\n\\begin{proof}\nIf $P_{\\mathfrak{B}} v_2 \\neq 0$ take $u_1 \\in \\mathfrak{B}^*_1$ such that $d u_1 = P_\\mathfrak{B} v_2$, $\\phi_2 = 0$, $u_3 = \\phi_3 = 0$. \nThen if $\\langle u_1,v_1 \\rangle = 0$ take $u_2 = 0$ else take $u_2 = d v_1 \\frac{\\langle u_1,v_1 \\rangle}{\\langle d v_1, d v_1 \\rangle}$ \n($d v_1 \\neq 0$ since $P_{\\mathfrak{B}^*} v_1 \\neq 0$ as $\\langle u_1,v_1 \\rangle \\neq 0$).\nIf $P_\\mathfrak{B} v_2 = 0$, simply take $u_1 = v_1$,\n$u_2$ such that $d u_2 = P_{\\mathfrak{B}} v_3, P_{\\mathfrak{B}} u_2 = - d v_1, P_{\\mathfrak{H}} u_2 = \\chi_2$ (this is possible by the Hodge decomposition),\n$u_3$ such that $P_{\\mathfrak{B}} u_3 = - d v_2, P_{\\mathfrak{H}} u_3 = \\chi_3$,\n$\\phi_2 = P_{\\mathfrak{H}} v_2$ and\n$\\phi_3 = P_{\\mathfrak{H}} v_3$.\n\\end{proof}\n\nLemma \\ref{lemma:discretewp1} together with Lemma \\ref{lemma:discretewp2} give the conditions to apply the \\\\\nBabu\u0161ka\u2013Lax\u2013Milgram theorem.\nThis proves the continuous well-posedness.\nMoreover, we have:\n\\begin{equation}\n\\Vert u_1 \\Vert_{V^1} + \\Vert u_2 \\Vert_{V^2} + \\Vert u_3 \\Vert_{V^3} + \\Vert \\phi_2 \\Vert_{V^2} + \\Vert \\phi_3 \\Vert_{V^3} \\leq c \\Vert f \\Vert\n\\end{equation}\nwhere $c$ depends only on $c_p$ and $\\nu$.\n\\subsection{Discrete well-posedness} \nThe discrete problem reads: \\newline\nGiven $(f_2,f_3) \\in W^2 \\times W^3$,\nfind $(u_{1h},u_{2h},u_{3h},\\phi_{2h},\\phi_{3h}) \\in V^1_h \\times V^2_h \\times V^3_h \\times \\mathfrak{H}^2_h \\times \\mathfrak{H}^3_h $ such that \n$\\forall (v_{1h},v_{2h},v_{3h},\\chi_{2h},\\chi_{3h}) \\in V^1_h \\times V^2_h \\times V^3_h \\times \\mathfrak{H}^2_h \\times \\mathfrak{H}^3_h$,\n\n\\begin{equation} \\label{eq:discretelinearsteady}\nB_0((u_{1h},u_{2h},u_{3h},\\phi_{2h},\\phi_{3h}),(v_{1h},v_{2h},v_{3h},\\chi_{2h},\\chi_{3h})) = \\langle f_2,v_{2h}\\rangle + \\langle f_3,v_{3h} \\rangle.\n\\end{equation}\n\nSince we have a discrete Poincar\u00e9 inequality (see \\cite{feec-cbms}) and use a subcomplex \nwe can apply exactly the same proof as in the continuous case, \nsubstituting continuous spaces for their discrete counterparts.\n\\begin{remark}\nThe only differential operator used in the mixed formulation is $d$ and\nits discrete counterpart is merely its restriction to the discrete space.\nHowever since the discrete space of harmonic forms $\\mathfrak{H}_h$ is not necessarily a subspace of $\\mathfrak{H}$.\nWhen $\\mathfrak{H}^2$ or $\\mathfrak{H}^3$ is not zero, the method may be non-conforming.\n\\end{remark}\n\\subsection{Error estimate}\nWe will only derive a basic error estimate for a global norm here.\nFirst we define: \n\\begin{equation} \\label{eq:mu}\n\\mu := \\max_{k \\in \\lbrace 2,3 \\rbrace} \\sup_{r \\in \\mathfrak{H}^k, \\Vert r \\Vert = 1} \\Vert (I - \\pi_h^k) r \\Vert.\n\\end{equation}\n\\begin{theorem} \\label{th:unpertubedestimate}\nGiven $(f_2,f_3) \\in W^2 \\times W^3$,\nlet $(u_1,u_2,u_3,\\phi_2,\\phi_3)$ be the solution of the continuous problem \\eqref{eq:B0} and\n$(u_{1h},u_{2h},u_{3h},\\phi_{2h},\\phi_{3h})$ the solution of the discrete problem \\eqref{eq:discretelinearsteady},\nthen for $E$ given by \\eqref{eq:approxprop} it holds:\n\\\n\\begin{aligned}\n\\Vert& (u_1 - u_{1h},u_2 - u_{2h},u_3 - u_{3h}) \\Vert_V + \\Vert (\\phi_2 - \\phi_{2h},\\phi_3 - \\phi_{3h}) \\Vert \\\\ \n& \\lesssim \\inf_{v_1\u00a0\\in V^1_h} \\Vert u_1 - v_1 \\Vert_{V^1} + \\inf_{v_2\u00a0\\in V^2_h} \\Vert u_2 - v_2 \\Vert_{V^2}  + E(u_3) + E(\\phi_2) + E(\\phi_3) \\\\\n& \\ + \\mu (E(P_\\mathfrak{B} u_2) + E(P_\\mathfrak{B} u_3)).\n\\end{aligned}\n\\\n\\end{theorem}\n\\begin{proof}\nFor all $(v_{1h},v_{2h},v_{3h},\\chi_{2h},\\chi_{3h}) \\in V^1_h \\times V^2_h \\times V^3_h \\times \\mathfrak{H}^2_h \\times \\mathfrak{H}^3_h$\nwe have:\n\\\n\\begin{aligned}\nB_0((u_1,u_2,u_3,\\phi_2,\\phi_3),(v_{1h},&v_{2h},v_{3h},\\chi_{2h},\\chi_{3h})) \\\\\n&= \\langle f_2,v_{2h}\\rangle + \\langle f_3,v_{3h}\\rangle + \\langle u_2,\\chi_{2h}\\rangle + \\langle u_3,\\chi_{3h}\\rangle\\ .\n\\end{aligned}\n\\\n\nLet $(v_{1},v_{2},v_{3},\\chi_{2},\\chi_{3})$ be the $V$-orthogonal projection \nof $(u_{1},u_{2},u_{3},\\phi_{2},\\phi_{3})$ into their respective discrete spaces.\nBy the continuity of $B_0$ it holds,\\\\\n$\\forall (v_{1h},v_{2h},v_{3h},\\chi_{2h},\\chi_{3h}) \\in V^1_h \\times V^2_h \\times V^3_h \\times \\mathfrak{H}^2_h \\times \\mathfrak{H}^3_h$\n\\\n\\begin{aligned}\n&B_0((u_{1h}-v_1,u_{2h}-v_2,u_{3h}-v_3,\\phi_{2h}-\\chi_2,\\phi_{3h}-\\chi_3),\n(v_{1h},v_{2h},v_{3h},\\chi_{2h},\\chi_{3h})) \\\\\n&\\quad= B_0((u_{1}-v_1,u_{2}-v_2,u_{3}-v_3,\\phi_{2}-\\chi_2,\\phi_{3}-\\chi_3) ,(v_{1h},v_{2h},v_{3h},\\chi_{2h},\\chi_{3h}))\\\\\n&\\quad\\ - \\langle u_2,\\chi_{2h}\\rangle - \\langle u_3,\\chi_{3h}\\rangle \\\\\n&\\quad= B_0((u_{1}-v_1,u_{2}-v_2,u_{3}-v_3,\\phi_{2}-\\chi_2,\\phi_{3}-\\chi_3) ,(v_{1h},v_{2h},v_{3h},\\chi_{2h},\\chi_{3h}))\\\\\n&\\quad\\ - \\langle P_{\\mathfrak{H}_h} u_2,\\chi_{2h}\\rangle - \\langle P_{\\mathfrak{H}_h} u_3,\\chi_{3h}\\rangle \\\\\n&\\quad\\lesssim (\\Vert u_{1}-v_1 \\Vert_{V^1} + \\Vert u_{2}-v_2\\Vert_{V^2} +\\Vert u_{3}-v_3\\Vert + \\Vert \\phi_{2}-\\chi_2\\Vert + \\Vert \\phi_{3}-\\chi_3\\Vert\\\\\n&\\quad\\ + \\Vert P_{\\mathfrak{H}_h} u_2 \\Vert_{V^2}+ \\Vert P_{\\mathfrak{H}_h} u_3 \\Vert_{V^3})\n(\\Vert v_{1h} \\Vert_{V^1} + \\Vert v_{2h} \\Vert_{V^2} + \\Vert v_{3h} \\Vert + \\Vert \\chi_{2h}\\Vert + \\Vert \\chi_{3h}\\Vert) ,\n\\end{aligned}\n\\\nwhere the hidden constant only depends on $\\nu$.\n\nFrom the discrete inf-sup condition we have \n\\begin{equation*}\n\\begin{aligned}\n(\\Vert u_{1h}-v_1 \\Vert_{V^1} &+ \\Vert u_{2h}-v_2\\Vert_{V^2} +\\Vert u_{3h}-v_3\\Vert + \\Vert \\phi_{2h}-\\chi_2\\Vert + \\Vert \\phi_{3h}-\\chi_3\\Vert) \\\\\n&\\lesssim (\\Vert u_{1}-v_1 \\Vert_{V^1} + \\Vert u_{2}-v_2\\Vert_{V^2} +\\Vert u_{3}-v_3\\Vert + \\Vert \\phi_{2}-\\chi_2\\Vert\\\\\n&\\ \\ \\ + \\Vert \\phi_{3}-\\chi_3\\Vert + \\Vert P_{\\mathfrak{H}_h} u_2 \\Vert_{V^2}+ \\Vert P_{\\mathfrak{H}_h} u_3 \\Vert),\n\\end{aligned}\n\\end{equation*}\nwhere the hidden constant only depends on $\\nu$ and the discrete constant of Poincar\u00e9.\n\nThe theorem follows from \n\\[ \\Vert P_{\\mathfrak{H}_h} u_i \\Vert_V \\lesssim \\mu E(P_\\mathfrak{B} u_i), \\forall i \\in \\lbrace 2,3 \\rbrace. \n\\]\nThis is proved in Theorem 5.2 of \\cite{feec-cbms}.\n\\end{proof}\n\\begin{remark}\n$E(\\phi_i)$ is understood as viewing $\\phi_i$ as an element of $V^i \\supset \\mathfrak{H}^i$,\n\\[ E(\\phi_i) = \\inf_{q \\in V^i_h} \\Vert \\phi_i - q \\Vert. \n\\]\n\\end{remark}\n\n\\section{Linearized problem} \\label{Pertubedproblem} \nStarting from the linear steady problem \\eqref{eq:B0},\nwe add some terms of lower order since\nthis will allow us to construct our scheme. \\newline\nAgain we recall the correspondence with the names used for variables in the introduction: $(u_1,u_2,u_3,u_p) \\equiv (\\omega,u,p,\\phi)$\nin \\eqref{eq:introformulation}.\nTo keep the notations bearable we shall also write $u_{\\mathfrak{B}} = P_{\\mathfrak{B}} u$, $u_{\\mathfrak{H}} = P_{\\mathfrak{H}} u$ and so on.\n\nWe consider two linear maps  $l_3: W^1 \\rightarrow W^2$ and $l_5: W^2 \\rightarrow W^2$ \n(we chose these names to match those used by D. Arnold \\cite{Arnold2016}).\nDefine $D = \\lbrace (u,p) \\in (V^2 \\cap V^*_2) \\times (V^*_3 \\cap \\mathfrak{H}^{3\\perp}) \\vert \\ d^* u \\in V^1 \\rbrace$\n, $W = W^2 \\times (W^3 \\cap \\mathfrak{H}^{3 \\perp})$,\n\\begin{equation} \\label{eq:Llambda}\nL := \\begin{bmatrix}\n(\\nu d + l_3) d^* + l_5 & - d^*\\\\\nd & 0 \n\\end{bmatrix}.\n\\end{equation}\nWe consider the primal problem:\\\\\nGiven $f \\in W$, to find $(u,p) \\in D$ such that \n\\begin{equation} \\label{eq:defmixedprimal}\nL (u,p) = f.\n\\end{equation}\nWe also define the dual operator $L'$ on $D' = \\lbrace (u,p) \\in (V^2 \\cap V^*_2) \\times (V^*_3 \\cap \\mathfrak{H}^{3\\perp}) \\vert\\  (\\nu d^* + l_3^*) u \\in V^1 \\rbrace$ by:\n\\begin{equation} \\label{eq:Llambdap}\nL' := \\begin{bmatrix}\nd (\\nu d^* + l_3^*) + l_5^* & d^*\\\\\n- d & 0 \n\\end{bmatrix}\n\\end{equation}\n\nAs an intermediary step, we wish to extend $L$ on a larger domain and introduce the following notations:\n\\begin{equation} \\label{eq:defweakL}\n\\begin{gathered}\n\\widebar{L_\\lambda}: (V^2 \\cap V^*_2) \\times (V^*_3 \\cap \\mathfrak{H}^{3\\perp}) \\rightarrow (V^*_2 \\times (W^3 \\cap \\mathfrak{H}^{3 \\perp}))', \\\\\n\\widebar{L_\\lambda}(u,p)(v,q) := \\langle \\nu d^* u,d^* v \\rangle + \\langle l_3 d^* u + l_5 u - \\lambda d^* p,v \\rangle + \\langle \\lambda d u,q \\rangle,\\\\\n\\widebar{L_\\lambda'}: (V^2 \\cap V^*_2) \\times (V^*_3 \\cap \\mathfrak{H}^{3\\perp}) \\rightarrow (V^*_2 \\times (W^3 \\cap \\mathfrak{H}^{3 \\perp}))',\\\\\n\\widebar{L_\\lambda'}(u,p)(v,q) := \\langle \\nu d^* u + l_3^* u, d^* v \\rangle + \\langle l_5^* u + \\lambda d^* p,v \\rangle + \\langle -\\lambda d u,q \\rangle.\n\\end{gathered}\n\\end{equation}\nHere $\\lambda$ is a positive parameter introduced to simplify the proof of Theorem \\ref{th:Liso}.\nIn the Theorem \\ref{th:Liso} we shall see that they are almost always isomorphisms.\nWe define the solution operator $K = (\\widebar{L_1'})^{-1}$, and we assume that \n\\begin{equation} \\label{eq:regularity35}\nd^* (\\widebar{L_1}^{-1})_2 (W) \\subset V^1, (\\nu d^* + l_3^*) (K)_2 (W) \\subset V^1 \n\\end{equation}\nwhere $(\\widebar{L_1}^{-1})_2$ and $(K)_2$ are the projections on the first component of the product space taken after the operators.\nMoreover, we assume that $\\Vert d d^* (K)_2 \\Vert_{W \\rightarrow W^2} $ and $\\Vert d l_3^* (K)_2 \\Vert_{W \\rightarrow W^2}$ are bounded.\nWe show in Section \\ref{Regularityassumptions} that these assumptions are satisfied when $l_3$ and $l_5$ are those used in our scheme.\n\nThe proof follows the same outline as \\cite{Arnold2016}.\nFirst we prove that the continuous primal formulation gives an isomorphism,\nthen we prove that the continuous mixed formulation is well-posed.\nLastly we prove the well-posedness of the discrete mixed formulation\nand give an estimation of the error in energy norm.\n\\subsection{Continuous primal formulation}\n\\begin{theorem} \\label{th:Liso}\n$\\widebar{L_\\lambda} + \\mu \\langle \\cdot , \\cdot \\rangle$ is a bounded isomorphism for all $\\mu \\in \\mathbb{C}$ except for a countable subset.\n\\end{theorem}\n\n\\begin{proof}\nLet $c = max(\\Vert l_3 \\Vert, \\Vert l_5 \\Vert)$,\nfor $(u,p) \\in (V^2 \\cap V^*_2) \\times (V^*_3 \\cap \\mathfrak{H}^{3\\perp})$\ntake \n$v_\\mathfrak{B} = u_\\mathfrak{B}$,\n$v_\\mathfrak{H} = u_\\mathfrak{H}$,\n$v_{\\mathfrak{B}^*} = - d^* p$,\n$q = d u$. We have:\n\\begin{equation} \\label{eq:Lextendproof}\n\\begin{aligned}\n(\\mu \\langle \\cdot , \\cdot \\rangle + \\widebar{L_\\lambda})(u,p)&(v,q) \\\\\n=&\\ \\nu \\langle d^* u,d^*u\\rangle  + \\lambda \\langle d^*p,d^*p\\rangle  + \\lambda \\langle d u,d u\\rangle  \\\\\n&\\  + \\langle l_3 d^* u,u_\\mathfrak{B} - d^* p + u_\\mathfrak{H}\\rangle  + \\langle l_5 u, u_\\mathfrak{B} - d^* p + u_{\\mathfrak{H}}\\rangle  \\\\\n&\\  + \\mu \\langle u,u_\\mathfrak{B} + u_\\mathfrak{H}\\rangle  - \\mu \\langle u,d^* p\\rangle  + \\mu\\langle p,d u\\rangle  \\\\\n=&\\ \\nu \\Vert d^* u\\Vert^ 2+ \\lambda \\Vert d^* p||^2 + \\lambda \\Vert d u \\Vert ^2 +  \\mu \\Vert u_\\mathfrak{B} + u_\\mathfrak{H} \\Vert ^2 \\\\\n&\\  + \\langle l_3 d^* u,u_\\mathfrak{B} -d^*p +u_\\mathfrak{H} \\rangle  + \\langle l_5 u,u_\\mathfrak{B} - d^* p + u_\\mathfrak{H}\\rangle. \n\\end{aligned}\n\\end{equation}\n\nWith the Cauchy\u2013Schwarz inequality and the identity $a b \\leq 2^{-1} (a^2 + b^2)$ we bound the last line from \\eqref{eq:Lextendproof}:\n\\\n\\begin{aligned}\n\\vert \\langle l_3 d^* u,&u_\\mathfrak{B} -d^*p +u_\\mathfrak{H} \\rangle \\vert + \\vert \\langle l_5 u,u_\\mathfrak{B} - d^* p + u_\\mathfrak{H}\\rangle \\vert \\\\\n&\\leq \\frac{c^2}{2\\nu} (\\Vert u_\\mathfrak{B} + u_\\mathfrak{H} \\Vert ^2 +  \\Vert d^* p \\Vert ^2)\n+ \\frac{\\nu}{2} \\Vert d^* u \\Vert ^2  \\\\\n&\\ + \\frac{c^2}{2}(\\Vert u_\\mathfrak{B} + u_\\mathfrak{H} \\Vert^2 + \\Vert d^* p \\Vert ^2) + \\frac{1}{2} \\Vert u \\Vert^2.\n\\end{aligned}\n\\\nSince $ \\Vert u_{\\mathfrak{B}^*} \\Vert ^2 \\leq c_p^2 \\Vert d u \\Vert ^2$ \nand $ \\Vert u \\Vert ^2 = \\Vert u_{\\mathfrak{B}^*} \\Vert ^2 + \\Vert u_\\mathfrak{B} + u_\\mathfrak{H} \\Vert^2$\nwe have:\n\\\n\\begin{aligned}\n(\\mu \\langle \\cdot , \\cdot \\rangle + \\widebar{L_\\lambda})(u,p)(v,q) \\geq &  \n\\ (\\mu - \\frac{c^2}{2 \\nu} - \\frac{c^2}{2} - \\frac{1}{2}) \\Vert u_\\mathfrak{B} + u_\\mathfrak{H} \\Vert ^2 + \\frac{\\nu}{2} \\Vert d^* u\\Vert^2\\\\ \n&+ (\\lambda - \\frac{c^2}{2 \\nu} - \\frac{c^2}{2}) \\Vert d^* p||^2 \n+ (\\lambda - \\frac{c_p^2}{2}) \\Vert d u \\Vert ^2  .\n\\end{aligned}\n\\\n\nUsing once again the Poincar\u00e9 inequality to bound $\\Vert u_{\\mathfrak{B}^*} \\Vert$ by $\\Vert d u \\Vert$\nand $\\Vert p_{\\mathfrak{B}} \\Vert$ by $\\Vert d^* p \\Vert$ (on the dual complex),\nand since $\\Vert p_{\\mathfrak{B}} \\Vert$ = $\\Vert p \\Vert$ (as $p \\in V^3 \\cap \\mathfrak{H}^{3 \\perp}$)\nwe have for $\\lambda$ and $\\mu$ large enough (solely depending on $c$, $\\nu$ and $c_p$):\n\\\n\\begin{aligned}\n(\\mu \\langle \\cdot , \\cdot \\rangle + \\widebar{L_\\lambda})(u,p)(v,q) &\\gtrsim \\Vert u \\Vert ^2 + \\Vert d u \\Vert ^2 \n+ \\Vert d^* u \\Vert ^2 + \\Vert p \\Vert ^2 + \\Vert d^* p \\Vert ^2 \\\\\n&\\gtrsim \\Vert (u,p) \\Vert _{V^2 \\cap V^*_2 \\times V^*_3} ^2.\n\\end{aligned}\n\\\n\nClearly $\\Vert (v,q) \\Vert_{V^*_2 \\times W^3} \\leq \\Vert (u,p) \\Vert _{V^2 \\cap V^*_2 \\times V^*_3}$ \nas $\\Vert d^* v \\Vert = \\Vert d^* u_\\mathfrak{B} \\Vert \\leq \\Vert d^* u \\Vert$ and\n$(\\mu \\langle \\cdot , \\cdot \\rangle + \\widebar{L_\\lambda})$ is continuous as a bilinear form from $(V^2 \\cap V^*_2 \\times V^*_3) \\times (V^*_2 \\times (W^3 \\cap \\mathfrak{H}^{3 \\perp}))$.\nThe only thing left to show in order to use the Babu\u0161ka\u2013Lax\u2013Milgram theorem is the second condition.\nFor any $(v,q) \\neq 0$ we must find $(u,p)$ such that \n$(\\mu \\langle \\cdot , \\cdot \\rangle + \\widebar{L_\\lambda})(u,p)(v,q) > 0$.\nWe can take $\\lambda$ and $\\mu$ such that $\\lambda ^2 = \\mu \\nu$.\nWe consider two cases: \\newline \nWhen $v \\neq 0$, we can find $u \\in \\lbrace w \\in V^2 \\cap V^*_2 \\vert d^* w \\in V^1, dw \\in V^*_3 \\rbrace$ such that $((\\nu d + l_3) d^* + l_5 + \\mu + \\nu d^* d) u = v$ \n(see \\cite{Arnold2016}).\nTake $p = - \\nu/\\lambda d u$,\nthen since $\\lambda = \\mu \\nu / \\lambda$ it holds:\n\\begin{equation*}\n\\begin{aligned}\n(\\mu \\langle \\cdot , \\cdot \\rangle + \\widebar{L_\\lambda})(u,p)(v,q)  &= \\langle \\nu d d^* u,v\\rangle  + \\langle (l_3 d^* + l_5 + \\mu)u,v\\rangle  - \\lambda \\langle d^* p,v\\rangle \\\\ \n&\\ + \\lambda \\langle d u,q\\rangle  + \\mu \\langle p,q \\rangle \\\\\n&= \\langle ((\\nu d + l_3) d^* + l_5 + \\mu + \\nu d^* d) u, v \\rangle \\\\\n&\\ + \\lambda \\langle d u,q \\rangle - \\frac{\\mu \\nu}{\\lambda} \\langle du, q \\rangle \\\\\n&= \\langle v,v\\rangle.\n\\end{aligned}\n\\end{equation*}\nWhen $v = 0$ take $u$ solution of the Hodge-Dirac problem (see \\cite{Leopardi2016}): $d u = q$, $d^* u = 0$.\nThen \\[(\\mu \\langle \\cdot , \\cdot \\rangle + \\widebar{L_\\lambda})(u,0)(0,q) = \\lambda \\langle d u,q\\rangle  = \\lambda \\langle q,q\\rangle  > 0. \n\\]\n\nThis shows that $(\\mu \\langle \\cdot , \\cdot \\rangle + \\widebar{L_\\lambda})$ is a bounded isomorphism from \n$(V^2 \\cap V^*_2) \\times (V^*_3 \\cap \\mathfrak{H}^{3 \\perp})$ to $(V_2^* \\times (W^3 \\cap \\mathfrak{H}^{3 \\perp}))'$.\nSince $I: (V^2 \\cap V^*_2) \\times (V^*_3 \\cap \\mathfrak{H}_3^\\perp) \\rightarrow (V^*_2 \\times (W^3 \\cap \\mathfrak{H}^{3 \\perp}))'$ \nis compact by the compactness property,\n$I(\\mu \\langle \\cdot , \\cdot \\rangle + \\widebar{L_\\lambda})^{-1}$ is also compact. Since the spectrum of a compact operator is at most countable, we have that \n$Id + \\eta I (\\mu \\langle \\cdot , \\cdot \\rangle + \\widebar{L_\\lambda})^{-1}$ has a bounded inverse for all $\\eta \\in \\mathbb{C}$\nexcept for a countable subset.\nTherefore, by composing to the right with $\\mu \\langle \\cdot,\\cdot \\rangle + \\widebar{L_\\lambda}$ we get that $\\widebar{L_\\lambda} + (\\mu + \\eta) I$ has almost always a bounded inverse.\n\\end{proof}\n\nHence, up to an arbitrary small perturbation, $\\widebar{L_\\lambda}$ is a bounded isomorphism \nfrom $(V^2 \\cap V^*_2) \\times (V^*_3 \\cap \\mathfrak{H}_3^\\perp) $ to $(V^*_2 \\times (W^3 \\cap \\mathfrak{H}^{3 \\perp}))'$.\n\n\\begin{remark}\nWe could have left $\\mathfrak{H}^3$ in the domain \nand the proof above would still work.\nHowever, in this case, $\\widebar{L_\\lambda}$ would never have\nbeen an isomorphism since its image cannot reach $\\mathfrak{H}^3$.\n\\end{remark}\n\nWe have the same result for the dual problem.\n\\begin{lemma}\nFor all $\\mu \\in \\mathbb{C}$ except for a countable subset, $\\widebar{L_\\lambda'} + \\mu \\langle \\cdot , \\cdot \\rangle$ is a bounded isomorphism.\n\\end{lemma}\n\\begin{proof}\nThe same proof as the one of Theorem \\ref{th:Liso} works.\nThe only differences will be a sign in the chosen $(v,q)$,\n$l_5$ and $l_3$ substituted by $l_5^*$ and $l_3^*$ \nand $(l_3 d^* u,v)$ changed to $(l_3^* u,d^* v)$,\nwhich does not add any difficulty in the proof.\n\\end{proof}\n\n\\begin{remark}\nThe proof Theorem \\ref{th:Liso} requires taking $\\lambda$ to be sufficiently large,\nhowever for any $f_2 \\in V_2^{*'}$, $f_3 \\in (W^3 \\cap \\mathfrak{H}^{3 \\perp})'$, $\\lambda_0 > 0$, $\\lambda_1 > 0$ we have the following equivalence:\n\\[ \\widebar{L_{\\lambda_0}} (u,p) = (f_2,f_3) \\Leftrightarrow \\widebar{L_{\\lambda_1}} (u, \\frac{\\lambda_0}{\\lambda_1} p) = (f_2, \\frac{\\lambda_1}{\\lambda_0} f_3)\\ .\n\\]\nTherefore, if $\\widebar{L_{\\lambda_0}}$ is a bounded isomorphism for a given $\\lambda_0$, it easily follows that $\\widebar{L_{\\lambda_1}}$ is a bounded isomorphism for any $\\lambda_1 > 0$,\nin particular for $\\lambda_1 = 1$.\nThe same argument works for $\\widebar{L_{\\lambda}'}$.\n\\end{remark}\n\nFrom here onward, we assume that $\\widebar{L_1}$ and $\\widebar{L_1'}$ are bounded isomorphisms. \n\n\\subsection{Well-posedness of the continuous mixed formulation}\nAs we did in the unperturbed case, \nwe introduce an auxiliary variable in the problem.\nIn the following, $\\mathbf{u}$ is a shortcut for $(u_1,u_2,u_3,u_p)$.\n\nWe define $B$ by:\n\\begin{equation} \\label{eq:defBlambda}\n\\begin{aligned}\nB(\\mathbf{u},\\mathbf{v}) :=&\\  \\langle u_1,v_1\\rangle  - \\langle u_2,d v_1\\rangle \\\\\n&\\ + \\nu \\langle  d u_1,v_2\\rangle  - \\langle u_3, d v_2\\rangle  + \\langle d u_2,v_3\\rangle  \\\\\n&\\ + \\langle l_3 u_1,v_2\\rangle  + \\langle l_5 u_2,v_2\\rangle  + \\langle u_p,v_3\\rangle  + \\langle u_3,v_p\\rangle. \n\\end{aligned}\n\\end{equation}\nThe mixed formulation is:\\\\\nGiven $(f_2,f_3) \\in W$, find $\\mathbf{u} = (u_1,u_2,u_3,u_p) \\in V^1 \\times V^2 \\times V^3 \\times \\mathfrak{H}^3 $ such that \n$\\forall \\mathbf{v} = (v_1,v_2,v_3,v_p) \\in V^1 \\times V^2 \\times V^3 \\times \\mathfrak{H}^3$,\n\\begin{equation} \\label{eq:defmixedcontinuous}\nB(\\mathbf{u},\\mathbf{v}) = \\langle f_2,v_2\\rangle  + \\langle f_3,v_3\\rangle.\n\\end{equation}\n\nFor $(u,p)$ solution of \\eqref{eq:defmixedprimal}, \nit immediately appears that \\\\\n$(d^* u,u,p,0)$ solves \\eqref{eq:defmixedcontinuous}.\nNow for $(u_1,u_2,u_3,u_p)$ solution of \\ref{eq:defmixedcontinuous},\nthe first line implies that $u_2 \\in V^*_2$, $d^* u_2 = u_1$ and therefore $d^* u_2 \\in V^1$.\nThe second line implies that $u_3 \\in V^*_3$.\nAnd the last implies that $u_3 \\perp \\mathfrak{H}^3$.\nTherefore $(u_2,u_3) \\in D$, and it obviously solves \\eqref{eq:defmixedprimal}.\n\n\\begin{theorem}\nUnder the regularity assumption \\eqref{eq:regularity35}, Problem \\eqref{eq:defmixedcontinuous} is well-posed.\n\\end{theorem}\nThe proof follows from the Babu\u0161ka\u2013Lax\u2013Milgram theorem along with Lemma \\ref{lemma:pertubedwp2} and inf-sup condition \\eqref{eq:pertubedwp1}.\nIn the following hidden constants only depend on $\\Vert l_3 \\Vert$, $\\Vert l_5 \\Vert$, $\\Vert K \\Vert$, $\\Vert d (d^* + l_3^*) (K)_2 \\Vert$, $\\nu$ and on constants of Poincar\u00e9.\nWe shall write $V = V^1 \\times V^2 \\times V^3 \\times \\mathfrak{H}^3$.\n\n\\begin{lemma} \\label{lemma:defz}\nFor all $(0,u_2,u_3,0) = \\mathbf{u} \\in V$, there exists $\\mathbf{z} \\in V$ such that $\\Vert \\mathbf{z} \\Vert_V \\lesssim \\Vert \\mathbf{u} \\Vert_V$\nand \n$\\forall \\mathbf{\\omega} \\in V, B(\\mathbf{\\omega},\\mathbf{z}) = \\langle \\mathbf{\\omega},\\mathbf{u}\\rangle $.\n\\end{lemma}\n\\begin{proof}\nTake $(z_2,z_3) = K (u_2,P_{\\mathfrak{H}^\\perp} u_3)$ and $\\xi = - (\\nu d^* + l_3^*)z_2$.\nThen $\\xi \\in V^1$ by assumption \\eqref{eq:regularity35} and\nby definition of $K$ we have $\\forall \\mathbf{\\omega} \\in V$: \n\\begin{equation} \\label{eq:lemmadefz1}\n\\begin{aligned}\n- \\langle d z_2,\\omega_3 \\rangle =& \\langle P_{\\mathfrak{H}^\\perp} u_3, \\omega_3 \\rangle, \\\\\n\\langle d (\\nu d^* + l_3^*) z_2 + l_5^* z_2 + d^* z_3, \\omega_2 \\rangle =& \\langle u_2,\\omega_2 \\rangle. \n\\end{aligned}\n\\end{equation}\nWe set $\\mathbf{z} := ( \\xi,z_2,z_3,u_{3 \\mathfrak{H}})$. Applying \\eqref{eq:lemmadefz1} we have:\n\\begin{equation*}\n\\begin{aligned}\nB( \\mathbf{\\omega},\\mathbf{z}) \n=& \\langle \\omega_1, \\xi\\rangle  - \\langle \\omega_2,d \\xi\\rangle  \\\\\n& + \\langle ( \\nu d + l_3)  \\omega_1,z_2\\rangle  - \\langle \\omega_3,d z_2\\rangle  + \\langle l_5 \\omega_2,z_2\\rangle  \\\\\n& +\\langle d \\omega_2, z_3\\rangle  + \\langle \\omega_3,u_{3 \\mathfrak{H}}\\rangle  + \\langle \\omega_p,z_3\\rangle  \\\\\n=& \\langle \\omega_1, \\xi\\rangle  - \\langle \\omega_2,d \\xi\\rangle  \\\\\n& + \\langle \\omega_1, (\\nu d^* + l_3^*) z_2\\rangle  + \\langle \\omega_3,P_{\\mathfrak{H}^\\perp} u_3 \\rangle  + \\langle \\omega_2,l_5^* z_2\\rangle  \\\\\n& + \\langle \\omega_2, d^* z_3\\rangle  + \\langle \\omega_3,u_{3 \\mathfrak{H}}\\rangle  \\\\\n=& \\langle \\omega_1, \\xi\\rangle  - \\langle \\omega_2,d \\xi\\rangle  \\\\\n& + \\langle \\omega_1,(\\nu d^* + l_3^*) z_2 \\rangle - \\langle \\omega_2,d (\\nu d^* + l_3^*)z_2\\rangle  \\\\\n& + \\langle \\omega_2,u_2\\rangle + \\langle \\omega_3, P_{\\mathfrak{H}^\\perp} u_3\\rangle  + \\langle \\omega_3,u_{3 \\mathfrak{H}}\\rangle  \\\\\n=& -\\langle \\omega_1,( \\nu d^* + l_3^*)  z_2\\rangle  + \\langle \\omega_2,d ( \\nu d^* + l_3^*)  z_2\\rangle  \\\\\n& + \\langle \\omega_1,(\\nu d^* + l_3^*) z_2\\rangle  - \\langle \\omega_2,d (\\nu d^* + l_3^*)z_2\\rangle  \\\\\n& + \\langle \\omega_3,u_3\\rangle  + \\langle \\omega_2,u_2\\rangle  \\\\\n=& \\langle \\omega_2,u_2\\rangle  + \\langle \\omega_3,u_3\\rangle. \n\\end{aligned}\n\\end{equation*}\nMoreover since $\\widebar{L_1'}$ is a bounded isomorphism, so is $K$ thus\n\\begin{equation*}\n\\begin{aligned}\n\\Vert \\mathbf{z} \\Vert_V &\\leq \\Vert \\nu d^* z_2 \\Vert + \\Vert l_3^* z_2 \\Vert + \\Vert z_2 \\Vert_{V^2} + \\Vert z_3 \\Vert + \\Vert d \\xi \\Vert + \\Vert u_{3\\mathfrak{H}} \\Vert \\\\\n&\\lesssim \\Vert z_2 \\Vert_{V^2 \\cap V^*_2} + \\Vert z_3 \\Vert_{V^*_3} + \\Vert d \\xi \\Vert + \\Vert u_{3 \\mathfrak{H}} \\Vert \\\\\n&\\lesssim (\\Vert K \\Vert + \\Vert d(d ^* + l_3^*) K \\Vert + 1) \\Vert \\mathbf{u} \\Vert \\lesssim \\Vert \\mathbf{u} \\Vert_V.\n\\end{aligned}\n\\end{equation*}\n\\end{proof}\n\n\\begin{lemma} \\label{lemma:continuous2}\nFor all $\\mathbf{u} \\in V$, there exists $\\mathbf{z} \\in V$ such that $\\Vert \\mathbf{z} \\Vert_V \\lesssim \\Vert \\mathbf{u} \\Vert_V$\nand $B(\\mathbf{u},\\mathbf{z}) \\gtrsim \\Vert d u_1 \\Vert ^2 + \\Vert d u_2 \\Vert^2 + \\Vert u_1 \\Vert ^2 + \\Vert u_p \\Vert ^2 - \\Vert u_2 \\Vert^2$.\n\\end{lemma}\n\\begin{proof}\nLet $c = \\max(\\Vert l_3 \\Vert,\\Vert l_5 \\Vert)$. \nWe begin with some preliminary computations:\n\\begin{equation} \\label{eq:mixedeq1}\n\\begin{aligned}\nB(\\mathbf{u},(\\nu u_1,u_{2 \\mathfrak{B}},d u_2,0)) =&\\  \\nu \\langle u_1,u_1 \\rangle - \\nu \\langle u_2,d u_1 \\rangle + \\nu \\langle  d u_1,u_{2\\mathfrak{B}}\\rangle\\\\\n&- \\langle u_3, d u_{2\\mathfrak{B}}\\rangle  + \\langle d u_2,d u_2\\rangle + \\langle l_3 u_1,u_{2 \\mathfrak{B}}\\rangle\\\\\n&+ \\langle l_5 u_2,u_{2 \\mathfrak{B}}\\rangle  +\n\\langle u_p,d u_2\\rangle  + \\langle u_3,0\\rangle  \\\\\n\\geq& \\frac{\\nu}{2} \\Vert u_1 \\Vert ^2 + \\Vert d u_2 \\Vert ^2  - \\left (\\frac{c^2}{2 \\nu} + c \\right ) \\Vert u_2 \\Vert ^2,\n\\end{aligned}\n\\end{equation}\n\n\\begin{equation} \\label{eq:mixedeq2}\n\\begin{aligned}\nB(\\mathbf{u},(0,d u_1,0,0)) =&\\ \\nu \\langle d u_1,d u_1\\rangle  + \\langle l_3 u_1,d u_1\\rangle  + \\langle l_5 u_2,d u_1\\rangle  \\\\\n\\geq& \\frac{1}{2} \\nu \\Vert d u_1 \\Vert ^2 - \\frac{c^2}{\\nu} ( \\Vert u_1 \\Vert^2 + \\Vert u_2 \\Vert ^2 ),\n\\end{aligned}\n\\end{equation}\n\n\\begin{equation} \\label{eq:mixedeq3}\n\\begin{aligned}\nB(\\mathbf{u},(0,0,u_p,0)) = \\langle u_p,u_p\\rangle .\n\\end{aligned}\n\\end{equation}\nClearly it is possible to construct a suitable $\\mathbf{z}$ from a linear combination of \\eqref{eq:mixedeq1}, \\eqref{eq:mixedeq2} and \\eqref{eq:mixedeq3}.\nBounds on norms are easily checked, for example:\n\\[\\Vert (0, d u_1,0,0) \\Vert_V = \\Vert d u_1 \\Vert + \\Vert d d u_1 \\Vert = \\Vert d u_1 \\Vert \\lesssim \\Vert \\mathbf{u} \\Vert_V. \n\\]\n\\end{proof}\n\nCombining the two preceding lemmas gives\n\\begin{equation} \\label{eq:pertubedwp1}\n\\forall \\mathbf{u} \\in V, \\sup_{\\Vert \\mathbf{v} \\Vert_V = 1} \\vert B(\\mathbf{u},\\mathbf{v}) \\vert \\gtrsim \\Vert \\mathbf{u} \\Vert_V\\ .\n\\end{equation}\n\n\\begin{lemma} \\label{lemma:pertubedwp2}\nFor any $\\mathbf{v} \\in V$, there is $\\mathbf{u} \\in V$ such that $B(\\mathbf{u},\\mathbf{v}) > 0$.\n\\end{lemma}\n\\begin{proof}\nGiven $\\mathbf{v} \\neq 0 \\in V$, if $v_2 = 0$, $v_3 = 0$ and $v_p = 0$\ntake $\\mathbf{u} = (v_1,0,0,0)$ and \n\\[B(\\mathbf{u},\\mathbf{v}) = \\langle v_1,v_1\\rangle  > 0.\n\\]\nElse take $(u_2,u_3) = \\widebar{L_1}^{-1}(v_2,P_{\\mathfrak{H}^\\perp}v_3) + (0,v_p)$,\n$u_p = P_\\mathfrak{H} v_3$ and $u_1 = d^* u_2$ ($u_1 \\in V^1$ by assumption \\eqref{eq:regularity35})\nthen \\[B(\\mathbf{u},\\mathbf{v}) = \\langle v_2,v_2\\rangle  + \\langle v_3,v_3\\rangle  + \\langle q,q\\rangle  > 0.\n\\]\n\\end{proof}\n\n\\subsection{Discrete well-posedness}\\label{Discretewellposedness}\n\nWe introduce the notation $V_h = V_h^1 \\times V_h^2 \\times V_h^3 \\times \\mathfrak{H}^3_h$.\nThe discrete variational problem is the same as the continuous, \nsubstituting $V$ by $V_h$.\nHence we shall still use the notation $B$, this time as a function \nfrom $V_h \\times V_h$ to $\\mathbb{R}$.\nSo that the discrete problem is:\\\\\nGiven $(f_2,f_3) \\in W$, find $\\mathbf{u_h} = (u_{1h},u_{2h},u_{3h},u_{ph}) \\in V^1_h \\times V^2_h \\times V^3_h \\times \\mathfrak{H}^3_h $ such that \n$\\forall \\mathbf{v_h} = (v_{1h},v_{2h},v_{3h},v_{ph}) \\in V^1_h \\times V^2_h \\times V^3_h \\times \\mathfrak{H}^3_h$,\n\\begin{equation} \\label{eq:defmixeddiscrete}\nB(\\mathbf{u_h},\\mathbf{v_h}) = \\langle f_2,v_{2h}\\rangle  + \\langle f_3,v_{3h}\\rangle.\n\\end{equation}\n\nConsidering the dual problem to the unperturbed problem with $\\nu = 1$.\nWe have: \\\\\n$D_0 = \\lbrace (u,p) \\in (V^2 \\cap V^*_2) \\times V^3_* \\vert\\  d^*u \\in V^1 \\rbrace$ and\n$L_0'(u,p) = (dd^* u + d^*p,-du)$.\nLet $K_0$ be the solution operator of the dual problem $L_0'$.\nWe have $K_0 = (L_0')^{-1}$ when $L_0'$ is viewed as an isomorphism from $P_{\\mathfrak{H}^\\perp} D_0$ to $P_{\\mathfrak{H}^\\perp} (W^2 \\times W^3)$ and $K_0$ is extended by $0$ on $\\mathfrak{H}$.\nExplicitly we have the decomposition: $\\forall (f_2,f_3) \\in W^2 \\times W^3$,\n\\begin{equation} \\label{eq:decompK}\n (f_2,f_3) = (d d^* (K_0)_2(f_2,f_3) + d^* (K_0)_3(f_2,f_3),-d (K_0)_2(f_2,f_3)) + (P_{\\mathfrak{H}} f_2,P_{\\mathfrak{H}} f_3)\n\\end{equation}\nand a similar expression for their discrete counterparts.\\\\\nTherefore, $\\forall (z_2,z_3) \\in D_0, (P_{\\mathfrak{H}^\\perp} z_2,P_{\\mathfrak{H}^\\perp} z_3) = L_0' K_0 (z_2,z_3) = K_0 L_0' (z_2,z_3)$.\n\nAs mentioned before this problem is closely related to the one studied by D. Arnold \\cite{Arnold2016}.\nSince the mixed variable part is almost unchanged we shall\nuse the generalized canonical projection $\\Pi_h$ (see \\cite{Arnold2016}) and\nwe state its properties below.\n\n\\begin{lemma} Under the condition of \\cite[Theorem~5.1]{Arnold2016}: \\label{lemma:PIh}\n\\begin{itemize}\n\\item $\\Pi_h$ is a projection uniformly bounded in the V-norm.\n\\item $d \\Pi_h = P_{\\mathfrak{B}_h} d$.\n\\item $\\forall w \\in V^k, \\Vert (I - \\Pi_h)w \\Vert \\lesssim \\Vert (I - \\pi_h)w \\Vert + \\eta_0' \\Vert d w \\Vert$.\n\\item $\\forall w,v \\in V^k, \\vert \\langle(I - \\Pi_h)w,v\\rangle \\vert \\lesssim  (\\Vert (I - \\pi_h) w \\Vert + \\eta_0' \\Vert d w \\Vert)(\\Vert (I - \\pi_h) v \\Vert + \\eta_0' \\Vert d v \\Vert) + \\alpha_0' \\Vert dw \\Vert \\Vert dv \\Vert$.\n\\end{itemize}\n\\end{lemma}\nwhere $\\eta_0', \\alpha_0' \\to 0$ when $h \\to 0$, they are given, along the proof in the reference \\cite{Arnold2016}.\n\n\\begin{definition} \\label{def:estimates}\nWe shall use the following notations in this section:\n\\[ \\delta_0 = \\Vert (I - \\pi_h) K_0 \\Vert, \\mu_0 = \\Vert (I - \\pi_h) P_\\mathfrak{H} \\Vert, \n\\]\n\\[ \\eta_0 = \\max \\lbrace \\Vert (I - \\pi_h) d K_0 \\Vert,\\Vert (I - \\pi_h) d^* (K_0)_2 \\Vert \\rbrace, \n\\]\n\\[ \\alpha_0 = \\eta_0(1 + \\eta_0) + \\mu_0 + \\delta_0 + \\mu_0 \\delta_0 + \\eta_0', \n\\]\n\\[ \\eta = \\max \\lbrace \\delta_0, \\mu_0, \\eta_0,\\Vert (I - \\pi_h) l_3^* (K)_2 \\Vert,\\Vert (I - \\pi_h)dl_3^* (K)_2 \\Vert\\rbrace. \n\\]\n\\end{definition}\n\n\\begin{lemma} \\label{lemma:estimatesforzh}\nWe have: \n\\[\\Vert K_0 - K_{0h}P_h \\Vert \\lesssim \\alpha_0, \n\\]\n\\[ \\Vert d K_0 - d K_{0h} P_h \\Vert + \\Vert d^* (K_0)_2 - d_h^* (K_{0h})_2 P_h \\Vert \\lesssim \\eta_0. \n\\]\n\\end{lemma}\n\\begin{proof}\nThe idea is for $(f_2,f_3) \\in (W^2 \\times W^3)$ to apply the error estimate of Theorem \\ref{th:unpertubedestimate} for $(u_2,u_3) = K_0 (f_2,f_3)$,\n$(\\phi_2,\\phi_3) = P_{\\mathfrak{H}}(f_2,f_3)$, $u_1 = d^* u_2$, $(u_{2h},u_{3h}) = K_{0h}P_h (f_2,f_3)$,\n$(\\phi_{2h},\\phi_{3h}) = P_{\\mathfrak{H}_h} P_h (f_2,f_3)$, $u_{1h} = d_h^* u_{2h}$. Unfortunately we cannot conclude with the crude estimate of Theorem \\ref{th:unpertubedestimate}  \nbecause of the error on $u_1$.\nWe need improved estimates that give\n\\begin{equation*}\n\\begin{aligned}\n\\Vert u_2 - u_{2h} \\Vert + \\Vert u_3 - u_{3h} \\Vert &\\lesssim  (1 + \\mu_0) E(u_2) + E(u_3) + \\eta_0 E(u_1)\\\\\n&\\+ (\\eta_0^2 + \\delta_0 + \\eta_0') E(d u_1) + \\eta_0' E(\\phi_2) + E(d u_2),\\\\\n\\Vert d u_2 - d u_{2h} \\Vert + \\Vert u_1 - u_{1h} \\Vert &\\lesssim E(d u_2) + E(u_1) + \\eta_0 E(d u_1).\n\\end{aligned}\n\\end{equation*}\nWe conclude since $E(u_2) + E(u_3) \\leq \\delta_0 \\Vert (f_2,f_3) \\Vert$, $E(u_1) + E(d u_2) \\leq \\eta_0 \\Vert (f_2,f_3) \\Vert$ and \n$E(d u_1) + E(\\phi_2) \\leq \\Vert (f_2,f_3) \\Vert$, the last coming from $d u_1 = P_\\mathfrak{B} f_2$.\nThese proofs are lengthy, technical and mostly follow those in theorem 3.11 of \\cite{Arnold_2010}. \n\\end{proof}\n\n\\begin{lemma} \\label{lemma:erroronharmonics}\n\nFor $f \\in \\mathfrak{H}^\\perp$ we have $\\Vert P_{\\mathfrak{H}_h} f \\Vert \\lesssim \\mu_0 \\Vert f \\Vert$\n\\end{lemma}\n\\begin{proof}\nWe recall the mixed formulation for the Hodge-Laplacian problem.\nThe bilinear form is given by:\n\\[B((\\sigma,u,p),(\\tau,v,q)) = \\langle \\sigma,\\tau\\rangle  - \\langle u,d \\tau\\rangle  + \\langle d \\sigma,v\\rangle  + \\langle d u,d v\\rangle  + \\langle p,v\\rangle  + \\langle u,q\\rangle .\n\\]\nIn the continuous case the bilinear form acts on $(V^1 \\times V^2 \\times \\mathfrak{H}^2)^2$\nand on $(V^1_h \\times V^2_h \\times \\mathfrak{H}_h^2)^2$ in the discrete case.\nLet $(\\sigma,u,p) \\in V^1 \\times V^2 \\times \\mathfrak{H}^2$ be such that $\\forall (\\tau,v,q) \\in V^1 \\times V^2 \\times \\mathfrak{H}^2$, $B((\\sigma,u,p),(\\tau,v,q)) = (f,v)$,\nand let $(\\sigma_h,u_h,p_h) \\in V^1_h \\times V^2_h \\times \\mathfrak{H}^2_h$ be such that $\\forall (\\tau,v,q) \\in V^1_h \\times V^2_h \\times \\mathfrak{H}^2_h$, $B((\\sigma_h,u_h,p_h),(\\tau,v,q)) = (f,v)$.\nThen \\cite[Theorem~5.6 p.~62]{feec-cbms} gives the error estimate $\\Vert p - p_h \\Vert \\lesssim E(p) + \\mu_0 E(d \\sigma)$.\nWe have $P_{\\mathfrak{H}} f = p = 0$ and $P_{\\mathfrak{H}_h} f = p_h$ thus\n\\[\\Vert P_{\\mathfrak{H}_h} f \\Vert = \\Vert p - p_h \\Vert \\lesssim 0 + \\mu_0 E(d \\sigma) \\lesssim \\mu_0 \\Vert f \\Vert\n\\]\nsince $E(p) = 0$ and by the well-posedness of the Hodge-Laplacian problem for the last inequality.\n\\end{proof}\n\n\\begin{theorem} \\label{th:estimatezh}\nFor $z = (z_2,z_3) \\in D_0$, let $z_h = (z_{2h},z_{3h}) = K_{0h} P_h L_0' z + P_{\\mathfrak{H}_h} P_\\mathfrak{H} z$, we have:\n\\[ \\Vert z - z_h \\Vert \\lesssim \\alpha_0 \\Vert L_0' z\\Vert, \\Vert d (z - z_h) \\Vert + \\Vert d^* z_2 - d^*_h z_{2h} \\Vert \\lesssim \\eta_0 \\Vert L_0' z \\Vert, \n\\]\n\\[\u00a0\\Vert P_h (d d^* z_2 + d^* z_3) - (d d^*_h z_{2h} + d^*_h z_{3h}) \\Vert \\leq \\mu_0 \\Vert L_0' z \\Vert.\n\\]\n\\end{theorem}\n\\begin{proof}\nThe same proof as \\cite[Theorem~5.2]{Arnold2016} works.\nIt is duplicated here since the proof is short\nand demonstrates that our change in the definition of $L_0'$ does not interfere.\n\\begin{equation*}\n\\begin{aligned}\nz - z_h &= (P_{\\mathfrak{H}^\\perp} z - P_{\\mathfrak{H}_h^\\perp} z_h) +\n(P_{\\mathfrak{H}} z - P_{\\mathfrak{H}_h} z_h) \\\\\n&= ( K_0 - K_{0h} P_h) L_0' z + (I - P_{\\mathfrak{H}_h})P_\\mathfrak{H} z.\n\\end{aligned}\n\\end{equation*}\nAs $P_{\\mathfrak{H}_h} P_{\\mathfrak{H}} = P_{\\mathfrak{Z}_h} P_{\\mathfrak{H}}$ and $\\pi_h \\mathfrak{Z} \\subset \\mathfrak{Z}_h$ we have:\n\\[\\Vert (I - P_{\\mathfrak{H}_h})P_\\mathfrak{H} z\\Vert \\leq \\Vert (I - \\pi_h) P_\\mathfrak{H} z \\Vert \\leq \\mu_0 \\Vert z \\Vert \\lesssim \\alpha_0 \\Vert L_0' z \\Vert. \n\\]\nWe then get the expected result by Lemma \\ref{lemma:estimatesforzh}.\nThe second part follows directly from Lemma \\ref{lemma:estimatesforzh}.\nFinally for the last estimate, \\eqref{eq:decompK} gives \n\\begin{equation*}\nd d_h^* z_{2h} + d_h^* z_{3h} = (L_{0h}')_2 z_h = (L_{0h}' K_{0h} P_h)_2 L_0' z = (P_{\\mathfrak{B}_h} + P_{\\mathfrak{B}^*_h}) P_h (L_0')_2 z\n\\end{equation*}\nso: \n\\begin{equation*}\n\\begin{aligned}\n\\Vert P_h (d d^* z_2 + d^* z_3) - (d d^*_h z_{2h} + d^*_h z_{3h}) \\Vert &= \n\\Vert (I - (P_{\\mathfrak{B}_h} + P_{\\mathfrak{B}^*_h})) P_h (L_0')_2 z \\Vert \\\\\n& = \\Vert P_{\\mathfrak{H}_h} (L_0')_2 z \\Vert.\n\\end{aligned}\n\\end{equation*}\nAnd we conclude with Lemma \\ref{lemma:erroronharmonics}.\n\\end{proof} \n\nGiven $\\mathbf{u} \\in V_h$,\ndefine $g = (u_2,P_{\\mathfrak{H}^\\perp}u_3)$, $z = Kg$, $\\xi = -(d^* + l_3^*)z_2$ and $\\mathbf{z} = (\\xi,z,P_\\mathfrak{H} u_3)$.\n\\begin{theorem} \\label{th:discreteproj}\nThere is $\\mathbf{z_h} \\in V_h$ such that $\\forall \\mathbf{\\omega} \\in V_h$,\n$\\Vert \\mathbf{z_h} \\Vert_V \\lesssim  \\Vert \\mathbf{z} \\Vert_V$ uniformly in h and\n$\\vert B(\\mathbf{\\omega},\\mathbf{z - z_h}) \\vert \\leq \\epsilon_h \\Vert \\mathbf{\\omega} \\Vert_V \\Vert \\mathbf{u} \\Vert$,\nwhere $\\epsilon_h \\rightarrow 0$ as $h \\rightarrow 0$.\n\\end{theorem}\n\\begin{proof}\nTake $z_h = K_{0h} P_h L_0' z + P_{\\mathfrak{H}_h} P_\\mathfrak{H} z$,\n$\\xi_h = - d_h^* z_{2h} - \\Pi_h l_3^* z_2$.\nBy Theorem \\ref{th:estimatezh} we have:\n\\begin{equation}\n\\begin{gathered}\n\\Vert z - z_h \\Vert \\lesssim \\alpha_0 \\Vert g \\Vert,\\ \\Vert d(z - z_h) \\Vert \\lesssim \\eta_0 \\Vert g \\Vert,\\  \n\\Vert d^* z_2 - d^*_h z_{2h} \\Vert \\lesssim \\eta_0 \\Vert g \\Vert,\\\\\n\\Vert \\xi - \\xi_h\\Vert \\leq \\Vert d^* z_2 - d^*_h z_{2h} \\Vert + \\Vert (I - \\Pi_h) l_3^* z_2 \\Vert.\n\\end{gathered}\n\\end{equation}\nUsing Lemma \\ref{lemma:PIh} and the boundedness of $d\\,l^*_3\\,K$ we get\n\\[ \\Vert (I - \\Pi_h) l_3^* z_2 \\Vert \\lesssim \\Vert (I - \\pi_h)l^*_3 z_2 \\Vert + \\eta_0' \\Vert dl^*_3z_2 \\Vert \\lesssim (\\eta + \\eta_0') \\Vert g \\Vert. \n\\]\n\nFinally, since $\\forall \\omega_2 \\in V^2_h \\subset V^2$\n\\[ \\langle d \\omega_2, z_3 \\rangle = \\langle \\omega_2, d^* z_3 \\rangle, \\langle d \\omega_2, z_{3h} \\rangle = \\langle \\omega_2, d^*_h z_{3h} \\rangle\\ , \n\\]\nwe have $\\forall \\mathbf{\\omega} \\in V_h$:\n\\begin{equation*}\n\\begin{aligned}\n\\vert B(\\mathbf{\\omega},&(\\xi - \\xi_h, z_2 - z_{2h}, z_3 - z_{3h},P_\\mathfrak{H} u_3 - P_{\\mathfrak{H}_h}P_\\mathfrak{H}  u_3))\\vert =\\ \\vert \\langle \\omega_1,\\xi - \\xi_h\\rangle\\\\\n&\\ - \\langle \\omega_2,d(\\xi - \\xi_h)\\rangle  + \\langle (\\nu d + l_3)\\omega_1,z_2 - z_{2h}\\rangle  - \\langle \\omega_3,d(z_2 - z_{2h})\\rangle \\\\\n&\\ + \\langle l_5 \\omega_2,z_2 - z_{2h}\\rangle  + \\langle d\\omega_2, z_3 - z_{3h}\\rangle \\\\\n&\\ + \\langle \\omega_p,z_3 - z_{3h}\\rangle + \\langle \\omega_3,P_\\mathfrak{H} u_3 - P_{\\mathfrak{H}_h}P_\\mathfrak{H} u_3 \\rangle \\vert\\\\\n&\\lesssim \\ \\Vert \\mathbf{\\omega} \\Vert_V (\\Vert \\xi - \\xi_h \\Vert \n + 2 \\Vert z_2 - z_{2h} \\Vert + \\Vert d(z_2 - z_{2h}) \\Vert \\\\ \n&\\ + \\Vert z_3 - z_{3h} \\Vert + \\Vert (I - P_{\\mathfrak{H}_h}) P_\\mathfrak{H} u_3 \\Vert)  \n+ \\vert  \\langle \\omega_2,P_h d^* z_3 - d_h^* z_{3h} - P_h d(\\xi - \\xi_h)\\rangle \\vert \\\\\n&\\lesssim \\ \\Vert \\mathbf{\\omega} \\Vert_V \\left[ (\\eta + 2 \\eta_0\n+ 2 \\alpha_0 + \\eta_0' + \\mu_0) \\Vert \\mathbf{u} \\Vert + \\Vert P_h d^* z_3 - d_h^* z_{3h} - P_h d( \\xi - \\xi_h) \\Vert \\right].\n\\end{aligned}\n\\end{equation*}\nSince $\\eta$, $\\eta_0$, $\\eta_0'$, $\\mu_0$ and $\\alpha_0$ all converge toward $0$ when $h \\rightarrow 0$,\nthe only thing left to prove is that \n$\\Vert P_h d^* z_3 - d_h^* z_{3h} - P_h d( \\xi - \\xi_h) \\Vert \\lesssim \\epsilon \\Vert \\mathbf{u} \\Vert$ \nwhere $\\epsilon \\rightarrow 0$ when $h \\rightarrow 0$. We have \n\\[ -d (\\xi - \\xi_h) = d (d^* + l_3^*) z_2 - d d^*_h z_{2h} - d \\Pi_h l^*_3 z_2 = d d^* z_2 - d d^*_h z_{2h} + d(I - \\Pi_h) l^*_3 z_2 .\n\\]\nTheorem \\ref{th:estimatezh} gives:\n\\[\\Vert P_h d^* z_3 - d_h^*z_{3h} + P_h d d^* z_2 - d d^*_h z_{2h} \\Vert \\lesssim \\mu_0 \\Vert g \\Vert.\n\\]\nAnd we conclude with Lemma \\ref{lemma:PIh} since we can find a bounded cochain projection $\\pi_h$ \nsuch that $\\pi_h d = P_{\\mathfrak{B}_h} d$ (see \\cite[Theorem~3.7]{Arnold_2010}) so\n\\[ \\Vert d(I - \\Pi_h) l_3^* z_2 \\Vert = \\Vert (I - P_{\\mathfrak{B}_h}) d l^*_3 z_2 \\Vert \\lesssim \\Vert (I - \\pi_h) d l^*_3 z_2 \\Vert \\leq \\eta \\Vert g \\Vert. \n\\]\n\\end{proof}\n\n\\begin{lemma} \\label{lemma:defzh}\nFor all $\\mathbf{u} \\in V_h$ and $\\mathbf{z} \\in V$ defined in Theorem \\ref{th:discreteproj},\nthere exists a constant $c$ independent of $h$ and $\\mathbf{\\sigma} \\in V_h$ such that $\\Vert \\mathbf{\\sigma} \\Vert_V \\lesssim \\Vert \\mathbf{u} \\Vert_V$\nand $B(\\mathbf{u},\\mathbf{z} + \\mathbf{\\sigma}) \\geq c \\Vert \\mathbf{u}\\Vert_V^2 $.\n\\end{lemma}\n\\begin{proof}\nStarting from Lemma \\ref{lemma:defz}, we construct $\\mathbf{\\sigma}$ in the same way as we did in Lemma \\ref{lemma:continuous2} in the continuous case.\nWe must simply add\n\\[B(\\mathbf{\\omega},(0,0,-P_{\\mathfrak{H}_h} z_3,0)) = - \\langle \\omega_p,z_3\\rangle \n\\]\nto correct the harmonic part.\n\\end{proof}\n\n\\begin{theorem} \\label{th:wellposednesspertubeddiscrete}\nThere are two positive constants $h_0$ and $C_0$ \nsuch that for all $h \\in (0,h_0]$,\nthere exists a unique $\\mathbf{u} \\in V_h$ such that $\\forall \\mathbf{v} \\in V_h$,\n$B(\\mathbf{u},\\mathbf{v}) = (\\mathbf{f},\\mathbf{v})$.\nMoreover we have $\\Vert \\mathbf{u} \\Vert_V \\leq C_0 \\Vert \\mathbf{f} \\Vert$.\n\\end{theorem}\n\\begin{proof}\nFor $\\mathbf{u} \\in V_h$ and $\\mathbf{z}$, $\\mathbf{z_h}$ defined in Theorem \\ref{th:discreteproj},\nLemma \\ref{lemma:defzh} gives $\\mathfrak{\\sigma} \\in V_h$ with $\\Vert \\mathbf{\\sigma} \\Vert_V \\lesssim \\Vert \\mathbf{u} \\Vert_V$ and a constant $c$ independent of $h$\nsuch that:\n\\[ \\vert B(\\mathbf{u},\\mathbf{z} + \\mathbf{\\sigma}) \\vert \\geq c \\Vert \\mathbf{u} \\Vert_V ^2. \n\\]\nBy Theorem \\ref{th:discreteproj} we have\nfor a constant $b$ independent of $h$.\n\\[B(\\mathbf{u},\\mathbf{z_h - z}) \\leq \\epsilon_h b \\Vert \\mathbf{u} \\Vert_V ^2. \n\\]\nTherefore:\n\\begin{equation*}\n\\begin{aligned}\n\\vert B(\\mathbf{u},\\mathbf{\\sigma} + \\mathbf{z_h}) \\vert =&\\ \\vert B(\\mathbf{u},\\mathbf{z + \\sigma}) + B(\\mathbf{u},\\mathbf{z_h - z}) \\vert \\\\\n\\geq&\\ \\vert B(\\mathbf{u},\\mathbf{z} + \\mathbf{\\sigma}) \\vert - \\vert B(\\mathbf{u},\\mathbf{z_h - z}) \\vert \\\\\n\\geq&\\ c \\Vert \\mathbf{u} \\Vert_V^2 - \\epsilon_h b \\Vert \\mathbf{u} \\Vert_V^2\n\\geq (c - \\epsilon_h b) \\Vert \\mathbf{u} \\Vert^2_V.\n\\end{aligned}\n\\end{equation*}\nSince $\\epsilon_h \\rightarrow 0$ as $h \\rightarrow 0$ \nwe can find $h_0$ such that $\\forall h \\in (0,h_0]$,\\\\\n$c - \\epsilon_h b > c - \\epsilon_{h_0} b > 0$.\n\nBy Theorem \\ref{th:discreteproj} and by the expression of $\\mathbf{\\sigma}$ we find:\n\\[\\Vert \\mathbf{\\sigma + z_h} \\Vert_V \\lesssim \\Vert \\mathbf{u} \\Vert_V + \\Vert \\mathbf{z} \\Vert_V \\lesssim \\Vert \\mathbf{u} \\Vert_V.\n\\]\nThis ends the proof since $V_h$ has finite dimension.\n\\end{proof}\n\n\\begin{corollary} \\label{corollary:pertubedestimate}\nIf assumption \\eqref{eq:regularity35} holds then for $h \\leq h_0$ given by Theorem \\ref{th:wellposednesspertubeddiscrete},\nand for $\\mathbf{u}$ (resp. $\\mathbf{u_h}$) the solution of the continuous problem \\eqref{eq:defmixedcontinuous} (resp. of the discrete problem \\eqref{eq:defmixeddiscrete}) \nit holds:\n\\begin{equation*}\n\\begin{aligned}\n\\Vert \\mathbf{u}& - \\mathbf{u_h} \\Vert_V \\lesssim \\\\\n& \\inf_{v_1 \\in V^1_h} \\Vert u_1 - v_1 \\Vert_{V^1} + \\inf_{v_2 \\in V^2_h} \\Vert u_2 - v_2 \\Vert_{V^2} + E(u_3) + E(u_p) + \\mu_0 E(P_\\mathfrak{B} u_3).\n\\end{aligned}\n\\end{equation*}\n\\end{corollary}\n\\begin{proof}\nThe proof is the same as in Theorem \\ref{th:unpertubedestimate}\n\\end{proof}\n\n\\begin{remark}\nThe hidden constant of Corollary \\ref{corollary:pertubedestimate} depends on $\\Vert l_5 \\Vert$ which, in the case of problem \\eqref{eq:NS_Eulercont}, blows up when $\\delta t \\to 0$. \nA more subtle analysis is required to explicit the dependency of the error on $\\delta t$.\nFor a single time step in the setting of Corollary \\ref{corollary:pertubedestimate} the error will actually decrease when $\\delta t \\to 0$.\n\\end{remark}\n\n\\section{Conserved quantities} \\label{Conservedquantities}\nLastly we prove that our scheme does indeed verify properties mentioned in the introduction as well as the regularity assumption \\eqref{eq:regularity35}.\n\n\\subsection{Regularity assumptions} \\label{Regularityassumptions}\nProblem \\eqref{eq:NS_Eulercont} is a special case of Problem \\eqref{eq:defmixedcontinuous} taking suitable $l_3$ and $l_5$.\nWe prove below that assumptions \\eqref{eq:regularity35} are valid if $u^{n-1} \\in H^2(\\Omega)$ and \nif the domain is smooth enough to have $H_0(\\mathrm{curl},\\Omega) \\cap H(\\mathrm{div},\\Omega) \\subset H^1(\\Omega)$, as discussed in \\cite[Chapter~3.2]{GiraultRaviart}.\nWe use the notation $H^1(\\Omega)$ both for scalar and for vector fields.\nFirst we need the following lemma:\n\\begin{lemma} \\label{lemma:sobolev}\nIf $B \\in H^1(\\Omega)$ and $A \\in H^2(\\Omega)$ then $A \\times B \\in H^1(\\Omega)$ for a smooth enough domain $\\Omega$ of $\\mathbb{R}^3$ (or $\\mathbb{R}^2$).\n\\end{lemma}\n\\begin{proof}\nWe have $H^1(\\Omega) \\subset L^4(\\Omega)$ and $H^2(\\Omega) \\subset C^{0}(\\widebar{\\Omega})$\nby Sobolev Embedding theorems\nthus $\\forall i,j,k \\in \\lbrace x,y,z\\rbrace$, $\\partial_i A_j \\in L^4(\\Omega)$, $\\partial_i B_j \\in L^2(\\Omega)$ and terms of the form $A_i\\, \\partial_j B_k$ \nare the product of a bounded function with a function in $L^2(\\Omega)$ and those of the form $B_i\\, \\partial_j A_k$ are the product of two functions in $L^4(\\Omega)$.\n\\end{proof}\nGoing back to problem \\eqref{eq:NS_Eulercont}, we take\n$V^1 = H(\\mathrm{curl},\\Omega)$, $V^2 = H(\\mathrm{div},\\Omega)$, $V^3 = L^2(\\Omega)$ and $\\mathfrak{H}^3 = {0}$.\nRegarding assumption \\eqref{eq:regularity35},\n$K$ is a bounded isomorphism from $L^2 \\times L^2$ to $(H(\\mathrm{div}) \\cap H_0(\\mathrm{curl})) \\times H^1_0 \\subset H^1 \\times H^1$.\nIf we assume $u \\in H^2$\nand $v \\in H^1$ then using the scalar triple product we have $\\forall \\sigma \\in H(\\mathrm{curl})$: \n\\[2 \\langle l_3 \\sigma, v \\rangle = \\int (\\sigma \\times u^{n-1}) \\cdot v = \\int (u^{n-1} \\times v) \\cdot \\sigma = 2 \\langle \\sigma, l_3^* v \\rangle. \n\\]\nThus $l_3^* v = \\frac{1}{2} u^{n-1} \\times v$ on $H^1$, $l_3^*$ maps $H^1$ into itself by Lemma \\ref{lemma:sobolev}, and \n\\[l_3^* (K)_2 (W) \\subset H^1 \\subset H(\\mathrm{curl}) = V^1.\n\\]\nThus we have \n$\\Vert (\\nabla \\times) l_3^* (K)_2 \\Vert_{W \\rightarrow L^2} \\lesssim \\Vert (K)_2 \\Vert_{W \\rightarrow H^1}$\nand by the boundedness of $\\Vert K \\Vert_{L^2 \\times L^2 \\rightarrow H^1 \\times H^1} $ we get the boundedness of $\\Vert d l_3^* (K)_2 \\Vert_{W \\rightarrow L^2}$.\nFinally we have $\\nu d d^* (K)_2 = I - d l_3^* (K)_2 + l_5^* (K)_2 + d^* (K)_3$ as distributions.\nFrom the $L^2 \\rightarrow L^2$ boundedness of the right-hand side we get both $\\nu d^* (K)_2 (W) \\subset H(\\mathrm{curl}) = V^1$ and the boundedness of $\\Vert d d^* (K)_2 \\Vert_{W \\rightarrow L^2}$.\n\nThe same argument applied to $\\widebar{L}^{-1}$ shows that $d^* (\\widebar{L}^{-1})_2(W) \\subset V^1$. \\\\\nHence \\eqref{eq:regularity35} is fulfilled.\n\n\\begin{remark}\nAssuming $u^{n-1} \\in H^2(\\Omega)$ is very mild as any solution $u$ of \\eqref{eq:defmixedprimal} must have $\\nabla \\times u \\in H(\\mathrm{curl},\\Omega)$, $\\nabla \\cdot u = 0$ thus $\\Delta u \\in L^2(\\Omega)$.\nHence by elliptic regularity for $\\Omega$ smooth enough and if $u$ satisfies appropriate boundary conditions then $u \\in H^2$.\n\\end{remark}\n\n\\subsection{Pointwise vanishing divergence}\nThis is a simple fact that follows from the use of a discrete subcomplex.\nThe operator $d_h = \\mathrm{div}$ used in the formulation is simply the restriction of the continuous operator to the discrete space $V_h^2$ \nand its image is contained in the discrete space $V_h^3$. \nThis holds even for condition \\eqref{eq:bc2} and \\eqref{eq:bc3} which does not give a complex structure.\nFrom \\eqref{eq:NS_Eulercont} we have $\\forall q_h \\in V^3_h$, $\\langle \\nabla \\cdot u_h + \\phi_h, q_h \\rangle = 0$ \nwith $\\nabla \\cdot u_h \\perp \\phi_h$ and $\\nabla \\cdot u_h \\in V^3_h$ by construction.\nTherefore, taking $q_h = \\nabla \\cdot u_h$ we have $\\langle \\nabla \\cdot u_h + \\phi_h, \\nabla \\cdot u_h \\rangle = \\Vert \\nabla \\cdot u_h \\Vert^2 = 0$.\n\n\\subsection{Pressure-robustness} \\label{Pressurerobustness}\nA scheme is called pressure-robust (\\cite{zbMATH07185373,Linke2020,Lederer2019}) if \nonly the pressure (and not the velocity) changes when the external forces acting on the system are modified by a gradient.\nThis property is only valid if \nthere are no harmonic $2$-forms, i.e.\\ if the domain is simply connected.\n\nEvery vector field $f \\in L^2(\\Omega)$ can be written as $\\nabla \\times g + \\nabla p$\nfor some fields $g$ and $p$.\nIn a bounded domain we only have uniqueness with correct boundary conditions on $g$ and $p$.\nAs long as these boundary conditions match the ones given in the complex, \nwe have by viewing $f$ as a 2-form, $ \\nabla \\times g = P_\\mathfrak{B} f$ \nand $\\nabla p = P_{\\mathfrak{B}^*} f$.\nLet $\\bar{f} \\in L^2(\\Omega)$ be another source term and\nwrite $(\\omega_h,u_h,p_h,\\phi_h)$ (resp. $(\\bar{\\omega}_h,\\bar{u}_h,\\bar{p}_h,\\bar{\\phi}_h)$)\nthe solution of \\eqref{eq:defmixeddiscrete} (resp. \\eqref{eq:defmixeddiscrete}) for the external force $(f,0)$ (resp. $(\\bar{f},0)$).\n\\begin{theorem}\nIf $f$ and $\\bar{f}$ are such that $\\exists p \\in V^3$, $\\bar{f} = f + \\nabla p$ then\n$\\omega_h = \\bar{\\omega}_h$ and $u_h = \\bar{u}_h$.\n\\end{theorem}\n\\begin{proof}\nIf there is $p \\in V^3$ such that $f + \\nabla p = \\bar{f}$ then $P_\\mathfrak{B} (f - \\bar{f}) = 0$\nso $\\forall g \\in V^1, \\langle f - \\bar{f}, \\nabla \\times g \\rangle = 0$ and in particular\n\\[\\forall g_h \\in V^1_h \\subset V^1,  \\langle f - \\bar{f}, \\nabla \\times g_h \\rangle = 0\\ .\n\\]\nTherefore $P_{\\mathfrak{B}_h} (f - \\bar{f}) = 0$ and since we assumed that there were no harmonic $2$-forms,  $P_h (f - \\bar{f}) = P_{\\mathfrak{B}_h^*} (f - \\bar{f})$.\nThus we can find $\\xi_h \\in V^3_h$ such that $\\xi_h \\perp \\mathfrak{H}_h$ and $d^*_h \\xi_h = - P_h (f - \\bar{f})$.\nMoreover $(0,0,\\xi_h,0)$ verifies:\n$\\forall (\\tau_h,v_h,q_h,\\chi_h) \\in V^1_h \\times V^2_h \\times V^3_h \\times \\mathfrak{H}_h$:\n\\begin{equation*}\n\\begin{aligned}\n\\langle 0,\\tau_h \\rangle - \\langle 0,\\nabla \\times \\tau_h \\rangle =&\\ 0, \\\\\n\\langle \\nu d 0 + l_3 0 + l_5 0, v_h \\rangle - \\langle \\xi_h, \\nabla \\cdot v_h \\rangle)\n=&\\ \\langle f - \\bar{f},v_h \\rangle, \\\\\n\\langle \\nabla \\cdot 0,q_h \\rangle + \\langle 0,q_h \\rangle  =&\\ 0,\\\\\n\\langle \\xi_h,\\chi_h \\rangle =&\\ 0.\n\\end{aligned}\n\\end{equation*}\nTherefore by linearity and uniqueness of the solution we have \\\\$(\\omega_h,u_h,p_h,\\phi_h) = (\\bar{\\omega}_h,\\bar{u}_h,\\bar{p}_h,\\bar{\\phi}_h) + (0,0,\\xi_h,0)$\nand  $\\omega_h = \\bar{\\omega}_h, u_h = \\bar{u}_h$.\n\\end{proof}\n\\begin{remark}\nConsider a time iterating scheme. \nAt any time step $n$, the linear maps $l_3^n$, $l_5^n$ involved in the determination of $(\\omega^n_h,u^n_h,p^n_h,\\phi^n_h)$ \nare function of $u^{n-1}_h$ and $\\omega^{n-1}_h$ (more precisely $l_3^n = l_3(u^{n-1}_h)$ and $l_5^n = l_5(\\omega^{n-1}_h)$).\nHowever since both $\\omega_h$ and $u_h$ remain unchanged when the external force is modified by a gradient, \nas long as $u^0_h = \\bar{u}^0_h$ and $\\omega^0_h = \\bar{\\omega}^0_h$ an immediate recursion shows that for all step $n$, $u^n_h = \\bar{u}^n_h$ and $\\omega^n_h = \\bar{\\omega}^n_h$.\n\\end{remark}\n\n\\subsection{Pressure independent estimate}\nThe pressure-robustness allows us to remove the pressure from the error estimate.\n\\begin{theorem}[Pressure-robust estimate] \\label{th:pressurerobustestimate}\nLet $(\\omega,u,p,\\phi) = (u_1,u_2,u_3,u_p)$ be the solution of the continuous problem \\eqref{eq:defmixedcontinuous}\nand $(\\omega_h,u_h,p_h,\\phi_h) = (u_{1h},u_{2h},u_{3h},u_{ph})$ be the solution of the discrete problem \\eqref{eq:defmixeddiscrete}.\nThen it holds:\n\\begin{equation*}\n\\Vert \\omega - \\omega_h \\Vert +  \\Vert \\nabla \\times (\\omega - \\omega_h) \\Vert +\\Vert u - u_h \\Vert \\lesssim\n\\inf_{\\chi \\in V^1_h} \\Vert \\omega - \\chi \\Vert_{V^1} + \\inf_{v \\in V^2_h} \\Vert u - v \\Vert_{V^2}.\n\\end{equation*}\n\\end{theorem}\n\\begin{proof}\nConsider an alternative problem with the same $l_3$ and $l_5$ as before but with the source term replaced by $\\tilde{f_2} := f_2 - \\nabla p$.\nLet $(\\tilde{\\omega},\\tilde{u},\\tilde{p},\\tilde{\\phi})$ and $(\\tilde{\\omega}_h,\\tilde{u}_h,\\tilde{p}_h,\\tilde{\\phi}_h)$ be respectively the continuous and discrete solution to this alternative problem.\nBy construction, we have $\\tilde{p} = 0$, $\\tilde{\\phi} = 0$.\nHence the estimate of Corollary \\ref{corollary:pertubedestimate} gives:\n\\begin{equation*}\n\\Vert \\tilde{\\omega} - \\tilde{\\omega}_h \\Vert +  \\Vert \\nabla \\times (\\tilde{\\omega} - \\tilde{\\omega_h}) \\Vert +\\Vert \\tilde{u} - \\tilde{u_h} \\Vert \\lesssim\n\\inf_{\\chi \\in V^1_h} \\Vert \\tilde{\\omega} - \\chi \\Vert_{V^1} + \\inf_{v \\in V^2_h} \\Vert \\tilde{u} - v \\Vert_{V^2}.\n\\end{equation*}\nWe conclude since by the pressure-robustness we must have:\n\\begin{equation*}\n\\tilde{\\omega} = \\omega,\\ \\tilde{u} = u,\\ \n\\tilde{\\omega}_h = \\omega_h,\\ \\tilde{u}_h = u_h\\ .\n\\end{equation*}\n\\end{proof}\n\n\\section{Numerical simulations} \\label{Numericalsimulations}\nWe give the results of three numerical simulations done to demonstrate the validity of our scheme.\nWe checked the norm of the divergence of the fluid velocity at every time step, and we found a value vanishing up to machine precision.\nThe first simulation aims to verify the pressure-robustness property,\nthe second is based on an exact and fully 3D solution of the Navier-Stokes equation constructed by Ethier \\cite{Ethier1994}.\nWe use it to check the convergence rate in space, first on a steady problem then on an unsteady problem.\nThe last simulation focuses on a system of two rotating cylinders and shows the good agreement with the theory on the value of the critical speed of the inner cylinder \nat which Taylor vortices appear.\nIt is based upon \\cite{aitmoussa,Gebhardt1993,Hoffmann2005}.\nIn any case, we took a unit kinematic viscosity and polynomials of degree $2$.\nOur codes are written with the FEniCS computing platform, version 2019.1.0 \n(See \\url{fenicsproject.org} and \\cite{FEniCS})\nand are available at \\url{https://github.com/mlhanot/Navier-Stokes-feec}. \n\n\\subsection{Pressure robustness}\nWe wish to verify that if the external forces acting on two flows differ only by a gradient,\nthen only the pressure differs between the flows.\nWe took the Stokes no-flow problem in a ($3$-dimensional) glass (see \\cite{SMAI-JCM_2019__5__89_0}).\nThe setup is rather simple, the mesh is a cylinder along the $z$ axis of height \\num{2.}, base radius \\num{1.} and top radius \\num{1.5} and the force $f$ derives from a potential:\n\\[f = \\frac{\\nabla \\Phi} {\\int_{\\Omega} \\Phi}, \\Phi = z^\\gamma\n\\]\nfor $\\gamma = 1,2,4,7$.\nWe start from a fluid at rest and enforce a no slip condition on the whole boundary.\nIn every case we found a velocity equal to zero at the order of the machine accuracy.\nThis is not trivial as the same test conducted with Taylor-Hood elements ($P_2/P_1$) gave a velocity of norm up to \\num{3.3e-4} for $\\gamma = 7$.\n\n\\subsection{Convergence rate to an exact solution}\nWe have conducted a convergence analysis with an exact solution.\nThe expression for the solution is given by Ethier \\cite{Ethier1994} and depends on two real parameters $a$ and $b$. It is given by:\n\\[ u = \\begin{bmatrix} \n-a (\\exp(a x)\\sin(a y + d z) + \\exp(a z)\\cos(a x + d y))\\exp(-d^2 t)\\\\ \n-a (\\exp(a y)\\sin(a z + d x) + \\exp(a x)\\cos(a y + d z))\\exp(-d^2 t)\\\\\n-a (\\exp(a z)\\sin(a x + d y) + \\exp(a y)\\cos(a z + d x))\\exp(-d^2 t)\n\\end{bmatrix}. \n\\]\nWe have performed two sets of experiments: the first with $a = 2$ and $d = 0$ and the \nsecond with $a = 2$ and $d = 1$. \nThe domain consists of a cylinder of height $2$ and radius $1$.\nIn the latter case computations were done for $t$ between $0$ and $1$ \nwith a time step of \\num{1.e-3}.\nWe set the velocity to be equal to the velocity of the exact solution on the boundary (without enforcing any boundary condition on the vorticity).\nFor the stationary case ($d = 0$) we start from a fluid at rest, otherwise we start from the exact solution at $t = 0$.\nWe found a rate of convergence in space for the velocity of order $2.0$ in both cases which is in agreement with the theory. \nFigure \\ref{fig:Convrate} shows the convergence of the velocity in the relative $H(\\text{div})$-norm $\\frac{\\Vert u_h - u \\Vert_{H(\\text{div})}}{\\Vert u \\Vert_{H(\\text{div})}}$\n(which is the same as the $L^2$-norm since the solution is exactly divergence free) \nwith a log-log scale.\n\\begin{remark}\nDespite the fact that the time discretization is Euler implicit and only first order \nby taking the time step small enough we can observe the second order convergence rate in space.\n\\end{remark}\n\\begin{figure}\n\\centering\n\\begin{tikzpicture}[scale=0.65]\n\\begin{loglogaxis}[grid, xlabel=h, ylabel=Error, legend style={at={(0.0,1.0)},anchor=north west},\nxtick={0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9},\nxticklabels={0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9},\nytick={0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01},\nyticklabels={\\pgfmathprintnumber[sci]{0.001},\\pgfmathprintnumber[sci]{0.002},,,,,,,,\\pgfmathprintnumber[sci]{0.01}},\nyticklabel style={/pgf/number format/.cd,sci}\n]\n\\addplot plot coordinates {\n(0.3701523923234168, 0.010096232860397174)\n(0.2818874705382209, 0.0059842337475502399)\n(0.22402549828914003, 0.0037725667958979302)\n(0.1873969335628237, 0.0026026329284828472)\n(0.16111550980350983, 0.0019131416970179491)\n(0.14123099900924302, 0.0014777998363669485)\n};\n\\addplot plot coordinates {\n(0.37015239232341673, 0.0116568662938769)\n(0.2790259042326483, 0.0065680806462421158)\n(0.22581158624630765, 0.0042434222824466843)\n(0.1875652678647778, 0.0029236973946968718)\n(0.16089234492850563, 0.0021669763798863981)\n(0.1407680298887053, 0.0016708889368816321)\n};\n\\legend{$a = 2$, $d=0$\\\\$a=2$, $d=1$\\\\}\n\\logLogSlopeTriangle{0.90}{0.4}{0.1}{2}{black};\n\\end{loglogaxis}\n\\end{tikzpicture}\n\\caption{Convergence rate of the velocity with relative $H(\\text{div})$ error on a log log scale.}\n\\label{fig:Convrate}\n\\end{figure}\n\\subsection{Taylor-Couette flow} \\label{TaylorCouetteflow}\n\\begin{figure}\n\\centering\n\\includegraphics[width=0.9\\textwidth]{5072}\n\\caption{Isolines for azimuthal velocity and the pressure for $Re_o = 0$ at $Re_i = 50$ (a) and $Re_i = 72$ (b). }\n\\label{fig:5072}\n\\end{figure}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=1\\textwidth]{Stream}\n\\caption{Stream function on cross section $y = 0$, $x > 0$ for $\\eta = 0.5$, $Re_o = 0$ at $Re_i = 50$ (top) and $Re_i = 72$ (bottom).}\n\\label{fig:Stream}\n\\end{figure}\n\n\\begin{figure}\n\\centering\n\\includegraphics[width=1\\textwidth]{Streame08}\n\\caption{Stream function on cross section $y = 0$, $x > 0$ for $\\eta = 0.8$, $Re_o = 0$ at $Re_i = 90$ (top), $Re_i = 95$ and $Re_i = 100$ (bottom). }\n\\label{fig:Streame08}\n\\end{figure}\n\n\\begin{figure}\n\\begin{minipage}[b]{0.48\\textwidth}\n\\centering\n\\includegraphics[width=1.0\\linewidth]{3}\n\\caption{Comparison of the critical value of $Re_i$ at $Re_o = 0$ for various $\\eta$.}\n\\label{fig:3}\n\\end{minipage}\n\\begin{minipage}[b]{0.48\\textwidth}\n\\centering\n\\includegraphics[width=1.0\\linewidth]{5}\n\\caption{Comparison of the critical value of $Re_i$ at $\\eta = 0.5$ for various $Re_o$.}\n\\label{fig:5}\n\\end{minipage}\n\\end{figure}\nThis test focuses on Taylor-Couette flow.\nWe follow the work of Gebhardt et al.\\ \\cite{aitmoussa,Gebhardt1993,Hoffmann2005}.\nThe geometry consists in two concentric cylinders of constant radius $R_i$ for the inner and $R_o$ for the outer,\nrotating at an angular velocities $\\Omega_i$ and $\\Omega_o$ respectively, and both of height $a$.\nThe system is closed by two fixed lids at the bottom and top ends. We characterized the system by two geometric parameters: \n$\\eta = R_i/R_o$ and $\\Lambda = a/d$ with the gap $d = R_o - R_i$.\nWe also need to define two quantities: the inner Reynolds number $Re_i = \\Omega_i R_i d /\\nu$ and the outer Reynolds number $Re_0 = \\Omega_o R_o d/ \\nu$ \nwhere $\\nu$ is the kinematic viscosity.\nIt is a well known fact that (for an infinite height $a$) at low speed the flow is steady and fully azimuthal and that vortices start to form at a critical speed.\nSince $a$ is finite we expect to see vortices near the lids for speeds way under the critical value (they are however fundamentally different from the Taylor vortices, see \\cite{Hoffmann2005}).\n\nWe compare results obtained from our code with the reference \\cite{Gebhardt1993,Hoffmann2005}. \nThe simulations are done starting from a fluid at rest with no slip boundary condition and studied at $t = 0.1$. \nThe time step depends on the angular velocities but is most often at $\\delta t = 0.001$ and only decreased for the highest velocities.\nWe check the value of $Re_i$ at which the transition occurs for various values of $Re_o$ and $\\eta$.\nWe see very good agreement, though we used a much coarser mesh and a smaller aspect ratio $\\Lambda$ of $10$ instead of $20$, for computational cost reasons.\nWe display some values taken on the half plane $y = 0, x > 0$. Figure \\ref{fig:5072} shows a comparison between the azimuthal velocity and the pressure for two values of $Re_i$ at $\\eta = 0.5$ and $Re_o = 0$.\nIn Figure \\ref{fig:Stream} we see the stream function (the azimuthal component of the vector potential) on the same system.\nStream functions for three values of $Re_i$ at $\\eta = 0.8$ are shown in Figure \\ref{fig:Streame08}.\nLastly Figures \\ref{fig:3} and \\ref{fig:5} give the aforementioned comparison, our results being shown in red and the reference curve in black.\n\n\\section*{Acknowledgements} I thank Pascal Azerad for his\ncomments and helpful advice.\nI also thank Kaibo Hu for his comments and suggestions\nand the anonymous referees for many helpful remarks.\n\n\\printbibliography\n\n\\end{document}\n\n", 0.7909461537096657], ["\\section{Introduction}\nThe free central limit theorem (due to Voiculescu \\cite{VoiZGW} in the one-dimensional\ncase, and to Speicher \\cite{SpeZGW} in the multivariate case) is one of the basic results\nin free probability theory. Investigations on the speed of convergence to the limiting\nsemicircular distribution, however, were taken up only recently. In the classical\ncontext, the analogous question is answered by the famous Berry-Esseen theorem, which\nstates, in its simplest version, the following: If $X_i$ are i.i.d. random variables,\nwith mean zero and variance 1, then the distance between $S_n:=(X_1+\\cdots+X_n)/\\sqrt n$\nand a normal variable $\\gamma$ of mean zero and variance 1 can be estimated in terms of\nthe Kolmogorov distance $\\Delta$ by\n$$\\Delta(S_n,\\gamma)\\leq C\\frac 1{\\sqrt n} \\rho,$$\nwhere $C$ is a constant and $\\rho$ is the absolute third moment of the variables $x_i$.\n\nThe question for a free analogue of the Berry-Esseen estimate in the case of one random\nvariable was answered by Chistyakov and G\\\"otze \\cite{CG}: If $x_i$ are free identically\ndistributed random variables with mean zero and variance 1, then the distance between\n$S_n:=(X_1+\\cdots+X_n)/\\sqrt n$ and a semicircular variable $s$ of mean zero and variance\n1 can, under the assumption of finite fourth moment, be estimated as\n$$\\Delta(S_n,s)\\leq c \\frac {\\vert m_3\\vert +\\sqrt {m_4}}{\\sqrt n},$$\nwhere $c>0$ is an absolute constant, and $m_3$ and $m_4$ are the third and fourth moment,\nrespectively, of the $x_i$. (Independently, the same kind of question was considered,\nunder the more restrictive assumption of compact support for the $x_i$, by Kargin\n\\cite{K}.)\n\nIn this paper we want to address the multivariate version of a free Berry-Esseen theorem.\nIn contrast to the classical situation, the multivariate situation is of a quite\ndifferent nature than the one-dimensional case, because we have to deal with\nnon-commuting operators and all the analytical tools, which are available in the\none-dimensional case, break down. However, we are able to deal with this situation by\ninvoking recent ideas of Haagerup and Thorbjornsen \\cite{HT,HST}, in particular, their\nlinearization trick which allows to reduce the multivariate (scalar-valued) to an\nanalogous one-dimensional operator-valued problem. Estimates for the operator-valued\nCauchy transform of this operator-valued operator are quite similar to estimates in the\nscalar-valued case. Actually, on the level of deriving equations for these Cauchy\ntransforms we can follow ideas which are used for dealing with speed of convergence\nquestions for random matrices; here we are inspired in particular by the work of G\\\"otze\nand Tikhomirov \\cite{GT}, but see also \\cite{B1,B2}. Our main theorem on the speed of\nconvergence in an operator-valued free central limit theorem is the following.\n\n\\begin{theorem}\\label{thm:operator-valued}\nLet $1\\in\\mathcal{B}\\subset\\mathcal{A}$, $E:\\mathcal{A}\\to\\mathcal{B}$ be an operator-valued probability space. Consider\nselfadjoint $X_1,X_2,\\dots \\in\\mathcal{A}$ which are free with respect to $E$ and have identical\n$\\mathcal{B}$-valued distribution. Assume that the first moments vanish,\n$$E[X_i]=0$$\nand let\n$$\\eta:\\mathcal{B}\\to\\mathcal{B},\\qquad \\eta(b)=E[X_ibX_i]$$\nbe their covariance. Denote\n$$\\alpha_2:=\\sup_{b\\in\\mathcal{B}\\atop \\Vert b\\Vert =1} \\Vert E[X_ibX_i]\\Vert=\\Vert \\eta\\Vert$$\nand\n$$\\alpha_4:=\\sup_{b\\in\\mathcal{B}\\atop \\Vert b\\Vert=1}\\Vert E[X_ibX_iX_ib^*X_i]\\Vert.$$\nConsider now the normalized sums\n$$S_n:=\\frac{X_1+\\cdots+X_n}{\\sqrt n}$$\nand their $\\mathcal{B}$-valued Cauchy transforms\n$$G_n(b):=E[\\frac 1{b-S_n}] \\qquad (b\\in\\mathcal{B}_+)$$\non the ``upper half plane'' $\\mathcal{B}_+$ in $\\mathcal{B}$,\n$$\\mathcal{B}_+:=\\{b\\in \\mathcal{B}\\mid \\Im b\\geq 0 \\text{ and $\\Im b$ invertible}\\}.$$\nBy $G$ we denote the operator-valued Cauchy transform of a $\\mathcal{B}$-valued semicircular\nelement with covariance $\\eta$.\n\nThen we have for all $b\\in\\mathcal{B}_+$ and all $n\\in {\\mathbb N} $ that\n\\begin{equation}\n\\Vert G_n(b)-G(b)\\Vert\\leq 4c_n(b)\\left(\\Vert b\\Vert +\\alpha_2 \\cdot\\Vert \\frac 1{\\Im\nb}\\Vert\\right)\\cdot \\Vert \\frac 1{\\Im b}\\Vert^2,\n\\end{equation}\nwhere\n$$c_n(b):=\\frac 1{\\sqrt n} \\bigl\\Vert\\frac 1{\\Im b}\\bigr\\Vert^3\\sqrt{\\alpha_2}\n\\cdot(2\\alpha_2+\\sqrt{\\alpha_4+2\\alpha_2^2})+\\frac 1n \\bigl\\Vert\\frac 1{\\Im\nb}\\bigr\\Vert^4\\alpha_2^2.\n$$\n\\end{theorem}\n\n\nIn the one-dimensional scalar case one can derive from such estimates corresponding\nestimates for the Kolmogorov distance between the distribution of $S_n$ and the limiting\nsemicircle $s$. This relies on the fact that the Kolmogorov metric measures how close the\ndistribution functions of two measures are, and the Stieltjes inversion formula allows to\nrelate the distribution function with Cauchy transforms. (In the proof of the classical\nBerry-Esseen theorem one follows a similar route, using Fourier transforms instead of\nCauchy transforms.) For the multivariate case, say of $d$ variables, where we would like\nto say something about the speed of convergence of the $d$-tuple of partial sums\n$(S_n^{(1)},\\dots,S_n^{(d)})$ to the limiting semicircular family $(s_1,\\dots,s_d)$,\nthere is no nice replacement for the distribution function, and we also do not know of a\ncanonical metric on joint distributions of several non-commuting variables which relates\ndirectly with the above estimates for operator-valued Cauchy transforms.\n\n\nHowever, there is a kind of replacement for this; namely, following again \\cite{HST},\nestimates for Cauchy transforms of linear combinations with operator-valued coefficients\nof the variables $(S_n^{(1)},\\dots,S_n^{(d)})$ should imply corresponding estimates for\nany non-commutative scalar polynomial in those variables and from those one should be\nable to estimate, for any selfadjoint non-commutative polynomial $p$, the Levy distance\nbetween $p(S_n^{(1)},\\dots,S_n^{(d)})$ and $p(s_1,\\dots,s_d)$. However, one has to deal\nwith the following problem in such an approach: as is shown in \\cite{HST} one can get the\nCauchy transform of a polynomial $p(s_1,\\dots,s_d)$ as a corner of an operator-valued\nCauchy transform of a linear combination $P$, with matrix-valued coefficients, of\n$s_1,\\dots,s_d$; but, even if $p$ is a selfadjoint polynomial, the corresponding\nmatrix-valued operator $P$ is not selfadjoint, and thus our operator-valued estimates,\nwhich were only shown for selfadjoint $X$, cannot be used directly for $P$; one would\nhave to reprove most of our statements also for $P$. It is conceivable that this can be\ndone in a similar manner as in \\cite{HST}; as this approach is getting quite technical,\nwe will pursue the details in a forthcoming investigation.\n\nNote that for proving such a kind of Berry-Esseen theorem for polynomials\n$p(s_1,\\dots,s_d)$ one also has to face another kind of question: estimates for the\ndifference of Cauchy transforms translate directly only in estimates for the Levy\ndistance between the corresponding measures; in order to get also estimates for the more\nintuitive Kolmogorov distance one needs to know that the distribution of\n$p(s_1,\\dots,s_d)$ has a continuous density, in particular, has no atoms. We conjecture\nthat this is true for all non-commutative selfadjoint polynomials $p$ in a semicircular\nfamily, but this seems to be a non-trivial problem. Note that the question of absence of\natoms can be seen as an analogue of the Zero-Divisor Theorem for the free group. We hope\nto address this question in some future work.\n\n\nThe paper is organized as follows. In the next section we will first relate a\nmultivariate free central limit theorem with a one-dimensional operator-valued free\ncentral limit theorem. The proof of Theorem \\ref{thm:operator-valued} will be given in\nSection \\ref{sect:operator}.\n\n\n\n\\section{Multivariate free central limit theorem}\n\\subsection{Setting}\\label{subsection:setting}\nLet $\\bigl(x^{(k)}_1\\bigr)_{k=1}^d,\\bigl(x^{(k)}_2\\bigr)_{k=1}^d,\\dots$ be free and\nidentically distributed sets of $k$ selfadjoint random variables in some non-commutative\nprobability space $(\\mathcal{C},\\varphi)$, such that the first moments vanish and the second moments\nare given by a covariance matrix $\\Sigma=(\\sigma_{kl})_{k,l=1}^d$. We put\n$$S^{(k)}_n=\\frac{x^{(k)}_1+\\dots+x^{(k)}_n}{\\sqrt n}.$$\nWe know \\cite{SpeZGW} that $(S_n^{(1)},\\dots,S_n^{(d)})$ converges in distribution for\n$n\\to\\infty$ to a semicircular family $(s_1,\\dots,s_d)$ of covariance $\\Sigma$. We want\nto analyze the rate of this convergence. We would like to get an estimate which involves\nonly small moments of the given variables. As we will see, the second and fourth moments\nof our variables will show up in the estimates and we will use the upper bound\n$$\\beta_2:=\\max_{k,l}\\vert\\sigma_{k,l}\\vert=\\max_{k,l}\\varphi(x_i^{(k)}x_i^{(l)})$$\nfor the second and the upper bound\n$$\\beta_4:=\\max_{r,p,k,l}\\vert\\varphi(x_i^{(r)}x_i^{(p)}x_i^{(k)}x_i^{(l)})\\vert$$\nfor the fourth moments.\n\n\\subsection{Transition to operator-valued frame}\\label{subsection:transition}\nWe will analyze the rate of convergence of the multivariate problem,\n$$(S_n^{(1)},\\dots,S_n^{(d)})\\to (s_1,\\dots,s_n)$$\nby replacing this by an\none-dimensional operator-valued problem. The underlying idea for that is the\nlinearization trick \\cite{HT,HST} that one can understand the joint distribution of\nseveral scalar random variables by understanding the distribution of each operator-valued\nlinear combination of those random variables.\n\nLet $\\mathcal{B}=M_N({\\mathbb C})$ and put $\\mathcal{A}:=M_N({\\mathbb C})\\otimes \\mathcal{C}=M_N(\\mathcal{C})$. Then $\\mathcal{B}\\cong\\mathcal{B}\\otimes\n1\\subset\\mathcal{A}$ is an operator-valued probability space with respect to the conditional\nexpectation\n$$\nE=\\text{id}\\otimes \\varphi:\\mathcal{B}\\otimes\\mathcal{C}\\to\\mathcal{B},\\qquad b\\otimes c\\mapsto \\varphi(c)b.\n$$\n\nFor some fixed $b_1,\\dots,b_k\\in M_N({\\mathbb C})$ we put\n$$X_i:=\\sum_{k=1}^d b_k\\otimes x_i^{(k)}$$\nand\n$$S_n:=\\sum_{k=1}^d b_k\\otimes S^{(k)}_n$$\nNote that $X_1,X_2,\\cdots$ are free with respect to $E$ and that we have\n$$S_n=\\frac{X_1+\\cdots+X_n}{\\sqrt n}.$$\nThe limit of $S_n$ is\n$$s:=\\sum_{k=1}^d b_k\\otimes s_k,$$\nwhich is an $\\mathcal{B}=M_N({\\mathbb C})$-valued semicircular element with covariance mapping\n$\\eta:\\mathcal{B}\\to\\mathcal{B}$ given by\n\\begin{align*}\n\\eta(b)=E[s b\\otimes 1 s]&=\\sum_{k,l=1}^d E[b_k\\otimes s_k\\cdot  b\\otimes 1\\cdot  b_l\n\\otimes s_l]\\\\&= \\sum_{k,l=1}^d b_k b b_l \\varphi(s_ks_l)= \\sum_{k,l=1}^d b_k b b_l\n\\sigma_{kl}.\n\\end{align*}\n\nWe want to determine the rate of convergence for $S_n$ to $s$. We will do this in the\nnext section in the context of a general operator-valued free central limit theorem.\n\n\\section{Rate of convergence for operator-valued free central limit\ntheorem}\\label{sect:operator}\n\n\\subsection{Setting} Let $1\\in\\mathcal{B}\\subset\\mathcal{A}$, $E:\\mathcal{A}\\to\\mathcal{B}$ be an operator-valued probability\nspace. This means that $\\mathcal{A}$ is a von Neumann algebra, $\\mathcal{B}$ is a sub von Neumann\nalgebra, which contains the identity of $\\mathcal{A}$, and $E$ is a conditional expectation from\n$\\mathcal{A}$ onto $\\mathcal{B}$, i.e., a linear map which satisfies the property\n$$E[b_1 a b_2]=b_1E[a]b_2$$\nfor all $a\\in\\mathcal{A}$ and $b_1,b_2\\in\\mathcal{B}$.\n\nConsider selfadjoint $X_1,X_2,\\dots \\in\\mathcal{A}$ which are free with respect to $E$ and have\nidentical $\\mathcal{B}$-valued distribution. Assume that the first moments vanish,\n$$E[X_i]=0$$\nand let\n$$\\eta:\\mathcal{B}\\to\\mathcal{B},\\qquad \\eta(b)=E[X_ibX_i]$$\nbe their covariance. We will need\n$$\\alpha_2:=\\sup_{b\\in\\mathcal{B}\\atop \\Vert b\\Vert =1} \\Vert E[X_ibX_i]\\Vert=\\Vert \\eta\\Vert$$\nand\n$$\\alpha_4:=\\sup_{b\\in\\mathcal{B}\\atop \\Vert b\\Vert=1}\\Vert E[X_ibX_iX_ib^*X_i]\\Vert.$$\nConsider now the normalized sums\n$$S_n:=\\frac{X_1+\\cdots+X_n}{\\sqrt n}.$$\nWe know that $S_n$ converges in distribution to an operator-valued semicircular element $s$\nwith covariance $\\eta$, see \\cite{Spe}\n\nWe want to estimate the rate of this convergence. Let us denote by $\\mathcal{B}_+$ the ``upper\nhalf plane'' in $\\mathcal{B}$, i.e.,\n$$\\mathcal{B}_+:=\\{b\\in \\mathcal{B}\\mid \\Im b\\geq 0 \\text{ and $\\Im b$ invertible}\\}.$$\nWe consider, for $b\\in\\mathcal{B}_+$, the resolvents\n$$R_n(b):=\\frac 1{b-S_n},\\qquad R(b):=\\frac 1{b-s}$$\nand the Cauchy transforms\n$$G_n(b):=E[R_n(b)],\\qquad G(b):=E[R(b)].$$\n$G_n$ and $G$ are analytic functions in $\\mathcal{B}_+$.\n\n\\subsection{The main estimates}\nWe will show that $G_n(b)$ converges to $G(b)$, where we have good control over the\ndifference in terms of $n$ and $b$. The idea for showing this is the same as in\n\\cite{HT}. First we show that $G_n$ satisfies an approximate version of an equation\nsatisfied by $G$ and then we show that this actually implies that $G_n$ and $G$ must be\nclose to each other.\n\nLet us start with deriving the equations for $G$ and $G_n$.\n\nSince $s$ is an operator-valued semicircular element with covariance $\\eta$ we know\n\\cite{Voi,Spe} that its Cauchy transform satisfies the equation\n\\begin{equation}\\label{eq:semi}\nb G(b)-1=\\eta\\left( G(b)\\right)\\cdot G(b).\n\\end{equation}\n\nWe want to derive an approximate version of this equation for $G_n$. For this, we will\nlook at $E[S_n R_n(b)]$.\n\nLet us denote by $S_n^{[i]}$ the version of $S_n$ where the $i$-th variable $X_i$ is\nabsent, i.e.,\n$$S_n^{[i]}:=S_n-\\frac 1{\\sqrt n} X_i,$$\nand by $R_n^{[i]}$ and $G_n^{[i]}$ the corresponding resolvent and Cauchy\ntransform, respectively, i.e.,\n$$R_n^{[i]}(b)=\\frac 1{b -S_n^{[i]}}$$\nand\n$$G_n^{[i]}(b):=E[R_n^{[i]}(b)].$$\nFor each $i=1,\\dots,n$ we have the resolvent identity\n\\begin{align*}\nR_n(b)&=R_n^{[i]}(b)+\\frac1{\\sqrt n} R_n^{[i]}(b)\\cdot  X_i \\cdot R_n^{[i]}(b) \\\\&\\quad+\n\\frac 1n R_n(b)\\cdot X_i\\cdot R_n^{[i]}(b)\\cdot X_i\\cdot R^{[i]}_n(b).\n\\end{align*}\n\nNow we can write\n\\begin{align*}\nE[S_nR_n(b)]&=\\sum_{i=1}^n E\\bigl[\\frac{X_i}\n{\\sqrt n}\\cdot R_n(b)\\bigr]\\\\\n&=\\sum_{i=1}^n\\frac 1{\\sqrt n} \\Bigl\\{ E\\bigl[X_i\n \\cdot R_n^{[i]}(b)\\bigr]\n\\\\&\\quad+\n\\frac 1{\\sqrt n} E\\bigl[X_i \\cdot R_n^{[i]}(b)\\cdot X_i \\cdot R_n^{[i]}(b)\\bigr]\\\\& \\quad\n+\\frac 1n E\\bigl[X_i\\cdot R_n(b)\\cdot X_i \\cdot R_n^{[i]}(b)\\cdot X_i\\cdot\nR^{[i]}_n(b)\\bigr]\\Bigr\\}\n\\end{align*}\n\nNow we use our assumption that $X_1,X_2,\\dots$ are free with respect to $E$, which\nimplies that $X_i$ is free from $R_n^{[i]}(b)$ with respect to $E$. This implies that\n$$\n E[X_i\n \\cdot R_n^{[i]}(b)]=\n E[X_i]\n \\cdot E[ R_n^{[i]}(b)]=0\n$$\nand\n\\begin{align*}\nE\\left[X_i \\cdot R_n^{[i]}(b)\\cdot X_i\\cdot R_n^{[i]}(b)\\right] &= E\\left[X_i \\cdot E[\nR_n^{[i]}(b)]\\cdot X_i\\right] \\cdot E\\left[R_n^{[i]}(b) \\right]\\\\&\\qquad + E[X_i] \\cdot\nE\\left[R_n^{[i]}(b)\\cdot E[X_i]\\cdot R_n^{[i]}(b) \\right]\\\\&\\qquad\n- E[X_i] \\cdot E[R_n^{[i]}(b)]\\cdot E[X_i]\\cdot E[R_n^{[i]}(b)]\\\\\n&= E\\left[X_i \\cdot E[ R_n^{[i]}(b)]\\cdot X_i\\right] \\cdot E[R_n^{[i]}(b)]\\\\\n&=\\eta\\left(G^{[i]}_n(b)\\right)\\cdot G^{[i]}_n(b).\n\\end{align*}\n\n\nSo we have got finally\n\\begin{equation}\\label{eq:five}\nE[S_n R_n(b)]=\\frac 1n\\left(\\sum_{i=1}^n \\eta\\left(G^{[i]}_n(b)\\right)\\cdot G^{[i]}_n(b)\n+r_1^{[i]}\\right),\n\\end{equation}\nwhere\n$$\nr_1^{[i]}= \\frac 1{\\sqrt n} E\\left[X_i \\cdot R_n(b)\n \\cdot X_i\\cdot  R_n^{[i]}(b)\\cdot\nX_i  \\cdot R^{[i]}_n(b)\\right]\n$$\n\nWe will now estimate the norm of $r_1^{[i]}$. We could of course just estimate against the\noperator norm of $X_i$; however, we prefer, in analogy with the classical case, to do\nbetter without invoking the operator norm and use only as small moments of $X_i$ as\npossible.\n\nNote that for our conditional expectation $E$ we have the Cauchy-Schwarz inequality\n$$\\Vert E[AB]\\Vert^2\\leq \\Vert E[AA^*]\\Vert\\cdot \\Vert E[B^*B]\\Vert,$$\nand also\n$$E[A]^*E[A]\\leq E[A^*A]\\qquad\\text{and}\\qquad\nE[ABB^*A^*]\\leq \\Vert BB^*\\Vert\\cdot E[AA^*]$$ and\n$$\\Vert E[A]\\Vert \\leq \\Vert A\\Vert$$\nfor any $A,B\\in\\mathcal{A}$. Thus, for any $i=1,\\dots,n$, we can estimate\n\\begin{align*}\n\\Vert E\\bigl[X_i\\, R_n(b)\\,&\n X_i\\, R_n^{[i]}(b)\\,\nX_i\\,  R^{[i]}_n(b)\\bigr]\\Vert^2 \\\\\n&\\leq \\Vert E\\bigl[X_i \\, R_n(b)\\, R_n(b)^* \\, X_i\\bigl]\\Vert\\cdot\n\\\\&\\quad\\cdot\n\\bigl\\Vert E\\bigl[ R_n^{[i]}(b)^*\\, X_i \\, R_n^{[i]}(b)^* \\, X_i\\, X_i\\, R_n^{[i]}(b)\\,\nX_i\\, R_n^{[i]}(b)\\bigl]\\bigr\\Vert\n\\end{align*}\n\nWe estimate the first factor by\n\\begin{align*}\n\\Vert E\\bigl[X_i \\, R_n(b)\\, R_n(b)^* \\, X_i\\bigl]\\Vert &\\leq \\Vert R_n(b)\\Vert^2\n\\cdot\\bigl\\Vert\n E\\bigl[X_i X_i\\bigl]\\bigr\\Vert\\\\\n&=\\Vert R_n(b)\\Vert^2\\cdot\\Vert \\eta(1)\\Vert\\\\\n&=\\alpha_2\\Vert R_n(b)\\Vert^2\n\\end{align*}\n\nFor the second factor we use again the freeness between $X_i$ and $R_n^{[i]}(b)$. Let us\nput\n$$R:=R_n^{[i]}(b)$$\n\nThen $X_i$ and $R$ are $*$-free with respect to $E$ and thus, by also invoking\n$E[X_i]=0$, we have\n\\begin{align*}\nE[R^*X_iR^*X_iX_iRX_iR]&= E\\Bigl[R^*\\cdot E\\bigl[X_i\\, E[R^*]\\, X_i\\, X_i\\, E[R]\\,\nX_i\\bigr]\\cdot R\\Bigr]\n\\\\\n&\\quad+\nE\\Bigl[R^*\\cdot\\eta\\bigl( E[R^*\\,\\eta(1)\\, R] \\bigr)\\cdot R\\Bigr]\\\\\n&\\quad- E\\Bigl[R^*\\cdot \\eta\\bigl(E[R^*]\\,  \\eta(1)\\, E[R]\\bigr)\\cdot R\\Bigr],\n\\end{align*}\nand thus\n\\begin{align*}\n\\left\\Vert E\\bigl[R^*X_iR^*X_iX_iRX_iR\\bigr]\\right\\Vert&\\leq \\left\\Vert E\\Bigl[R^*\\cdot\nE\\bigl[X_i\\, E[R^*]\\, X_i\\, X_i\\, E[R]\\, X_i\\bigr]\\cdot R\\Bigr]\\right\\Vert\n\\\\\n&\\quad+\\left\\Vert\nE\\Bigl[R^*\\cdot\\eta\\bigl( E[R^*\\,\\eta(1)\\, R] \\bigr)\\cdot R\\Bigr]\\right\\Vert \\\\\n&\\quad+ \\left\\Vert E\\Bigl[R^*\\cdot \\eta\\bigl(E[R^*]\\,  \\eta(1)\\, E[R]\\bigr)\\cdot\nR\\Bigr]\\right\\Vert\n\\end{align*}\nWe estimate\n\\begin{align*}\n&\\left\\Vert E\\Bigl[R^*\\cdot E\\bigl[X_i\\, E[R^*]\\, X_i\\, X_i\\, E[R]\\, X_i\\bigr]\\cdot\nR\\Bigr]\\right\\Vert\\\\ &\\qquad\\qquad\\qquad\\qquad\\leq\\Vert R\\Vert \\cdot \\Vert R^*\\Vert\\cdot\n\\bigl\\Vert E\\bigl[X_i\\, E[R^*]\\, X_i\\, X_i\\, E[R]\\,\nX_i\\bigr]\\bigr\\Vert\\\\\n&\\qquad\\qquad\\qquad\\qquad\\leq \\Vert R\\Vert^2\\cdot \\alpha_4 \\cdot \\Vert E[R]\\Vert\\cdot \\Vert E[R^*]\\Vert\\\\\n&\\qquad\\qquad\\qquad\\qquad\\leq \\alpha_4\\cdot \\Vert R\\Vert^4\n\\end{align*}\n\\begin{align*}\n\\left\\Vert E\\Bigl[R^*\\,\\eta\\bigl( E[R^*\\,\\eta(1)\\, R] \\bigr)\\, R\\Bigr]\\right\\Vert \\leq\n\\alpha_2^2\\cdot\\Vert R\\Vert^4,\n\\end{align*}\nand\n$$\n\\left\\Vert E\\Bigl[R^*\\cdot \\eta\\bigl(E[R^*]\\,  \\eta(1)\\, E[R]\\bigr)\\cdot\nR\\Bigr]\\right\\Vert \\leq\\alpha_2^2\\cdot \\Vert R\\Vert^4$$\n\nPutting this together yields\n$$\\left\\Vert E\\bigl[ R_n^{[i]}(b)^*\\, X_i \\, R_n^{[i]}(b)^* \\, X_i\\, X_i\\, R_n^{[i]}(b)\\,\nX_i\\, R_n^{[i]}(b)\\bigl]\\right\\Vert \\leq (\\alpha_4+2\\alpha_2^2)\\cdot \\Vert\nR_n^{[i]}(b)\\Vert^4,$$ and finally\n$$\\Vert r_1^{[i]}\\Vert \\leq \\frac 1{\\sqrt n}\\cdot \\sqrt{\\alpha_2(\\alpha_4+2\\alpha_2^2)}\\cdot\n\\Vert R_n(b)\\Vert\\cdot \\Vert R_n^{[i]}(b)\\Vert^2.$$\n\nWe still need to replace, in \\eqref{eq:five}, $G_n^{[i]}(b)=E[R_n^{[i]}(b)]$ by\n$G_n(b)=E[ R_n(b)]$. By using the resolvent identity\n$$R_n(b)=R_n^{[i]}(b)+\\frac 1{\\sqrt n} R_n^{[i]}(b)\\cdot X_i \\cdot R_n(b)$$\nwe have\n$$G_n^{[i]}(b)=G_n(b)+r_2^{[i]},$$\nwhere\n$$r_2^{[i]}:=-\n\\frac 1{\\sqrt n} E[R_n^{[i]}(b)\\, X_i \\, R_n(b)].$$ As before, we estimate\n\\begin{align*}\n\\Vert E[R_n^{[i]}(b)\\, X_i \\, R_n(b)]\\Vert^2 &\\leq \\Vert E[R_n^{[i]}(b)\\, X_i\\, X_i\\,\nR_n^{[i]} (b)^*]\\Vert\\cdot \\Vert E[R_n(b)^* \\, R_n(b)]\\Vert \\\\&\\leq \\alpha_2\\cdot \\Vert\nR_n^{[i]}(b)\\Vert^2\\cdot \\Vert R_n(b)\\Vert^2.\n\\end{align*}\n\nLet us summarize. We have\n\\begin{align*}\nE[S_n R_n(b)]&=\\frac 1n \\sum_{i=1}^n\\left( \\eta\\left(G^{[i]}_n(b)\\right)\\cdot G^{[i]}_n(b)\n+r_1^{[i]}\\right)\\\\\n&=\\frac 1n\\sum_{i=1}^n\\left( \\eta\\left(G_n(b)+r_2^{[i]}\\right)\\cdot\n\\left(G_n(b)+r_2^{[i]}\\right) +r_1^{[i]}\\right),\n\\end{align*}\nand the estimates\n$$\\Vert r_1^{[i]}\\Vert \\leq \\frac 1{\\sqrt n}\\cdot \\sqrt{\\alpha_2(\\alpha_4+2\\alpha_2^2)}\\cdot\n\\Vert R_n(b)\\Vert\\cdot \\Vert R_n^{[i]}(b)\\Vert^2$$ and\n$$\\Vert r_2^{[i]}\\Vert\\leq \\frac 1{\\sqrt n}\\sqrt{\\alpha_2}\\cdot\n  \\Vert R_n^{[i]}(b)\\Vert\\cdot \\Vert R_n(b)\\Vert.$$\nIt remains to estimate $\\Vert R_n(b)\\Vert$ and $\\Vert R_n^{[i]}(b)\\Vert$. For those we use\nthe usual estimate for Cauchy transforms (where $\\Im b:=(b-b^*)/(2i)$ denotes the\nimaginary part of $b$),\n$$\\Vert R_n(b)\\Vert\\leq \\Vert \\frac 1{\\Im b}\\Vert ,\\qquad\n\\Vert R_n^{[i]}(b)\\Vert\\leq \\Vert\\frac 1{\\Im b}\\Vert.$$ For a formal proof of this estimate,\nsee, e.g., Lemma 3.1 in \\cite{HT}.\n\nWe have now\n$$\nE[S_n R_n(b)]=\\eta\\left(G_n(b)\\right)\\cdot G_n(b) +r_3,$$ where\n$$r_3=\\frac 1n\\sum_{i=1}^n\\Bigl(\\eta(G_n(b))\\cdot r_2^{[i]}\n+\\eta(r_2^{[i]})\\cdot G_n(b)+\\eta(r_2^{[i]})\\cdot r_2^{[i]}+r_1^{[i]}\\Bigr).$$ Hence\n\\begin{align*}\n\\Vert r_3\\Vert\\leq \\frac 1n\\sum_{i=1}^n\\left( 2\\Vert\\eta\\Vert\\cdot \\Vert\nG_n(b)\\Vert\\cdot\\Vert r_2^{[i]}\\Vert + \\Vert \\eta\\Vert\\cdot \\Vert r_2^{[i]}\\Vert^2+\\Vert\nr_1^{[i]}\\Vert\\right)\\leq{c_n},\n\\end{align*}\nwhere\n$$c_n:=c_n(b):=\\frac 1{\\sqrt n} \\bigl\\Vert\\frac 1{\\Im b}\\bigr\\Vert^3\\sqrt{\\alpha_2}\n\\cdot(2\\alpha_2+\\sqrt{\\alpha_4+2\\alpha_2^2})+\\frac 1n \\bigl\\Vert\\frac 1{\\Im\nb}\\bigr\\Vert^4\\alpha_2^2.\n$$\n\nNote that $S_nR_n(b)=-1+b R_n(b)$, hence\n$$E[S_nR_n(b)]=b G_n(b)-1,$$\nand so we finally have found\n\\begin{equation}\\label{eq:G-n}\n\\eta(G_n(b))\\cdot G_n(b)-b G_n(b)+1=-r_3,\n\\end{equation}\nor the inequality:\n\\begin{equation}\\label{eq:ineq}\n\\Vert \\eta(G_n(b))\\cdot G_n(b)-b G_n(b)+1\\Vert\\leq c_n.\n\\end{equation}\n\nIn order to get from this an estimate for the difference between $G_n(b)$ and $G(b)$, we\nwill now follow the ideas in Section 5 of \\cite{HT}, in the improved version from\n\\cite{HST}.\n\nBy \\eqref{eq:semi}, we have for all $b\\in\\mathcal{B}_+$ the equation\n\\begin{equation}\\label{eq:Gfixed}\nb=\\frac 1{G(b)}+\\eta\\bigl(G(b)\\bigr)\n\\end{equation}\nfor $G(b)$, and, by \\eqref{eq:G-n}, the corresponding approximate version for $G_n(b)$:\n\\begin{equation}\\label{eq:Gnfixed}\n\\Lambda_n(b)=\\frac 1{G_n(b)}+\\eta\\bigl(G_n(b)\\bigr),\n\\end{equation}\nwhere\n$$\\Lambda_n(b):=b- r_3\\cdot G_n(b)^{-1}.$$\nA crucial point is now to show that for a sufficiently large set $\\tilde O_n\\subset\n\\mathcal{B}_+$ the quantity $\\Im \\Lambda_n(b)$ is still positive, so that we can also use\nequation \\eqref{eq:Gfixed} for $\\Lambda_n(b)$. Let us try\n\\begin{multline*}\n\\tilde O_n:=\\Bigl\\{b\\in \\mathcal{B}_+\\mid c_n(b)<1/2 \\quad\\text{and}\\\\ c_n(b)\\cdot\\Big(\\Vert\nb\\Vert+\\alpha_2\\cdot\\bigl\\Vert\\frac 1{\\Im b}\\bigr\\Vert\\Bigr)\\cdot \\bigl\\Vert\\frac 1{\\Im\nb}\\bigr\\Vert<1/2 \\Bigr\\}.\n\\end{multline*}\n\nThe relevance of the condition $c_n(b)<1/2$ is the following: Let us denote\n$$B_n(b):=b-\\eta(G_n(b)),$$\nthen inequality \\eqref{eq:ineq} takes, for $b\\in\\tilde O_n$, the form\n$$\\Vert 1-B_n(b)G_n(b)\\Vert \\leq c_n(b)<1/2.$$\nThis, however, implies that $B_n(b)G_n(b)$ is invertible with\n$$\\Vert G_n(b)^{-1}B_n(b)^{-1}\\Vert=\\Vert (B_n(b)G_n(b))^{-1}\\Vert\\leq 2,$$\nand thus\n\\begin{align*}\n\\Vert G_n(b)^{-1}\\Vert&=\\Vert G_n(b)^{-1}B_n(b)^{-1}B_n(b)\\Vert\\\\\n&\\leq 2\\Vert B_n(b)\\Vert\\\\\n&=2\\Vert b-\\eta(G_n(b))\\Vert\\\\\n&\\leq 2\\left(\\Vert b\\Vert +\\alpha_2 \\cdot\\Vert G_n(b)\\Vert\\right)\\\\\n&\\leq 2\\left(\\Vert b\\Vert +\\alpha_2\\cdot \\Vert \\frac 1{\\Im b}\\Vert\\right).\n\\end{align*}\n\nBut then the other condition in the definition of $\\tilde O_n$ implies that for\n$b\\in\\tilde O_n$ we have\n\\begin{align}\\label{eq:abneu}\n\\Vert r_3\\cdot G_n(b)^{-1}\\Vert&\\leq \\Vert r_3\\Vert \\cdot\\Vert G_n(b)^{-1}\\Vert\\\\&\\leq\nc_n \\cdot 2\\left(\\Vert b\\Vert +\\alpha_2 \\cdot\\Vert \\frac 1{\\Im b}\\Vert\\right)<\\Vert \\frac\n1{\\Im b}\\Vert^{-1}.\\notag\n\\end{align}\nSince\n$$\\Im b\\geq \\Vert \\frac 1{\\Im b}\\Vert^{-1}\\cdot 1,$$\nit follows that, for $b\\in \\tilde O_n$, $\\Lambda_n(b)=b-r_3\\cdot G_n(b)^{-1}$ is still in\n$\\mathcal{B}_+$ and so we can use the equation \\eqref{eq:Gfixed} with $\\Lambda_n(b)$ as argument,\ni.e.,\n\\begin{equation}\\label{eq:GLfixed}\n\\Lambda_n(b)=\\frac 1{G(\\Lambda_n(b))}+\\eta\\bigl(G(\\Lambda_n(b))\\bigr).\n\\end{equation}\nThe point of having both equation \\eqref{eq:GLfixed} and equation \\eqref{eq:Gnfixed} is\nthat this implies that\n$$G(\\Lambda_n(b))=G_n(b).$$\nIn \\cite{HT,HST} this was shown by analytic continuation arguments. We can simplify that\nargument by using the fact from \\cite{HRS} that the equation\n\\begin{equation} \\label{eq:G}\nw=\\frac 1G +\\eta(G)\n\\end{equation}\nhas, for any $w$ with $\\Im w>0$, exactly one solution $G\\in \\mathcal{B}$ such that $\\Im G$ is\nnegative. Since both $G_n(b)$ and $G(\\Lambda_n(b))$ have negative imaginary parts (as\nCauchy transforms at some arguments) and both satisfy the same equation \\eqref{eq:G} (for\n$w=\\Lambda_n(b)$), they must agree.\n\nThen we can, still in the case $b\\in \\tilde O_n$, estimate in the usual way, by invoking\nthe resolvent identity:\n\\begin{align*}\n\\Vert G_n(b)-G(b)\\Vert&=\\Vert G(\\Lambda_n(b))-G(b)\\Vert\\\\\n&=\\Vert G(\\Lambda_n(b))\\cdot (\\Lambda_n(b)-b)\\cdot G(b)\\Vert\\\\\n&\\leq \\Vert (\\Lambda_n(b)-b)\\Vert\\cdot \\Vert G_n(b)\\Vert\\cdot \\Vert G(b)\\Vert.\n\\end{align*}\nBoth $\\Vert G(b)\\Vert$ and $\\Vert G_n(b)\\Vert$ can be estimated by $\\Vert 1/\\Im b\\Vert$\nand for the first factor we have, by the second inequality in \\eqref{eq:abneu}, that\n\\begin{align*}\n\\Vert(\\Lambda_n(b)-b)\\Vert=\\Vert {r_3}{G_n(b)}^{-1}\\Vert\\leq c_n \\cdot 2\\left(\\Vert\nb\\Vert +\\alpha_2 \\cdot\\Vert \\frac 1{\\Im b}\\Vert\\right)\n\\end{align*}\nThus, for $b\\in \\tilde O_n$, we have shown that\n\\begin{equation}\n\\Vert G_n(b)-G(b)\\Vert\\leq c_n \\cdot 2\\left(\\Vert b\\Vert +\\alpha_2 \\cdot\\Vert \\frac 1{\\Im\nb}\\Vert\\right)\\cdot \\Vert \\frac 1{\\Im b}\\Vert^2\n\\end{equation}\nFor $b\\in \\mathcal{B}_+\\backslash \\tilde O_n$, on the other hand, we just use the trivial\nestimate\n$$\\Vert G_n(b)-G(b)\\Vert\\leq 2\\cdot \\Vert\\frac 1{\\Im b}\\bigr\\Vert$$\ntogether with\n\\begin{itemize}\n\\item\nif we have $c_n(b)\\geq 1/2$, then\n\\begin{align*}\n\\Vert\\frac 1{\\Im b}\\Vert &\\leq 2c_n\\cdot \\Vert\\frac 1{\\Im b}\\Vert\\\\\n&\\leq 2c_n\\cdot \\Vert\\frac 1{\\Im b}\\Vert\\cdot \\Vert b\\Vert \\cdot \\Vert\\frac 1{\\Im\nb}\\Vert\\\\\n &\\leq  2c_n\\cdot \\Vert\\frac 1{\\Im b}\\Vert^2 \\cdot \\left(\\Vert b\\Vert +\\alpha_2\n\\cdot\\Vert \\frac 1{\\Im b}\\Vert\\right)\n\\end{align*}\n\\item\nif we have $c_n(b)\\cdot\\left(\\Vert b\\Vert+\\alpha_2\\cdot\\bigl\\Vert\\frac 1{\\Im\nb}\\bigr\\Vert\\right)\\cdot \\bigl\\Vert\\frac 1{\\Im b}\\bigr\\Vert\\geq 1/2$, then we have again\n\\begin{align*}\n\\Vert\\frac 1{\\Im b}\\Vert\n &\\leq  2c_n \\cdot \\left(\\Vert b\\Vert +\\alpha_2\n\\cdot\\Vert \\frac 1{\\Im b}\\Vert\\right)\\cdot \\Vert\\frac 1{\\Im b}\\Vert^2\n\\end{align*}\n\\end{itemize}\n\nThus we have proved the Theorem.\n\n\n\n\n", 0.7859736406093969], ["\\section{Introduction}\n\nA Banach space $X$ is said to have the Daugavet property (DP) if every rank-one operator $T:X\\longrightarrow X$ satisfies the equation\n\\begin{equation}\\label{ecuadauga}\n\\Vert I+T\\Vert=1+\\Vert T\\Vert,\n\\end{equation}\nwhere $I$ denotes the identity operator. The previous equality is known as \\textit{Daugavet equation} after I. Daugavet who proved in \\cite{dau} that every compact operator on $\\mathcal C([0,1])$ satisfies (\\ref{ecuadauga}). Since then, several examples of Banach spaces enjoying the Daugavet property have appeared such as $\\mathcal C(K)$ for a compact Hausdorff and perfect topological space $K$, $L_1(\\mu)$ and $L_\\infty(\\mu)$ for a non-atomic measure $\\mu$, and the space of Lipschitz functions $\\operatorname{Lip(M)}$ over a metrically convex space $M$ (see \\cite{ikw,kssw,werner} and the references therein for details). Moreover, in \\cite[Lemma 2.1]{kssw} a characterisation of the Daugavet property in terms of the geometry of the slices of $B_X$ appeared (see below). This celebrated characterisation opened the door to understanding the many geometrical interpretations of the DP and has motivated a lot of research on the Daugavet equation ever since (see for instance \\cite{bm2, bspw, ksw, kw}).\n\nOnce the DP has been understood for the classical Banach spaces, it is natural to study its stability under different combinations of these spaces. In this direction, it is important to understand its stability under tensor products. Given the preeminent position of the injective ($\\epsilon$) and projective ($\\pi$) norms as the smallest and largest respectively tensor norms, it becomes apparent the need to understand the stability of the DP under these norms. Indeed, in his 2001 survey paper \\cite{werner}, D. Werner posed a list of open problems related to the Daugavet property. In particular,  \\cite[Section 6, Question (3)]{werner} says:\n\\begin{center}\n    \\emph{If $X$ and/or $Y$ have the Daugavet property, what about their tensor products $X\\ensuremath{\\widehat{\\otimes}_\\varepsilon} Y$ and $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$?}\n\\end{center}\n\nSoon afterwards, V. Kadets, N. Kalton and D. Werner provided an example of a two dimensional complex Banach space $Y$ without the DP such that both $L_1^\\mathbb C([0,1])\\ensuremath{\\widehat{\\otimes}_\\varepsilon} Y$ and  $L_\\infty^\\mathbb C([0,1])\\ensuremath{\\widehat{\\otimes}_\\pi} Y^*$ fail the Daugavet property \\cite[Theorem 4.2 and Corollary 4.3]{kkw}. Real counterexamples were given in \\cite[Remark 3.13]{llr2} (the examples given there actually fail a weaker requirement than the Daugavet property). Therefore, the ``or'' part of the previous question was answered in the negative. \n\nIn view of the preceding paragraph, the following question remains open:\n\n\\begin{question}\\label{questiondproyect}\nLet $X$ and $Y$ be Banach spaces with the Daugavet property. Do $X\\ensuremath{\\widehat{\\otimes}_\\varepsilon} Y$ and $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ have the Daugavet property?\n\\end{question}\n\nIt is well known that $L_1(\\mu)\\ensuremath{\\widehat{\\otimes}_\\pi} X=L_1(\\mu,X)$ has the Daugavet property whenever $L_1(\\mu)$ does. Concerning non-trivial positive results, we only know of two results. On the one hand, in \\cite{br} it is proved, making a strong use of the theory of centralizer and function module representation of Banach spaces, that the projective tensor product of a Banach space without minimal $L$-summands with another non-zero Banach space has the Daugavet property. On the other hand, the first author proved in \\cite{rueda} that $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ has the Daugavet property provided $X$ is a separable $L$-embedded Banach space with the Daugavet property and $Y$ is a non-zero Banach space with the metric approximation property. Anyway, a common denominator in both results is that, in order to get that $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ has the Daugavet property, only one of the spaces is required to enjoy the property. As a consequence of this fact, to the best of our knowledge, no positive result is known in the direction of Question \\ref{questiondproyect}. \n\nThe main results of this paper are two positive partial answers to  Question \\ref{questiondproyect}:\n\n\\begin{theorem}\\label{theo:L1itenL1}\nLet $(\\Omega_1,\\Sigma_1,\\mu_1)$ and $(\\Omega_2,\\Sigma_2,\\mu_2)$ be measure spaces with purely non-atomic measures. Then the space $L_1(\\Omega_1,\\Sigma_1,\\mu_1)\\ensuremath{\\widehat{\\otimes}_\\varepsilon} L_1(\\Omega_2,\\Sigma_2,\\mu_2)$ has the Daugavet property.\n\\end{theorem}\n\n\n\n\\begin{theorem}\\label{theo:maintheorem}\nLet $X$ and $Y$ be two $L_1$-preduals. If $X$ and $Y$ have the Daugavet property, then so does $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$. In particular, $C(K_1)\\ensuremath{\\widehat{\\otimes}_\\pi} C(K_2)$ has the Daugavet property if $K_1$ and $K_2$ are compact spaces without isolated points.\n\\end{theorem}\n\n\nThe proof of Theorem \\ref{theo:L1itenL1} is based on a discretization approach. It requires only the definition of the injective tensor norm and some fine measure theoretical arguments. Section \n\\ref{section:inyedp} is devoted to this proof and it is totally self contained. \n\n\\smallskip\n\nBy the duality of the injective and projective tensor norms, Theorems \\ref{theo:L1itenL1} and \\ref{theo:maintheorem} are somehow dual to each other. Therefore, it is not too surprising that it is possible to use similar measure theoretical arguments as in the proof of Theorem \\ref{theo:L1itenL1} to prove that $C(K_1)\\ensuremath{\\widehat{\\otimes}_\\pi} C(K_2)$ has the Daugavet property whenever $C(K_1)$ and $C(K_2)$ have it. However, a more abstract version of this proof allows us to extend the result to general $L_1$ preduals with the DP, and this is the proof we present in Section \\ref{section:ODP}, which is again quite self contained. \n\n\\smallskip\n\n\nAn inspection to this last proof together with the characterisation of the Daugavet property in $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ given in Proposition \\ref{propmotiexten} point out the convenience of identifying a certain property about extension of bounded operators from $X$ to $Y^*$. Motivated by this, in Section \\ref{section:ODP1} (see Definition \\ref{defi:odp}), we introduce  the \\textit{operator Daugavet property (ODP)}. Since ODP is a sufficient condition for Banach spaces $X$ and $Y$ in order to make $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ enjoy the DP (see Theorem \\ref{theo:odpwerneranswer}), the rest of Section \\ref{section:ODP1} is devoted to providing new examples of Banach spaces with the ODP. These include for instance, $L_1$-preduals with the Daugavet property, $L_1(\\mu)$ spaces with non-atomic measures, or $\\ell_\\infty$-sums of spaces with the ODP.\n\n\n\\smallskip\n\nIn Section \\ref{section:appodp}, we show how ODP can be applied to solve different isometric problems in the setting of tensor products. First, in Theorem \\ref{theo:symmocta}, it is proved that if a Banach space $X$ has ODP, then all the projective symmetric tensor products $\\widehat{\\otimes}_{\\pi,s,N} X$ have an octahedral norm (see definition below). In general, we are not able to get Daugavet property in such symmetric tensor product spaces because we lack a good description of norming sets for spaces of polynomials. However, making  use of the Dunford-Pettis property, we will prove in Proposition \\ref{propo:tensosime} that $\\widehat{\\otimes}_{\\pi,s,N} \\mathcal C(K)$ has the Daugavet property whenever $K$ is a compact Hausdorff topological space without any isolated point and $N\\in\\mathbb N$. Let us point out that, to the best of our knowledge, the first (non-trivial) examples of projective symmetric tensor product spaces with the Daugavet property or with an octahedral norm are those given in Section \\ref{section:appodp}. \n\nIn this same section, we also use the ODP in order to get some consequences about roughness in projective tensor products. Indeed, we prove in Proposition \\ref{propo2-ruda} that the norm of $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ is $2$-rough whenever $X$ has the ODP and $Y$ is non-zero. As an application, we derive consequences about stability of diameter two properties by injective tensor products of the form $L_1\\ensuremath{\\widehat{\\otimes}_\\varepsilon} X$. These are motivated by the question, posed in \\cite[Question (b)]{aln}, about how diameter two properties are preserved by tensor product spaces.\n\n\n\\subsection{Terminology} \nWe will consider only real Banach spaces. Given a Banach space $X$, we will denote the closed unit ball and the unit sphere of $X$ by $B_X$ and $S_X$ respectively. We will also denote by $X^*$ the topological dual of $X$. Given a bounded subset $C$ of $X$, $x^*\\in X^*$ and $\\alpha>0$, a \\textit{slice of $C$} is given by\n$$S(C,x^*,\\alpha):=\\{x\\in C:x^*(x)>\\sup x^*(C)-\\alpha\\}.$$\n\n A Banach space $X$ is said to have the \\textit{Daugavet property} if every rank-one operator $T:X\\longrightarrow X$ satisfies the equation\n$$\\Vert I+T\\Vert=1+\\Vert T\\Vert,$$\nwhere $I:X\\longrightarrow X$ denotes the identity operator. It is known \\cite{kssw} that a Banach space $X$ has the Daugavet property if, and only if, for every $\\varepsilon>0$, every point $x\\in S_X$ and every slice $S$ of $B_X$ there exists a point $y\\in S$ such that $\\Vert x+y\\Vert>2-\\varepsilon$. This characterisation will be freely used throughout the text without any explicit mention.\n\nBy an $L_1$-predual we will mean a Banach space $X$ such that $X^*=L_1(\\mu)$ for certain measure $\\mu$. We refer the reader to the seminal paper \\cite{linds} for background on these spaces and the connection with norm preserving extension of operators. Also, we refer to \\cite{bm} for background about $L_1$-preduals with the Daugavet property.\n\nGiven two Banach spaces $X$ and $Y$, we denote by $L(X,Y)$ the space of bounded linear operators $T:X\\longrightarrow Y$. Also, we denote by $B(X,Y)$ the space of bounded bilinear maps $G:X\\times Y\\rightarrow \\mathbb R$. Recall that the\n\\textit{projective tensor product} of $X$ and $Y$, denoted by\n$X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$, is the completion of the algebraic tensor product $X\\otimes Y$ under the norm given by\n\\begin{equation*}\n   \\Vert u \\Vert :=\n   \\inf\\left\\{\n      \\sum_{i=1}^n  \\Vert x_i\\Vert\\Vert y_i\\Vert\n      : u=\\sum_{i=1}^n x_i\\otimes y_i\n      \\right\\}.\n\\end{equation*}\nIt follows easily from the definition  that $B_{X\\ensuremath{\\widehat{\\otimes}_\\pi} Y}=\\overline{\\operatorname{co}}(B_X\\otimes B_Y)\n=\\overline{\\operatorname{co}}(S_X\\otimes S_Y)$.\nMoreover, given Banach spaces $X$ and $Y$, it is well known that\n$(X\\ensuremath{\\widehat{\\otimes}_\\pi} Y)^*=L(X,Y^*)=B(X,Y)$ \\cite{DeFl}.\n\nThe \\textit{injective tensor product} of $X$ and $Y$, denoted by $X \\ensuremath{\\widehat{\\otimes}_\\varepsilon} Y$, is the completion of $X\\otimes Y$ under the norm given by\n\\begin{equation*}\n   \\Vert u\\Vert:=\\sup\n   \\left\\{\n      \\sum_{i=1}^n \\vert x^*(x_i)y^*(y_i)\\vert\n      : x^*\\in S_{X^*}, y^*\\in S_{Y^*}\n   \\right\\},\n\\end{equation*}\nwhere $u:=\\sum_{i=1}^n x_i\\otimes y_i$. Note that, in the above formula, $S_{X^*}$ and $S_{Y^*}$ can be replaced with norming sets for $X$ and $Y$ respectively. We refer the reader to \\cite{DeFl,rya} for a detailed treatment of tensor product spaces.\n\n\n\n\n\\section{$L_1\\ensuremath{\\widehat{\\otimes}_\\varepsilon} L_1$ has the Daugavet property}\\label{section:inyedp}\n\nIn this section, we prove Theorem \\ref{theo:L1itenL1}. As mentioned in the introduction, the proof only requires the definition of the injective tensor product and measure theoretical reasonings. \n\n\n\\begin{proof}[Proof of Theorem \\ref{theo:L1itenL1}]\nFor brevity, let $X=L_1(\\Omega_1,\\Sigma_1,\\mu_1)\\ensuremath{\\widehat{\\otimes}_\\varepsilon} L_1(\\Omega_2,\\Sigma_2,\\mu_2)$.\n\nLet $\\alpha \\in X$ and $\\varphi \\in X^*$ with $\\|\\alpha\\|_X=1=\\|\\varphi\\|_{X^*}$, and let $\\varphi\\otimes\\alpha$ denote the rank-one operator given by $\\varphi\\otimes\\alpha(x)=\\varphi(x)\\alpha$ for $x\\in X$. We will show that \n$$\n\\|I+\\varphi\\otimes\\alpha\\|=1+\\|\\varphi\\otimes\\alpha\\|=2.\n$$ \nTo this end fix $\\varepsilon>0$. Notice that, up a perturbation argument, there is no loss of generality in assuming that $\\varphi$ is a norm-attaining functional. Let $\\beta \\in X$ with $\\|\\beta\\|=1$ such that \n$$\n\\varphi(\\beta)=1.\n$$\nSince simple functions are dense in any $L_1$ space, up to perturbation, we can assume without loss of generality that there are two collections of pairwise disjoint sets of finite measure $(A_i)_{i=1}^n\\subset \\Sigma_1$ and $(B_j)_{j=1}^n\\subset \\Sigma_2$, and scalars $(a_{ij})_{i,j=1}^n$, $(b_{ij})_{i,j=1}^n$ such that \n$$\n\\alpha =\\sum_{i,j=1}^n a_{ij}\\chi_{A_i}\\otimes \\chi_{B_j},\n$$\nand\n$$\n\\beta =\\sum_{i,j=1}^n b_{ij}\\chi_{A_i}\\otimes \\chi_{B_j}.\n$$\nAlso note that it follows immediately from the definition of the injective norm that the set \n$$\nN=\\{h_1\\otimes h_2: \\, h_i\\in \\operatorname{ext}(B_{L_\\infty(\\Omega_i,\\Sigma_i,\\mu_i)}), \\,\\text{ for }i=1,2\\}\n$$ is a norming subset of $X^*$; here $\\operatorname{ext}(B_{L_\\infty(\\Omega_i,\\Sigma_i,\\mu_i)})$ denotes the set of extreme points of $B_{L_\\infty(\\Omega_i,\\Sigma_i,\\mu_i)}$, or in other words, $|h_i(x)|=1$ for $\\mu_i$-almost every $x\\in \\Omega_i$.\n\nWe will need the following: \n\n\\begin{lemma}\\label{lemaLinfty}\nFor every $\\delta>0$ there exist $(A'_i)_{i=1}^n\\subset \\Sigma_1$, $(B'_j)_{j=1}^n\\subset\\Sigma_2$ such that\n\\begin{enumerate}\n\\item $A'_i\\subset A_i$, $\\mu_1(A'_i)<\\delta$,  for $1\\leq i\\leq n$.\n\\item $B'_j\\subset B_j$, $\\mu_2(B'_j)<\\delta$, for $1\\leq j\\leq n$.\n\\item If we denote $$\\beta'=\\sum_{i,j=1}^n b_{ij} \\frac{\\mu_1(A_i)\\mu_2(B_j)}{\\mu_1(A'_i)\\mu_2(B'_j)}\\chi_{A'_i}\\otimes\\chi_{B'_j},$$ then we have $$\\|\\beta'\\|_X=1\\quad\\textrm{ and }\\quad\\varphi(\\beta')>1-\\varepsilon.$$\n\\end{enumerate}\n\\end{lemma}\n\n\\begin{proof}\nFix $1\\leq i\\leq n$. Let $r_i=\\sum_{j=1}^n b_{ij}\\varphi(\\chi_{A_i}\\otimes \\chi_{B_j})$. Note that $\\sum_{i=1}^n r_i=\\varphi(\\beta)=1.$ For $f\\in L_1(\\Omega_1,\\Sigma_1,\\mu_1)$, let \n$$\n\\varphi_i(f)= \\sum_{j=1}^n b_{ij}\\mu_1(A_i)\\varphi(f\\chi_{A_i}\\otimes \\chi_{B_j}).\n$$\nClearly, $\\varphi_i$ is linear and \n$$\n|\\varphi_i(f)|\\leq \\|\\varphi\\|_{X^*} \\sum_{j=1}^n |b_{ij}|\\mu_1(A_i)\\mu_2(B_j)\\|f\\|_{L_1}\\leq C\\|f\\|_{L_1},\n$$\nfor some finite $C$. Moreover, $\\varphi_i(f)=0$ whenever $f\\chi_{A_i}=0$. Hence, by Radon-Nikodym Theorem, there is $g_i\\in L_\\infty(A_i,\\Sigma_1\\cap A_i,\\mu_1|_{A_i})$ such that\n$$\n\\varphi_i(f)=\\int_{A_i} g_i f d\\mu_1.\n$$\nSince \n$$\n\\frac{1}{\\mu_1(A_i)}\\int_{A_i} g_i d\\mu_1=\\varphi_i\\Big(\\frac{\\chi_{A_i}}{\\mu_1(A_i)}\\Big)=r_i,\n$$\nit follows that $g_i>r_i-\\varepsilon/2n$ on a subset of $A_i$ with positive measure. Let $A'_i$ be such a set satisfying the additional requirement that $\\mu_1(A'_i)<\\delta$.\nWe have that\n$$\n\\varphi_i\\Big(\\frac{\\chi_{A'_i}}{\\mu_1(A'_i)}\\Big)=\\frac{1}{\\mu_1(A'_i)}\\int_{A'_i} g_id\\mu_1>r_i-\\frac{\\varepsilon}{2n}.\n$$\n\nLet now \n$$\n\\beta'=\\sum_{i,j=1}^n b_{ij} \\frac{\\mu_1(A_i)}{\\mu_1(A'_i)}\\chi_{A'_i}\\otimes\\chi_{B_j}.\n$$\nIt follows that\n$$\n\\varphi(\\beta')=\\sum_{i,j=1}^n b_{ij} \\frac{\\mu_1(A_i)}{\\mu_1(A'_i)}\\varphi(\\chi_{A'_i}\\otimes\\chi_{B_j})=\\sum_{i=1}^n \\varphi_i\\Big(\\frac{\\chi_{A'_i}}{\\mu_1(A'_i)}\\Big)>\\sum_{i=1}^n r_i-\\frac{\\varepsilon}{2n}= 1-\\frac{\\varepsilon}{2}.\n$$\nMoreover, we have \n\\begin{align*}\n\\|\\beta'\\|_X&=\\sup_{h_1\\otimes h_2\\in N} \\langle h_1\\otimes h_2,\\beta'\\rangle\\\\\n&=\\sup_{h_1\\otimes h_2\\in N}  \\sum_{i,j=1}^n b_{ij} \\frac{\\mu_1(A_i)}{\\mu_1(A'_i)}\\int_{A'_i}h_1d\\mu_1\\int_{B_j}h_2d\\mu_2\\\\\n&=\\sup_{\\varepsilon_i,\\sigma_j\\in\\{-1,+1\\}}  \\sum_{i,j=1}^n b_{ij}\\varepsilon_i\\sigma_j \\mu_1(A_i)\\mu_2(B_j)\\\\\n&=\\sup_{h_1\\otimes h_2\\in N}  \\sum_{i,j=1}^n b_{ij} \\int_{A_i}h_1d\\mu_1\\int_{B_j}h_2d\\mu_2\\\\\n&=\\|\\beta\\|_X.\n\\end{align*}\n\nFinally, if we make the same argument starting with $\\beta'$ and interchanging the role of $i$ and $j$, then the result follows.\n\\end{proof}\n\n\nNow, let $f\\otimes g\\in N$ be such that\n\\begin{equation}\\label{ec:th41normavie}\n\\langle f\\otimes g,\\alpha\\rangle>1-\\varepsilon.\n\\end{equation}\nTake \n\\begin{equation}\\label{ec:th41delta}\n0<\\delta<\\frac{\\varepsilon}{2\\left(\\underset{1\\leq i\\leq n}{\\max}\\mu_1(A_i)+\\underset{1\\leq j\\leq n}{\\max}\\mu_2(B_j)\\right)\\sum_{i,j=1}^n|a_{ij}|},\n\\end{equation}\nand let $(A'_i)_{i=1}^n\\subset \\Sigma_1$, $(B'_j)_{j=1}^n\\subset\\Sigma_2$ and $\\beta'$ as given in Lemma \\ref{lemaLinfty}. Let also $f'\\otimes g'\\in N$ be such that \n\\begin{equation}\\label{ec:teo41normanuevo}\n\\langle f'\\otimes g',\\beta'\\rangle>1-\\varepsilon.\n\\end{equation}\n\nNow, let us define\n$$\n\\tilde f(x)=\n\\left\\{\n\\begin{array}{cc}\nf'(x)  & \\text{for }x\\in \\bigcup_{i=1}^n A'_i   \\\\\n&\\\\\nf(x)  & \\text{elsewhere,}     \n\\end{array}\n\\right.\n$$\n$$\n\\tilde g(y)=\n\\left\\{\n\\begin{array}{cc}\ng'(y)  & \\text{for }y\\in \\bigcup_{j=1}^n B'_j   \\\\\n&\\\\\ng(y)  & \\text{elsewhere.}     \n\\end{array}\n\\right.\n$$\n\nFirst, note that by our choice of $\\delta$ we have\n\\begin{align*}\n\\Big|\\sum_{i,j=1}^n a_{ij}&\\Big(\\langle (f'-f)\\otimes g,\\chi_{A'_i}\\otimes\\chi_{B_j}\\rangle+\\langle f\\otimes (g'-g),\\chi_{A_i}\\otimes \\chi_{B'_j}\\rangle\\Big)\\Big|\\leq\\\\\n&\\leq \\sum_{i,j=1}^n |a_{ij}|\\Big(\\int_{A'_i}|f'|+|f|d\\mu_1\\int_{B_j}|g|d\\mu_2+\\int_{A_i}|f|d\\mu_1\\int_{B'_j}|g'|+|g|d\\mu_2\\Big)\\leq\\\\\n&\\leq \\sum_{i,j=1}^n |a_{ij}|2(\\mu_1(A'_i)\\mu_2(B_j)+\\mu_1(A_i)\\mu_2(B'_j))\\mathop{\\leq}\\limits^{\\mbox{\\scriptsize(\\ref{ec:th41delta})}} \\varepsilon.\n\\end{align*}\n\nFrom the above estimate and taking into account that $\\varphi(\\beta')>1-\\varepsilon$, it follows that\n\\begin{align*}\n\\|I+\\varphi\\otimes \\alpha\\| &\\geq \\|\\beta'+\\varphi(\\beta')\\alpha\\|_X\\\\\n&\\geq \\langle\\tilde f\\otimes \\tilde g, \\beta'+\\varphi(\\beta')\\alpha\\rangle\\\\\n&= \\sum_{i,j=1}^n \\Big(b_{ij} \\frac{\\mu_1(A_i)\\mu_2(B_j)}{\\mu_1(A'_i)\\mu_2(B'_j)}\\langle\\tilde f\\otimes \\tilde g,\\chi_{A'_i}\\otimes\\chi_{B'_j}\\rangle+\\varphi(\\beta') a_{ij}\\langle\\tilde f\\otimes \\tilde g,\\chi_{A_i}\\otimes \\chi_{B_j}\\rangle\\Big)\\\\\n&= \\langle f'\\otimes g',\\beta'\\rangle+ \\varphi(\\beta')(\\sum_{i,j=1}^n a_{ij} \\big( \\langle f\\otimes  g,\\chi_{A_i}\\otimes\\chi_{B_j}\\rangle\\\\\n&+\\langle (f'-f)\\otimes g,\\chi_{A'_i}\\otimes\\chi_{B_j}\\rangle+\\langle f\\otimes (g'-g),\\chi_{A_i}\\otimes \\chi_{B'_j}\\rangle\\big)\\\\\n&\\mathop{>}\\limits^{\\mbox{\\scriptsize (\\ref{ec:teo41normanuevo})}} 1-\\varepsilon +\\varphi(\\beta')\\big(\\langle f\\otimes g,\\alpha\\rangle-\\varepsilon\\big)\\\\\n&\\mathop{>}\\limits^{\\mbox{\\scriptsize(\\ref{ec:th41normavie})}}1-\\varepsilon+(1-\\varepsilon)(1-2\\varepsilon).\n\\end{align*}\n\nSince $\\varepsilon>0$ was arbitrary, we get that $\\|I+\\varphi\\otimes \\alpha\\| \\geq2$ as claimed.\n\\end{proof}\n\n\n\n\n\\begin{remark}\nNotice that the same idea works for the injective tensor product of any finite number of $L_1$ spaces with the Daugavet property.\n\\end{remark}\n\n\n\n\n\\section[$C(K)\\ensuremath{\\widehat{\\otimes}_\\pi} C(K)$ has the DP]{The projective tensor product of $L_1$ preduals has the Daugavet Property}\\label{section:ODP}\n\nIn this section we prove Theorem \\ref{theo:maintheorem}. We start with the following Proposition, which is inspired by the characterisation of octahedrality of the norm appeared in \\cite[Lemma 3.21]{lan}.\n\n\\begin{proposition}\\label{propmotiexten}\nLet $X$ and $Y$ be Banach spaces. The following assertions are equivalent:\n\\begin{enumerate}\n    \\item \\label{propomotiexten1} $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ has the Daugavet property.\n    \\item \\label{propomotiexten2} Given a finite-dimensional subspace $E$ of $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$, a slice $S$ of $B_{X\\ensuremath{\\widehat{\\otimes}_\\pi} Y}$, an operator $T\\in L(X,Y^*)$ and $\\varepsilon>0$ we can find $x\\otimes y\\in S$ and $G\\in L(X,Y^*)$ such that $T=G$ on $E$, $G(x)(y)=G(x\\otimes y)=\\Vert T\\Vert$ and $\\Vert G\\Vert\\leq (1-\\varepsilon)^{-1}\\Vert T\\Vert$.\n\\end{enumerate}\n\\end{proposition}\n\n\\begin{proof}\n(1)$\\Rightarrow$(2). Let $E$ be a finite-dimensional subspace of $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$, $S$ a slice of $B_{X\\ensuremath{\\widehat{\\otimes}_\\pi} Y}$, $T\\in L(X,Y^*)$ and $\\varepsilon>0$. Since $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ has the Daugavet property, then \\cite[Lemma 2.8]{kssw} yields the existence of another slice $R\\subseteq S$ of $B_{X\\ensuremath{\\widehat{\\otimes}_\\pi} Y}$ such that\n\\begin{equation}\\label{prop:l1orto}\n\\Vert e+\\lambda z\\Vert>(1-\\varepsilon)(\\Vert e\\Vert+\\vert \\lambda\\vert)\\end{equation}\nholds for every $e\\in E$, every $z\\in R$ and every $\\lambda\\in\\mathbb R$. Since $R$ is a slice of $B_{X\\ensuremath{\\widehat{\\otimes}_\\pi} Y}$ and $B_{X\\ensuremath{\\widehat{\\otimes}_\\pi} Y}=\\overline{\\operatorname{co}}(S_X\\otimes S_Y)$ then we can find $x\\otimes y\\in R\\subseteq S$. Now, define the following functional\n$$\\begin{array}{ccc}\n\\varphi: E\\oplus\\mathbb R (x\\otimes y)\\subseteq X\\ensuremath{\\widehat{\\otimes}_\\pi} Y & \\longrightarrow & \\mathbb R\\\\\ne+\\lambda x\\otimes y & \\longmapsto & T(e)+\\Vert T\\Vert \\lambda.\n\\end{array}$$\nWe claim that $\\Vert \\varphi\\Vert\\leq \\frac{\\Vert T\\Vert}{1-\\varepsilon}$. In fact, given $e\\in E$ and $\\lambda\\in\\mathbb R$, we have\n\\[\n\\begin{split}\n\\varphi(e+\\lambda x\\otimes y)=T(e)+\\lambda \\Vert T\\Vert\\leq \\Vert T\\Vert (\\Vert e\\Vert+\\vert\\lambda\\vert)\\mathop{\n\\leq}\\limits^{\\mbox{(\\ref{prop:l1orto})}} \\frac{\\Vert T\\Vert}{1-\\varepsilon}\\Vert e+\\lambda x\\otimes y\\Vert.\n\\end{split}\n\\]\nSo $\\varphi\\in (E\\oplus \\mathbb R(x\\otimes y))^*$. By Hahn-Banach theorem we can extend $\\varphi$ to an element $G\\in (X\\ensuremath{\\widehat{\\otimes}_\\pi} Y)^*=L(X,Y^*)$, which satisfies the desired requirements.\n\n(2)$\\Rightarrow$(1). Pick $z\\in S_{X\\ensuremath{\\widehat{\\otimes}_\\pi} Y}$, a slice $S$ of $B_{X\\ensuremath{\\widehat{\\otimes}_\\pi} Y}$ and $\\varepsilon>0$. Let us find $x\\otimes y\\in S$ such that\n$$\\Vert z+x\\otimes y\\Vert>2(1-\\varepsilon).$$\nTo this aim, pick $E:=\\operatorname{span}\\{z\\}$ and an operator $T\\in S_{L(X,Y^*)}$ such that $T(z)=1$. By assumption, we can find an element $x\\otimes y\\in S$ and an operator $G\\in L(X,Y^*)$ with $\\Vert G\\Vert\\leq (1-\\varepsilon)^{-1}$, $G(z)=T(z)=1$ and $G(x\\otimes y)=1$. Now\n$$\n\\Vert z+x\\otimes y\\Vert\\geq \\frac{G(z+x\\otimes y)}{\\|G\\|}\\geq 2(1-\\varepsilon).\n$$\nThe arbitrariness of $z$, $S$ and $\\varepsilon$ implies that $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ has the Daugavet property, as desired.\n\\end{proof}\n\nIn view of the previous proposition it is clear that the Daugavet property on $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ is strongly related to the possibility of extending operators on $L(X,Y^*)$. Using this  and the celebrated work of J. Lindenstrauss \\cite{linds}, we prove the second of our main results.\n\n\n\\begin{proof}[Proof of Theorem \\ref{theo:maintheorem}]\nIn order to prove that $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ has the Daugavet property pick an element $z\\in S_{X\\ensuremath{\\widehat{\\otimes}_\\pi} Y}$, a slice $S=S(B_{X\\ensuremath{\\widehat{\\otimes}_\\pi} Y},B,\\alpha)$ and $\\varepsilon>0$, and let us find an element $x\\otimes y\\in S$ such that $\\Vert z+x\\otimes y\\Vert>2-\\varepsilon$.\n\nChoose $\\eta>0$ small enough so that $\\left(2-3\\eta-\\eta\\left( \\frac{1+\\eta}{1-\\eta}\\right)^2\\right)\\left(\\frac{1-\\eta}{1+\\eta} \\right)^2 >2-\\varepsilon$. Pick a norm-one bilinear form $G$ such that $G(z)>1-\\eta$ and $x_0\\in S_X,y_0\\in S_Y$ such that $G(x_0,y_0)>1-\\eta$.\n\nChoose $x'\\in S_X$ and $y'\\in S_Y$ such that $B(x',y')>1-\\alpha$. From the definition of projective norm consider $n\\in\\mathbb N, x_1,\\ldots, x_{ n}\\in X$ and $y_1,\\ldots, y_{ n}\\in Y$ such that\n\\begin{equation*\n\\left\\Vert z-\\sum_{i=1}^{ n} x_i\\otimes y_i\\right\\Vert<\\eta.\n\\end{equation*}\nDefine $E:=\\operatorname{span}\n\\{x_1,\\ldots, x_{ n}\\}\\subseteq X$. Since $X$ has the Daugavet property we can find from \\cite[Lemma 2.8]{kssw} an element $x\\in S(B_X,B(\\cdot, y'),\\alpha)$ (note that the previous set defines a slice of $B_X$ since $B$ is bilinear and continuous) such that\n$$\\Vert e+\\lambda x\\Vert>(1-\\eta)(\\Vert e\\Vert+\\vert\\lambda\\vert)$$\nholds for every $e\\in E$ and every $\\lambda\\in\\mathbb R$.\n\nSimilarly define $F:=\\operatorname{span}\\{y_1,\\ldots, y_{ n}\\}\\subseteq X$. Since $Y$ has the Daugavet property we can find from \\cite[Lemma 2.8]{kssw} an element $y\\in S(B_Y,B(x,\\cdot),\\alpha)$ (note that $B(x)\\in Y^*$, so the previous set defines a slice of $B_Y$) such that\n$$\\Vert f+\\lambda y\\Vert>(1-\\eta)(\\Vert f\\Vert+\\vert\\lambda\\vert)$$\nholds for every $f\\in F$ and every $\\lambda\\in\\mathbb R$.\nNotice that $B(x)(y)>1-\\alpha$ which means that $x\\otimes y\\in S$.\n\nDefine $\\psi:E\\oplus\\mathbb R x\\longrightarrow X$ by the equation\n$$\\psi(e+\\lambda x):=e+\\lambda x_0.$$\nWe claim that $\\Vert \\psi\\Vert\\leq \\frac{1}{1-\\eta}$. Indeed, given $e\\in E$ and $\\lambda\\in\\mathbb R$, we have\n$$\\Vert \\psi(e+\\lambda x)\\Vert=\\Vert e+\\lambda x_0\\Vert\\leq \\Vert e\\Vert+\\vert\\lambda\\vert\\leq \\frac{1}{1-\\eta}\\Vert e+\\lambda x\\Vert.$$\nNow, since $X$ is an $L_1$ predual and $\\psi$ is a compact operator (it is actually a finite-rank operator), we can find by \\cite[Theorem 6.1, (3)]{linds} an extension $\\varphi:X\\longrightarrow X$ such that $\\Vert \\varphi\\Vert\\leq \\frac{1+\\eta}{1-\\eta}$.\n\nSimilarly we can construct a bounded operator $\\phi:Y\\longrightarrow Y$ such that $\\phi(f)=f$ for every $f\\in F$, $\\phi(y)=y_0$ and $\\Vert \\phi\\Vert\\leq \\frac{1+\\eta}{1-\\eta}$. Now define the bilinear form $T(u,v):=G(\\varphi(u),\\phi(v))$ for every $u\\in X, v\\in Y$. We claim that $\\Vert T\\Vert\\leq \\left( \\frac{1+\\eta}{1-\\eta}\\right)^2$. Indeed, given $u\\in B_X$ and $y\\in B_Y$ we have\n$$\\vert G(\\varphi(u),\\phi(v))\\vert\\leq \\Vert G\\Vert\\Vert \\varphi(u)\\Vert \\Vert \\phi(v)\\Vert\\leq \\left( \\frac{1+\\eta}{1-\\eta}\\right)^2.$$\nFurthermore,\n\\[\\begin{split}\nT(z) \\geq \\sum_{i=1}^{ n} G(\\varphi(x_i),\\phi(y_i))-\\eta\\Vert T\\Vert & >\\sum_{i=1}^{ n} G(x_i)(y_i)-\\eta\\left( \\frac{1+\\eta}{1-\\eta}\\right)^2\\\\\n& >1-2\\eta-\\eta\\left( \\frac{1+\\eta}{1-\\eta}\\right)^2.\n\\end{split}\n\\]\nAlso\n$$T(x,y)=G(x_0,y_0)>1-\\eta.$$\nFinally\n\\[\n\\begin{split}\n\\Vert z+x\\otimes y\\Vert&  \\geq \\frac{T\\left( x\\otimes y+z\\right)}{\\Vert T\\Vert}\\\\\n& \\geq \\frac{1-\\eta+T(z)}{\\Vert T\\Vert}\\\\\n& \\geq \\frac{2-3\\eta-\\eta\\left( \\frac{1+\\eta}{1-\\eta}\\right)^2}{\\Vert T\\Vert}\\\\\n& \\geq \\left(2-3\\eta-\\eta\\left( \\frac{1+\\eta}{1-\\eta}\\right)^2\\right)\\left(\\frac{1-\\eta}{1+\\eta} \\right)^2\\\\\n& >2-\\varepsilon.\n\\end{split}\n\\]\nSince $\\varepsilon>0$ was arbitrary then $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ has the Daugavet property, so we are done.\n\\end{proof}\n\n\\begin{remark}\\label{remark:longtensorpredul1}\nNotice that the same idea works for the projective tensor product of any finite number of $L_1$ preduals with the Daugavet property.\n\\end{remark}\n\n\\section{The operator Daugavet Property}\\label{section:ODP1}\n\n\n\n\n\nIn view of the proof of Theorem \\ref{theo:maintheorem}, we will define and study in this section an operator version of the Daugavet property in the spirit of Proposition \\ref{propmotiexten}. This property will allow us to obtain, in the next section, further results about the geometry of tensor products.\n\n\\begin{definition}\\label{defi:odp}\nLet $X$ be a Banach space. We will say that $X$ has the \\textit{operator Daugavet property (ODP)} if, for every $x_1\\ldots x_n\\in S_X$, every slice $S$ of $B_X$ and every $\\varepsilon>0$ there exists an element $x\\in S$ such that, for every $x'\\in B_X$, there exists an operator $T:X\\longrightarrow X$ with $\\Vert T\\Vert\\leq 1+\\varepsilon$, $T(x)=x'$ and $\\Vert T(x_i)-x_i\\Vert<\\varepsilon$ for every $i\\in\\{1,\\ldots, n\\}$.\n\\end{definition}\n\n\\begin{remark}\\label{remaodpimplidp}\nNotice that the ODP implies the Daugavet property. Indeed, given a Banach space $X$ with the ODP, fix $x\\in S_X$, $\\varepsilon>0$ and a slice $S$ of $B_X$. By hypothesis, there is an operator $T:X\\longrightarrow X$ and a point $y\\in S$ such that $\\Vert T(x)-x\\Vert<\\varepsilon$, $\\Vert T\\Vert\\leq 1+\\varepsilon$ and $T(y)=x$. The existence of such operator implies that $\\Vert x+y\\Vert>\\frac{2-\\varepsilon}{1+\\varepsilon}$. Nevertheless, we do not know of any example satisfying the Daugavet property but not the ODP.\\end{remark}\n\n\nFrom the proof of Theorem \\ref{theo:maintheorem} the following result should be clear.\n\n\\begin{theorem}\\label{theo:odpwerneranswer}\nLet $X$ and $Y$ be two Banach spaces. If $X$ and $Y$ have the ODP then $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ has the Daugavet property.\n\\end{theorem}\n\nWe will devote the remainder of this section to give examples of Banach spaces with the ODP in order to enlarge the class of spaces where Theorem \\ref{theo:odpwerneranswer} applies.\n\nIt is clear from the proof of Theorem \\ref{theo:maintheorem} that $L_1$ predual spaces with the Daugavet property actually have the ODP. Let us see another classical space with the Daugavet property which actually enjoys the ODP.\n\n\\begin{proposition}\\label{prop:L1odp}\nLet $(\\Omega,\\Sigma,\\mu)$ be a non-atomic measure space. Then $L_1(\\mu)$ has the ODP.\n\\end{proposition}\n\n\\begin{proof}\nLet us write $X=L_1(\\mu)$ for short. Consider $x_1,\\ldots, x_n\\in B_X, \\varepsilon>0$ and a slice $S=S(B_X,\\varphi,\\alpha)$ of $B_X$. We can assume with no loss of generality that $\\varepsilon<\\alpha$. To begin with, we can assume $\\varphi\\in X^*$ has norm one, and pick some $f_0\\in S$. Now, since $f_0\\in L_1(\\mu)$ has $\\sigma$-finite support, we can find $g\\in L_\\infty(\\mu)$ such that \n$$\n\\varphi(f)= \\int_\\Omega fg d\\mu,\n$$\nfor every $f\\in L_1(\\mu)$ whose support is included in that of $f_0$.\nIn particular, this means that there exists $A\\in \\Sigma$ contained in the support of $f_0$, with $0<\\mu(A)<\\infty$, $\\vert g(t)\\vert>1-\\varepsilon$ for every $t\\in A$ and $\\operatorname{sign}(g_{|A})$ constant.\n\nSince $\\mu$ does not contain any atom, we can assume with no loss of generality that there exists a subset $B\\subseteq A$ which $\\mu(B)>0$ and such that \n$$\n\\|x_i\\chi_B\\|<\\frac{\\varepsilon}{4},\n$$\nfor every $i\\in\\{1,\\ldots, n\\}$.\n\nNow, for every $i\\in\\{1,\\ldots, n\\}$, we can find a simple functions $x_i'\\in S_X$ such that\n\\begin{equation}\\label{L1aproxsimple}\n\\Vert x_i\\chi_{\\Omega\\backslash B}-x_i'\\Vert<\\frac{\\varepsilon}{4},\n\\end{equation}\nwhere $x_i'=\\sum_{j=1}^{m} a_{ij}\\chi_{A_{j}}$ for suitable $m\\in\\mathbb N$, $a_{ij}\\in\\mathbb R$ and pairwise disjoint $A_{j}\\in \\Sigma$ with $B\\cap A_j=\\emptyset$ for $j\\in\\{1,\\ldots, m\\}$. \n\nLet also $f_B:=\\frac{1}{\\mu(B)}\\operatorname{sign}(g_{|B})\\chi_B$. Note that\n$$\\varphi(f_B)=\\frac{1}{\\mu(B)}\\int_\\Omega g(t)\\operatorname{sign}(g_{|B})\\chi_B\\ d\\mu=\\frac{1}{\\mu(B)}\\int_B \\vert g(t)\\vert\\ d\\mu\\geq 1-\\varepsilon>1-\\alpha,$$\nfrom where $f_B\\in S$.\n\nNow, in order to prove that $X$ has the ODP, pick an element $x'\\in B_X$ and let us construct an operator $T:X\\longrightarrow X$ satisfying the desired requirements. To this end, consider the operator $T:X\\longrightarrow X$ given by the equation\n$$\nT(f):=\\sum_{j=1}^m \\frac{1}{\\mu(A_j)}\\int_{A_j} f\\ d\\mu\\ \\chi_{A_j}+\\int_B f\\ d\\mu\\  \\operatorname{sign}(g_{|B}) x'.\n$$\nIt is clear from the disjointness of the sets $B, A_1,\\ldots, A_m$ and the fact that $\\Vert x'\\Vert\\leq 1$ that $\\Vert T(f)\\Vert\\leq \\Vert f\\Vert$ holds for every $f\\in X$. Furthermore, it is clear from the definition that $T(x_i')=x_i'$ , so\n\\[\n\\begin{split}\n\\Vert T(x_i)-x_i\\Vert& \\leq  \\Vert T(x_i-x_i')\\Vert+\\Vert x_i'-x_i\\Vert\\\\\n& \\leq \\Vert T\\Vert \\Vert x_i-x_i'\\Vert+\\Vert x_i-x_i'\\Vert\\\\\n& \\mathop{\\leq}\\limits^{\\mbox{\n(\\ref{L1aproxsimple})}}\\varepsilon.\\end{split}\n\\]\nOn the other hand,\n$$T(f_B)=\\frac{1}{\\mu(B)}\\operatorname{sign}(g_{|B})^2\\int_B \\chi_B\\ d\\mu x'=x',$$\n and the proof is finished.\n\\end{proof}\n\n\\begin{remark}\\label{remark:L1}\n\\begin{enumerate}\n    \\item Notice that an adaptation of the proof of Proposition \\ref{prop:L1odp} yields that $L_1(\\mu,X)$ has the ODP whenever $\\mu$ does not contain any atom regardless of the Banach space $X$.\n    \\item The fact that $L_1$-preduals with the Daugavet property have the ODP can be considered in part as a consequence of the fact that these are injective Banach spaces. The situation with $L_1(\\mu)$ spaces can also be regarded as somehow similar, since these spaces are in turn injective as Banach lattices (cf. \\cite{lotz, mn}).\n\\end{enumerate}\n\n\\end{remark}\n\n\nIt is known that if $X$ and $Y$ have the Daugavet property then so does $X\\oplus_\\infty Y$ \\cite[Theorem 1]{woj}. The following proposition proves that the same holds for the ODP.\n\n\\begin{proposition}\nLet $X$ and $Y$ be two Banach spaces with ODP. Then $X\\oplus_\\infty Y$ has ODP.\n\\end{proposition}\n\n\\begin{proof}\nLet $(x_1,y_1),\\ldots, (x_n,y_n)\\in S_{X\\oplus_\\infty Y}$, $\\varepsilon>0$, and a slice\n$$\nR=S(B_{X\\oplus_\\infty Y},(x^*,y^*),\\alpha).\n$$\nLet us find $(x,y)\\in R$ such that, for every $(x',y')\\in B_{X\\oplus_\\infty Y}$, there exists an operator $T:X\\oplus_\\infty Y\\longrightarrow X\\oplus_\\infty Y$ satisfying the desired requirements. \n\nSince $X$ has the ODP, there exist an element $x\\in \\{z\\in B_X: x^*(z)>\\Vert x^*\\Vert-\\frac{\\alpha}{2}\\}$ (which is a slice of $B_X$) such that, for every $x'\\in B_X$, there exists an operator $G:X\\longrightarrow X$ with $\\Vert G\\Vert\\leq 1+\\varepsilon$, $\\Vert G(x_i)-x_i\\Vert\\leq \\varepsilon$ and $G(x)=x'$.\n\nRepeating the same argument on the factor $Y$ find $y\\in \\{z\\in B_Y: y^*(z)>\\Vert y^*\\Vert-\\frac{\\alpha}{2}\\}$ (which is a slice of $B_Y$) such that, for every $y'\\in B_Y$, there exists an operator $S:Y\\longrightarrow Y$ with $\\Vert S\\Vert\\leq 1+\\varepsilon$, $\\Vert S(y_i)-y_i\\Vert\\leq \\varepsilon$ and $S(y)=y'$. \n\nConsider $(x,y)\\in B_{X\\oplus_\\infty Y}$. Then\n$$(x^*,y^*)( x, y)= x^*(x)+ y^*(y)>\\Vert x^*\\Vert+\\Vert y^*\\Vert-\\alpha=1-\\alpha,$$\nwhich means that $(x,y)\\in R$. In order to finish the proof let us show that $(x,y)$ satisfies the desired requirements. To this end, pick $(x',y')\\in B_{X\\oplus_\\infty Y}$. Note that $\\Vert(x',y')\\Vert=\\max\\{\\Vert x'\\Vert,\\Vert y'\\Vert\\} = 1$, which means that both $\\Vert x'\\Vert$ and $\\Vert y'\\Vert$ are less than or equal to $1$. Consider $G:X\\longrightarrow X$ and $S:Y\\longrightarrow Y$ with the properties described above associated to $x'$ and $y'$ respectively, and define $T:X\\oplus_\\infty Y \\longrightarrow X\\oplus_\\infty Y$ given by $T(u,v)=(G(u),S(v))$ for every $u\\in X, v\\in Y$. First of all, given $u\\in X, v\\in Y$, we get\n$$\\Vert T(u,v)\\Vert=\\max\\{\\Vert G(u)\\Vert,\\Vert S(v)\\Vert\\}\\leq (1+\\varepsilon)\\max\\{\\Vert u\\Vert,\\Vert v\\Vert\\}=(1+\\varepsilon)\\Vert (u,v)\\Vert_\\infty,$$\nso $\\Vert T\\Vert\\leq 1+\\varepsilon$. Moreover,\n$$\n\\Vert T(x_i,y_i)-(x_i,y_i)\\Vert=\\max\\{\\Vert G(x_i)-x_i\\Vert,\\Vert S(y_i)-y_i\\Vert\\}\\leq\\varepsilon.\n$$\nFinally notice that\n$$T(x,y)=(x',y'),$$\nwhich finishes the proof.\n\\end{proof}\n\nIn order to summarise the content of the section and to relate it with Question \\ref{questiondproyect} we get the following corollary.\n\n\\begin{corollary}\\label{cor:resumen}\nLet $X$ and $Y$ be two Banach spaces with the Daugavet property. Then $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ has the Daugavet property if $X$ and $Y$ satisfy any of the following requirements:\n\\begin{enumerate}\n    \\item To be an $L_1$-predual.\n    \\item To be an $\\ell_\\infty$ sum of $L_1(\\mu)$ spaces.\n    \\item To be an $\\ell_\\infty$ sum of an $L_1(\\mu)$ space and an $L_1$-predual.\n\\end{enumerate}\n\\end{corollary}\n\n\n\n\\section{Further consequences of the ODP}\\label{section:appodp}\n\nIn this section we obtain further connections between the ODP and different questions about the geometry of tensor product spaces.\n\n\n\\subsection{Daugavet property in the projective symmetric tensor product}\n\nGiven a Banach space $X$, we define the\n\\textit{($N$-fold) projective symmetric tensor product} of $X$, denoted by\n$\\widehat{\\otimes}_{\\pi,s,N} X$, as the completion of the space\n$\\otimes^{s,N}X$ under the norm\n\\begin{equation*}\n   \\Vert u\\Vert:=\\inf\n   \\left\\{\n      \\sum_{i=1}^n \\vert \\lambda_i\\vert \\Vert x_i\\Vert^N :\n      u:=\\sum_{i=1}^n \\lambda_i x_i^N, n\\in\\mathbb N, x_i\\in X\n   \\right\\}.\n\\end{equation*}\nThe dual, $(\\widehat{\\otimes}_{\\pi,s,N} X)^*=\\mathcal P(^N X)$, is\nthe Banach space of $N$-homogeneous continuous polynomials on $X$, and notice that $B_{\\widehat{\\otimes}_{\\pi,s,N} X}=\\overline{\\operatorname{co}}(\\{x^N:x\\in S_X\\})$ (see \\cite{flo} for background).\n\nAs far as we are concerned, no non-trivial example of projective symmetric tensor product with the Daugavet property is known. In the sequel, we will provide one such example using  the ODP. In order to do so, let us introduce a bit of notation. Recall that the norm of a Banach space $X$ is said to be \\textit{octahedral} if, whenever $Y$ is a finite-dimensional subspace of $X$ and $\\varepsilon>0$, there exists $x\\in S_X$ such that\n$$\\Vert y+\\lambda x\\Vert>(1-\\varepsilon)(\\Vert y\\Vert+\\vert\\lambda\\vert)$$\nholds for every $y\\in Y$ and every $\\lambda\\in\\mathbb R$. Daugavet property implies octahedrality by \\cite[Lemma 2.8]{kssw}, but the converse is not true as the norm of $\\ell_1$ is octahedral. Now we can prove the following result.\n\n\n\\begin{theorem}\\label{theo:symmocta}\nLet $X$ be a Banach space with the ODP and let $N\\in\\mathbb N$. Then the $N$-fold symmetric projective tensor product, $\\widehat{\\otimes}_{\\pi, s, N} X$, has an octahedral norm.\n\\end{theorem}\n\n\\begin{proof}In order to save notation define $Y:=\\widehat{\\otimes}_{\\pi, s, N} X$. In view of \\cite[Proposition 2.1]{hlp} it is enough to prove that, given $z_1,\\ldots, z_n\\in S_Y$ and $\\varepsilon>0$ we can find $x\\in S_X$ such that\n$$\\left\\Vert z_i+x^N\\right\\Vert>2-\\varepsilon$$\nholds for every $i\\in\\{1,\\ldots, n\\}$. Fix $i\\in\\{1,\\ldots, n\\}$ and choose a norm-one polynomial $P_i$ such that $P_i(z_i)=1$. Furthermore, since $B_Y=\\overline{\\operatorname{co}}(\\{x^N:x\\in S_X\\})$ choose $v_i\\in S_X$ such that $P_i(v_i)>1-\\varepsilon$. Furthermore, find $\\sum_{j=1}^{n_i}\\lambda_{ij}x_{ij}^N\\in \\operatorname{co}(\\{x^N:x\\in S_X\\})$ satisfying that\n\\begin{equation}\\label{ecuasimapprox}\n   \\left\\Vert  z-\\sum_{j=1}^{n_i}\\lambda_{ij}\nx_{ij}^N\\right\\Vert<\\varepsilon.\n\\end{equation}\nSince $X$ has the ODP we can find an element $x\\in S_X$ and operators $\\varphi_i:X\\longrightarrow X$ such that \n\\begin{equation}\\label{ecuasimpuntoscerca}\\Vert \\varphi_i(x_{ij})-x_{ij}\\Vert<\\varepsilon\n\\end{equation}\nholds for every $j\\in\\{1,\\ldots, n_i\\}$, $\\varphi_i(x)=v_i$ and $\\Vert \\varphi_i\\Vert\\leq 1+\\varepsilon$. Now define $Q_i:=P_i\\circ \\varphi_i:X\\longrightarrow \\mathbb R$. Notice that $Q_i$ is a $N$-homogeneous polynomial. In fact, if we denote by $\\hat{P_i}$ the $N$-linear form associated to $P_i$ (i.e. $P_i(z)=\\hat{P_i}(z,z,\\ldots z)$, then $\\hat Q_i(z_1,\\ldots, z_N)=\\hat P_i(\\varphi_i(z_1),\\varphi_i(z_2),\\ldots, \\varphi_i(z_N))$, which is an $N$-linear form because of the linearity and continuity of $\\varphi_i$). Furthermore, in order to estimate the polynomial norm of $ Q_i$, pick $x\\in X$. Then\n$$\\vert Q_i(x)\\vert=\\vert P_i(\\varphi_i(x))\\vert\\leq \\Vert P_i\\Vert \\Vert \\varphi_i(x)\\Vert^N\\leq \\Vert \\varphi_i\\Vert^N \\Vert x\\Vert^N\\leq (1+\\varepsilon)^N\\Vert x\\Vert^N.$$\nSo $\\Vert Q_i\\Vert\\leq (1+\\varepsilon)^N$. Furthermore\n\\[\n\\begin{split}Q_i(z_i)& \\geq \\sum_{j=1}^{n_i}\\lambda_{ij}Q_i(x_{ij})-\n(1+\\varepsilon)^N\\left\\Vert z-\\sum_{j=1}^{n_i}\\lambda_{ij}\nx_{ij}^N\\right\\Vert\\\\\n& \\mathop{>}\\limits^{\\mbox{\\tiny(\\ref{ecuasimapprox})}}\\sum_{j=1}^{n_i} \\lambda_{ij} P_i(x_{ij})-\\Vert P_i\\Vert\\Vert \\varphi_i(x_{ij})-x_{ij}\\Vert^N-(1+\\varepsilon)^N\n\\varepsilon^N\\\\\n& \\mathop{>}\\limits^{\\mbox{\\tiny(\\ref{ecuasimpuntoscerca})}}\\sum_{j=1}^{n_i}\\lambda_{ij}P_i(x_{ij})-\\varepsilon^N-\n(1+\\varepsilon)^N\\varepsilon\\\\\n& \\mathop{>}\\limits^{\\mbox{\\tiny(\\ref{ecuasimapprox})}} P_i(z_i)-\n\\varepsilon-\\varepsilon^N-(1+\\varepsilon)^N\\varepsilon^N\\\\\n& =1-\n\\varepsilon-\\varepsilon^N-(1+\\varepsilon)^N\\varepsilon^N.\n\\end{split}\n\\]\nMoreover\n$$Q_i(x)=P_i(v_i)>1-\\varepsilon.$$\nHence\n$$\\left\\Vert z_i+x^N\\right\\Vert\\geq \\frac{Q_i(z_i+x^N)}{\\Vert Q_i\\Vert}>\\frac{2-2\\varepsilon-\\varepsilon^N-(1\n+\\varepsilon)^N\\varepsilon^N}{(1+\\varepsilon)^N}.$$\nSince $i$ and $\\varepsilon$ were arbitrary we conclude that the norm of $Y$ is octahedral, as desired.\n\\end{proof}\n\n\\begin{remark}\\label{remarkdaugasym}\n Let $X$ be a Banach space with the ODP. Given $x_1,\\ldots, x_n\\in S_X, x'\\in B_X$ and $\\varepsilon>0$, consider the set $A$ of those $x\\in B_X$ for which there exists a bounded operator $\\varphi:X\\longrightarrow X$ such that $\\Vert \\varphi(x_i)-x_i\\Vert<\\varepsilon, \\varphi(x)=x'$ and $\\Vert \\varphi\\Vert\\leq 1+\\varepsilon$. Then, in order to ensure that $\\widehat{\\otimes}_{\\pi,s,N}X$ has the Daugavet property by an adaptation of the proof of Theorem \\ref{theo:symmocta} we need to guarantee that the set $\\{x^N:x\\in A\\}$ is norming for $\\mathcal P(^N X)$ for every $x_1,\\ldots, x_n,x'\\in B_X$ and every $\\varepsilon>0$.\n\\end{remark}\n\nAlthough we do not know whether the property exhibited in the preceding remark holds in general for every ODP space, we will prove in the following proposition that this is the case for spaces of continuous functions.\n\n\\begin{proposition}\\label{propo:tensosime}\nLet $K$ be a compact Hausdorff space without isolated points. Then $\\widehat{\\otimes}_{\\pi,s,N}\\mathcal C(K)$ has the Daugavet property.\n\\end{proposition}\n\nIn order to prove the Proposition we need the following lemma.\n\n\\begin{lemma}\\label{lemac(K)}\nLet $K$ be a compact Hausdorff space without isolated points. Let $E$ be a finite-dimensional subspace of $\\mathcal C(K)$ and $\\varepsilon>0$. Then the set \n$$A:=\\{g\\in S_{\\mathcal C(K)}:\\Vert f+\\lambda g\\Vert>(1-\\varepsilon)(\\Vert f\\Vert+\\vert\\lambda\\vert)  \\forall f\\in E,\\lambda\\in\\mathbb R\\}$$\nis weakly sequentially dense in $B_{\\mathcal C(K)}$.\n\\end{lemma}\n\n\n\\begin{proof} Write $X=\\mathcal C(K)$ for shorten. Let $0<\\delta<\\frac{\\varepsilon}{4}$. Pick $f_1,\\ldots, f_k$ to be a $\\delta$-net in $S_E$ and $h\\in S_X$. Let us find a sequence $\\{g_n\\}\\in S_X$ such that $\\{g_n\\}\\rightarrow h$ in the weak topology of $B_X$ and that $\\Vert f_i+g_n\\Vert>2-2\\delta$ holds for every $i\\in\\{1,\\ldots, k\\}$ and every $n\\in\\mathbb N$. This is enough in view of the proof of \\cite[Lemma 2.8]{kssw}. In order to do so consider, for every $i\\in\\{1,\\ldots, k\\}$, the set\n$$A_i:=\\{t\\in K: \\vert f_i(t)\\vert>1-\\delta\\}.$$\nNote that every $A_i$ is an (infinite) open subset of $K$. Now, making an inductive argument we can find, for every $i\\in\\{1,\\ldots, k\\}$, a sequence of non-empty open sets $V_n^i$ such that $\\overline{V}_n^i\\subseteq A_i$ for every $i\\in\\{1,\\ldots, k\\}$ and such that\n$$\\overline{V}_n^i\\cap \\overline{V}_m^j=\\emptyset$$\nholds for every $n,m\\in\\mathbb N$ and $i,j\\in\\{1,\\ldots, k\\}$ such that either $n\\neq m$ or $i\\neq j$. Now select, for every $i\\in\\{1,\\ldots, k\\}$ and every $n\\in\\mathbb N$ a point $t_n^i\\in V_n^i$. Making use of Urysohn lemma we can construct, for every $n\\in\\mathbb N$, a function $g_n\\in S_X$ such that $g_n(t_n^i)=\\operatorname{sign}(f_i(t_n^i))f_i(t_n^i)$ and $g_n=h$ on $K\\setminus\\bigcup\\limits_{i=1}^k V_n^i$. It is clear that the (bounded) sequence $\\{g_n\\}$ converges pointwise to $h$, so $\\{g_n\\}$ converges weakly to $h$ . Furthermore,\n$$\\Vert f_i+g_n\\Vert\\geq \\vert f_i(t_n^i)+g(t_n^i)\\vert=2\\vert f_i(t_n^i)\\vert>2-2\\delta.$$\nSo the lemma is proved.\n\\end{proof}\n\n\\begin{proof}[Proof of Proposition \\ref{propo:tensosime}]\nLet $X=\\mathcal C(K)$. According to Remark \\ref{remarkdaugasym} we will prove that, given a finite-dimensional subspace $E$ of $X$, $x'\\in B_X$ and a positive $\\varepsilon>0$, if we define\n$$A:=\\{g\\in S_X:\\Vert f+\\lambda g\\Vert>(1-\\varepsilon)(\\Vert f\\Vert+\\vert\\lambda\\vert)  \\forall f\\in E,\\lambda\\in\\mathbb R\\},$$\nthen $\\{g^N:g\\in A\\}$ is norming for $\\mathcal P(^N X)$. Note that if the assertion were proved then, given $x\\in A$, we could construct, by a similar argument to that of the proof of Theorem \\ref{theo:maintheorem}, an operator $\\varphi:X\\longrightarrow X$ such that $\\varphi(e)=e$ for every $e\\in E$, $\\varphi(x)=x'$ and $\\Vert \\varphi\\Vert\\leq 1+\\varepsilon$ since $X$ is an $L_1$- predual. So the Proposition would follow by an application of Remark \\ref{remarkdaugasym}.\n\nHence, in order to prove that $\\{g^N:g\\in A\\}$ is norming for $\\mathcal P(^N X)$, pick a norm-one polynomial $P\\in \\mathcal P(^N X)$, a positive $\\varepsilon>0$ and a point $y\\in S_Y$ such that $P(y)>1-\\varepsilon$. By Lemma \\ref{lemac(K)} we get that $A$ is weakly sequentially dense in $B_X$, so we can find a sequence of points in $A$, say $\\{g_n\\}$, such that $\\{g_n\\}\\rightarrow y$ in the weak topology of $B_X$. Now, since $X$ has the Dunford-Pettis property \\cite[Theorem 5.4.5]{alka} then $P(g_n)\\rightarrow P(y)>1-\\varepsilon$ \\cite[Corollary 5.1]{gjl}, so we can find $n\\in\\mathbb N$ such that $P(g_n)>1-\\varepsilon$. Since $g_n\\in A$, the arbitrariness of $P$ and $\\varepsilon$ proves the fact that $\\{g^N: g\\in A\\}$ is norming for $\\mathcal P(^N X)$, and the proposition is proved.\n\\end{proof}\n\n\\begin{remark}\nTo the best of our knowledge, Proposition \\ref{propo:tensosime} provides the first non-trivial example of a projective symmetric tensor product enjoying the Daugavet property.\n\\end{remark}\n\n\\begin{remark}\nNote that Lemma \\ref{lemac(K)} does not hold for general Banach spaces with the Daugavet property. Indeed, in \\cite[Theorem 2.5]{kw} an example of a Banach space with the Daugavet property and the Schur property is exhibited.\n\\end{remark}\n\n\\subsection{2-roughness in projective tensor product}\n\n\n\nLet $X$ be a Banach space. Recall that the norm of $X$ is said to be \\textit{$\\varepsilon$- rough} if \n$$\\limsup\\limits_{\\Vert h\\Vert\\rightarrow 0} \\frac{\\Vert x+h\\Vert+\\Vert x-h\\Vert-2\\Vert x\\Vert}{\\Vert h\\Vert}\\geq \\varepsilon$$\nholds for every $x\\in X$.\n\nNote that rough norms are ``uniformly non-Fr\\'echet differentiable\". See \\cite[Chapter 1]{dgz} for background on rough norms.\n\nOur aim will be to prove the following result.\n\n\\begin{proposition}\\label{propo2-ruda}\nLet $X$ be a Banach space with the ODP. Then the norm of $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ is $2$-rough for every non-zero Banach space $Y$.\n\\end{proposition}\n\nIn order to prove the proposition we need the following reformulation of 2-roughness, which will be useful in the sequel. The proof (1)$\\Longleftrightarrow$(2) is \\cite[Proposition I.1.11]{dgz} whereas that of (1)$\\Longleftrightarrow$(3) is \\cite[Lemma 3.1]{hlp}.\n\n\\begin{lemma}\\label{lema:cara2-ruda}\nLet $X$ be a Banach space. The following assertions are equivalent:\n\\begin{enumerate}\n    \\item The norm of $X$ is $2$-rough.\n    \\item Every weak-star slice of $B_{X^*}$ has diameter two.\n    \\item $X$ is \\textit{locally octahedral} (LOH), that is, for every $x\\in S_X$ and every $\\varepsilon>0$ there exists $y\\in S_X$ such that $\\Vert x\\pm y\\Vert>2-\\varepsilon$.\n\\end{enumerate}\n\\end{lemma}\n\n\n\\begin{proof}[Proof of Proposition \\ref{propo2-ruda}]\nIn order to prove that the norm of $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ is $2$-rough, we will make use of Lemma \\ref{lema:cara2-ruda} (3). Pick an element $z\\in S_{X\\ensuremath{\\widehat{\\otimes}_\\pi} Y}$ and $\\varepsilon>0$, and let us find an element $x\\otimes y\\in S_{X\\ensuremath{\\widehat{\\otimes}_\\pi} Y}$ such that $\\Vert z\\pm x\\otimes y\\Vert>2-\\varepsilon$.\n\nChoose $\\eta>0$ small enough so that $\\frac{2-\\eta(4+\\eta)}{1+\\eta}>2-\\varepsilon$. Pick a norm-one bilinear form $G$ such that $G(z)>1-\\eta$ and $x_0\\in S_X,y_0\\in S_Y$ such that $G(x_0,y_0)>1-\\eta$.\n\nFrom the definition of projective norm consider $n\\in\\mathbb N, x_1,\\ldots, x_{ n}\\in S_X$, $y_1,\\ldots, y_{ n}\\in S_Y$ and $\\lambda_1,\\ldots, \\lambda_n\\in [0,1]$ such that $\\sum_{i=1}^n \\lambda_i=1$ and \n\\begin{equation*\n\\left\\Vert z-\\sum_{i=1}^{ n} \\lambda_i x_i\\otimes y_i\\right\\Vert<\\eta.\n\\end{equation*}\nSince $X$ has the ODP we can find $x\\in S_X$ and  $\\psi^\\pm:X\\longrightarrow X$ such that $\\Vert \\psi^\\pm(x_i)-x_i\\Vert<\\eta, \\psi^\\pm(x)=\\pm x_0$ and \n$ \\Vert \\psi^\\pm\\Vert\\leq 1+\\eta$.\n\nNow, define the bilinear forms $T^\\pm(u,v):=G(\\psi^\\pm(u),v)$ for $u\\in X, v\\in Y$. We claim that $\\Vert T^\\pm \\Vert\\leq 1+\\eta$. Indeed, given $u\\in B_X$ and $y\\in B_Y$ we have\n$$\n\\vert G(\\psi^\\pm(u),v)\\vert\\leq \\Vert G\\Vert\\Vert \\psi^\\pm(u)\\Vert \\Vert v\\Vert\\leq 1+\\eta.\n$$\nFurthermore,\n\\[\\begin{split}\nT^\\pm(z) & \\geq \\sum_{i=1}^{ n} \\lambda_i G(\\psi^\\pm(x_i),y_i)-\\eta\\Vert T^\\pm\\Vert \\\\\n& \\geq \\sum_{i=1}^{ n} \\lambda_i G(x_i)(y_i)-\\sum_{i=1}^n \\lambda_i \\Vert G\\Vert \\Vert \\psi^\\pm (x_i)-x_i\\Vert\\Vert y_i\\Vert-\\eta(1+\\eta)\\\\\n& >1-\\eta(3+\\eta).\n\\end{split}\n\\]\nAlso\n$$T^\\pm(\\pm x,y)=G(x_0,y_0)>1-\\eta.$$\nFinally\n\\[\n\\begin{split}\n\\Vert z\\pm x\\otimes y\\Vert&  \\geq \\frac{T^\\pm\\left(\\pm x\\otimes y+z\\right)}{\\Vert T^\\pm\\Vert}\\\\\n& \\geq \\frac{1-\\eta+T^{\\pm}(z)}{1+\\eta}\\\\\n& \\geq \\frac{2-\\eta(4+\\eta)}{1+\\eta}\\\\\n&  >2-\\varepsilon.\n\\end{split}\n\\]\nSince $\\varepsilon>0$ was arbitrary then $X\\ensuremath{\\widehat{\\otimes}_\\pi} Y$ is LOH, and we are done.\n\\end{proof}\n\n\\begin{remark}\nAn examination of the previous proof yields that, in Proposition \\ref{propo2-ruda}, if we localise $z$ in a given slice then we can get the \\textit{local diametral diameter two property} (see \\cite{blr2} for definition and backgroud). We do thank Johann Langemets for pointing out to the authors this improvement.\n\\end{remark}\n\nIn order to obtain some consequences from Proposition \\ref{propo2-ruda}, let us introduce a bit of notation. Given a Banach space $X$, recall that $X$ is said to have the \\textit{slice diameter two property (slice-D2P)} if every slice of the unit ball of $B_X$ has diameter two. If $X$ is itself a dual Banach space, then $X$ is said to have the \\textit{weak-star slice diameter two property ($w^*$-slice-D2P)} if every weak-star slice of $B_X$ has diameter two. See \\cite{aln, blr, llr} and references therein for background on diameter two properties.\n\nTaking into account the duality $L(X,Y^*)=(X\\ensuremath{\\widehat{\\otimes}_\\pi} Y)^*$ and Lemma \\ref{lema:cara2-ruda} we get the following result.\n\n\\begin{corollary}\nLet $X$ be a Banach space with the ODP. Then $L(X,Y^*)$ has the $w^*$-slice-D2P.\n\\end{corollary}\n\nLet us end with an application of Proposition \\ref{propo2-ruda} to the study of the slice diameter two property in injective tensor products. \n\n\\begin{corollary}\\label{cor:sliced2pinyec}\nLet $(\\Omega,\\Sigma,\\mu)$ be a measure space and assume that $\\mu$ does not contain any atom. Let $X$ be a Banach space such that $X^*$ has the RNP. Then $L_1(\\mu)\\ensuremath{\\widehat{\\otimes}_\\varepsilon} X$ has the slice-D2P.\n\\end{corollary}\n\n\\begin{proof}\nNotice that $(L_1(\\mu)\\ensuremath{\\widehat{\\otimes}_\\varepsilon} X)^*=L_\\infty(\\mu)\\ensuremath{\\widehat{\\otimes}_\\pi} X^*$ by \\cite[Theorem 5.33]{rya}, so its norm is $2$-rough by Proposition \\ref{propo2-ruda}. Consequently, $L_1(\\mu)\\ensuremath{\\widehat{\\otimes}_\\varepsilon} X$ has the slice-D2P by \\cite[Theorem 3.3]{hlp}.\n\\end{proof}\n\n\nIn \\cite[Question b)]{aln} it is asked how are the diameter two properties, in general, preserved by tensor product spaces. Corollary \\ref{cor:sliced2pinyec} yields new examples of injetive tensor products with the slice-D2P different from those obtained in \\cite[Theorem 5.3]{abr} and of \\cite[Theorem 2.6]{llr}.\n\n\n\\section*{Acknowledgements}  We thank Vladimir Kadets and Dirk Werner for fruitful conversations. Part of the research of this paper was developed during several visits of the first author to Instituto de Ciencias Matem\\'aticas in June 2018 and to Facultad de Matem\\'aticas of the Universidad Complutense de Madrid in February 2019. He is grateful to both institutions for hospitality and excellent working conditions during both visits.\n\n", 0.7775705569034354], ["\\section{Introduction}\n\nLet $d$ be an integer, $f:\\R^d\\rightarrow\\R$ be a $\\mathcal{C}^2$ function, and consider the problem of minimizing $f$ over $\\R^d$. A well-known efficient algorithm to numerically solve this minimization problem is Newton's method: starting at some point $x_0$, it considers the sequence\n$$\nx_{k+1}=x_k-h_kH(f)^{-1}_{x_k}\\nabla f(x_k),\n$$\nwith $\\nabla f$ the gradient of $f$, $H(f)$ its Hessian, and $h_k>0$ some appropriate step. \n\n\\medskip\n\nHowever, very often the Hessian of $f$ is too difficult to compute, leading to the  introduction  of the so-called quasi-Newton methods. The method defines a sequence\n$$\nx_{k+1}=x_k-h_kB_k^{-1}\\nabla f(x_k),\n$$ \nwhere $(B_k)$ is a sequence of symmetric matrices such that\n\\begin{equation}\n\\label{bkN}\nB_{k+1}(x_{k+1}-x_k)=\\nabla f(x_{k+1})-\\nabla f(x_k).\n\\end{equation}\nIndeed, since\n$$\n\\begin{aligned}\n\\nabla f(x_{k+1})-\\nabla f(x_k)&=\\left(\\int_0^1H(f)_{x_k+t(x_{k+1}-x_k)}dt\\right)(x_{k+1}-x_k)\\simeq H(f)_{x_{k}}(x_{k+1}-x_k),\n                               \\end{aligned}\n$$\nwe get\n$$\nB_{k+1}(x_{k+1}-x_k)\\simeq H(f)_{x_k}(x_{k+1}-x_k).\n$$\nIt is then expected that $B_k$ is close to $H(f)_{x_k}$ in the direction $s_k=x_{k+1}-x_k$. See \\cite{qn1,qn2,qn3} for more. \n\n\n\nThere are many ways to build a matrix sequence $(B_k)$ satisfying \\eqref{bkN}. However, it was proved in \\cite{CGT} and \\cite{S} that some of these methods let $B_k$ approximate $H(f)_{x_k}$ in all directions instead of just one, i.e.\n$$\n\\Vert B_k-H(f)_{x_k}\\Vert\\underset{k\\rightarrow\\infty}{\\rightarrow }0\\quad\\Longrightarrow\\quad\n\\Vert B_k-H(f)_{x_*}\\Vert\\Vert B_k-H(f)_{x_k}\\Vert\\underset{k\\rightarrow\\infty}{\\rightarrow }0.\n$$\nwith the additional assumption of the uniform linear independence of the sequence \n$$\ns_k=x_{k+1}-x_k,\n$$\na notion that will be recalled later. In \\cite{CGT} for example, this is proved for the update of $B_k$ by\n\\begin{equation}\n\\label{bk}\ny_k=\\nabla f(x_{k+1})-\\nabla f(x_k)=A_ks_k,\\quad r_k=B_ks_k-y_k,\\quad B_{k+1}=B_k+\\frac{r_kr_k^T}{r_k^Ts_k},\n\\end{equation}\nwith \n$$\nA_k=\\int_0^1H(f)_{x_{k}+t(x_{k+1}-x_k)}dt.\n$$\n\nIn this paper, our aim is to generalize the approach in \\cite{CGT} by defining the above symmetric rank-one algorithm for any sequences of symmetric matrices $(A_k)$ and vectors $(s_k)$, and derive a convergence result, opening a wider range of applications. \n\n\\medskip\n\nFor instance, if a sequence $A_k$ converges to an invertible matrix $A_*$, we can use the above algorithm to approximate the inverse $A_*^{-1}$ of the limit $A_*$. Indeed, let $(e_0,\\dots,e_{d-1})$ be the canonical vector basis of $\\R^d$, and define the sequence $(s_k)$ in $\\R^d$ by\n\\begin{equation}\n\\label{appli}\ns_k:=A_ke_{k[d]},\\quad y_k=A_k^{-1}s_k=e_{k[d]},\n\\end{equation}\nwhere $k[d]$ is the remainder of the Euclidean division of $k$ by $d$. This sequence is uniformly linearly independent, hence the sequence $B_k$ defined by \\eqref{bk} will converge to $A_*^{-1}$. This convergence might be a little slow depending on the dimension $d$ and the rate of convergence of $A_k$, but $B_k$ is much easier to compute than $A_k^{-1}$.\n\n\\medskip\n\nThis can be used to compute geodesics constrained to embedded submanifolds of Riemannian spaces. Indeed, to obtain a geodesic between two fixed points of a submanifold, we need to find a converging sequence of maps $t\\mapsto\\lambda_k(t)$ given implicitly by an equation of the form (\\cite{ATY})\n$$\nA_k(t)\\lambda_k(t)=c_k(t),\n$$\nwhere $A_k(t)$ is a convergent sequence of symmetric, positive definite matrices of high dimension. The $\\lambda_k$ are the Lagrange multipliers induced by the equations of the submanifold. It is very consuming to solve such a linear system for every time $t$ and every step $k$. Instead, we can take\n$$\n\\lambda_k(t)=B_k(t)c_k(t),\n$$\nwith $B_k(t)$ obtained by applying the symmetric rank-one algorithm described in the previous paragraph. This is particularly useful in Shape Spaces, where the studied manifolds have a very high dimension and a very complex metric. The present article was actually motivated by such a problem appearing in shape analysis, investigated in \\cite{ATY}.\n\n\\medskip\n\nThis paper is structured as follows. We  give the general framework in Section \\ref{sec1}, then state the main result after recalling two equivalent definitions of the uniform linear independence of a sequence of vectors in Section \\ref{sec2}. Then, Section \\ref{sec3} is dedicated to intermediary results that will, along with notions developed in Section \\ref{sec4}, derive the proof of our theorem.\n\n\\section{Notations and symmetric rank-one algorithm}\n\\label{sec1}\n\nConsider a sequence $(A_k)_{k\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}}$ of real square symmetric matrices of size $d$. Assume that this sequence converges to some matrix $A_*$, i.e.\n$$\n\\Vert A_k-A_*\\Vert\\underset{k\\rightarrow\\infty}{\\rightarrow }0,\n$$\nwhere $\\Vert\\cdot\\Vert$ is the operator norm on $M_d(\\R)$ induced by the canonical Euclidean norm $\\vert\\cdot\\vert$ on $\\R^d$. Then define\n$$\n\\eta_{k,l}=\\sup_{k\\leq i\\leq l}\\Vert A_i-A_k\\Vert,\\quad\\text{and}\\quad\\eta_{k,*}=\\sup_{ i\\geq k}\\Vert A_i-A_k\\Vert\n$$\nfor all $k\\leq l\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$. Note that\n$$\n\\forall k\\leq l\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P},\\quad \\eta_{k,l}\\leq\\eta_{k,*}\\qquad\\text{and}\\qquad \\eta_{k,*}\\rightarrow 0\\quad\\text{as}\\quad k\\rightarrow\\infty.\n$$\nNow let $(s_k)_{k\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}}$ be a sequence of vectors of $\\R^d$.\n\n\\medskip\n\\noindent\nRecall that we want to find a somewhat simple sequence $(B_k)_{k\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}}$ of symmetric matrices such that $B_k\\rightarrow A_*$, using only $s_k$ and $y_k=A_ks_k$.\n\n\\medskip\n\nWe use the symmetric rank-one update method from \\cite{CGT}. Start with $B_0=I_d$. Then, define for $k\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$\n$$\ny_k=A_ks_k,\\quad r_k=(A_k-B_k)s_k=y_k-B_ks_k,\n$$ \nthen take\n$$\nB_{k+1}=B_k+\\frac{r_kr_k^T}{r_k^Ts_k}.\n$$\nIt is of course required that $r_k^T s_k\\neq 0$ for every $k$.\n\n\\section{Main Result}\n\n\\label{sec2}\n\nFor every $k$, we have\n$$\nB_{k+1}s_k=B_ks_k+r_k=B_ks_k+y_k-B_ks_k=y_k,\n$$\nso\n$$\nA_{k}s_k=B_{k+1}s_k.\n$$\nThe main idea is that if $A_k,$ $A_{k+1}$, $\\dots$, $A_{k+m}$ are not too far apart (i.e., for $k$ large enough), we expect $B_{k+m}s_{k+i}$ to be relatively close to $A_{k+m}s_{k+i}$ for $i\\leq m$. Then, if we can extract from every finite subsequence $(s_k,\\dots,s_{k+m})$ a vector basis of $\\R^d$, we will obtain the desired convergence.\n\n\\medskip\n\nFor a more precise statement, we next define the notion of uniform linear independence. The most intuitive and geometric definition is the following.\n\n\\begin{defi}\n\\label{uli1}\nTake a sequence $s=(s_k)_{k\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}}$ of vectors in $\\R^d$, $d\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}^*$, and let $m\\geq d$ be an integer. Then $s$ is said to be $m$-uniformly linearly independent if for some constant $\\alpha>0$, and for all $k\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$, there are $d$ integers $k\\leq k_1<\\dots<k_d\\leq k+m$ such that\n$$\n\\left\\vert\\det\\left( s_{k_1},\\dots,s_{k_d}\\right)\\right\\vert\\geq \\alpha\\vert s_{k_1}\\vert\\dots\\vert s_{k_d}\\vert.\n$$\n\\end{defi}\nIn other words, from every finite segment of $(s_k)$ of length $m$, we can extract a linear basis ${s_{k_1}},\\dots,{s_{k_d}}$ that will, once normalized, form a parallelepiped that does not become flat as $k$ goes to infinity.\n\n\\medskip\n\nAnother definition was given in \\cite{CGT} after \\cite{ORBOOK} as follows.\n\\begin{defi}\n\\label{uli2}\nA sequence $s=(s_k)_{k\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}}$ of vectors in $\\R^d$, $d\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}^*$, is said to be $(m,\\beta)$-uniformly linearly independent, where $d\\leq m\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$ and $\\beta\\in\\R$, if for all $k\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$, there are $d$ integers $k\\leq k_1<\\dots<k_d\\leq k+m$ such that\n$$\n\\left\\vert \\lambda\\left(\\frac{s_{k_1}}{\\vert s_{k_1}\\vert},\\dots,\\frac{s_{k_d}}{\\vert s_{k_d}\\vert}\\right)\\right\\vert\\geq \\beta,\n$$\nwhere $\\lambda(M)$ is the complex eigenvalue of the square matrix $M$ with smallest module.\n\\end{defi}\n\n\\textbf{Remark:} A sequence $s=(s_k)$ in $\\R^d$ is $(m,\\beta)$-uniformly linearly independent for some $m\\geq d$ and $\\beta>0$ if and only it is $m$-uniformly linearly independent in the sense of Definition \\ref{uli1}.  Indeed, let $v_1,\\dots,v_d\\in \\R^d$, and denote \n$\nV=\\displaystyle\\left(\\frac{v_1}{\\vert v_1\\vert},\\dots,\\frac{v_d}{\\vert v_d\\vert}\\right).\n$\nIf $\\vert\\lambda(V)\\vert\\geq \\beta>0,$ then\n$\n\\det(V)\\geq \\beta^d,\n$\n which proves the first part of the equivalence. On the other hand,\nwe know that the eigenvalue of $V$ with largest modulus has modulus less than\n$\n\\displaystyle\\sqrt{d}\\max_{i=1,\\dots,d}\\frac{\\vert s_{k_i}\\vert}{\\vert s_{k_i}\\vert}=\\sqrt{d}.\n$\nNow, assume that $\\det(V)\\geq \\alpha>0$. Then\n$\n\\vert\\lambda(V)\\vert\\geq \\frac{\\alpha}{d^{\\frac{d-1}{2}}},\n$\nensuring the second part of the equivalence.\n\n\\medskip\n\nThis definition is sufficient to state our main result. \n\\begin{theo}\nLet $(A_k),\\ (s_k),\\ (y_k),\\ (r_k)$ and $(B_k)$ be defined as in Section \\ref{sec1}, with $(A_k)$ having a limit $A_*$. Assume that for some fixed constant $c>0$,\n$$\n\\vert r_k^T s_k\\vert\\geq c\\vert r_k\\vert\\vert s_k\\vert.\n$$\nThen, for every $\\beta>0$ such that $(s_k)$ is $(m,\\beta)$-uniformly linearly independent in the sense of Definition \\ref{uli2}, we have for all $k\\in \\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$ the quantitative estimates\n\\begin{equation}\n\\label{resss}\n\\Vert B_{k+m}-A_*\\Vert\\leq \\left(1+\\left(\\frac{2+c}{c}\\right)^{m+1}\\right)\\frac{\\sqrt{d}}{\\beta}\\eta_{k,*}.\n\\end{equation}\n\\end{theo}\n\nThe next sections are dedicated to the proof of this theorem.\n\n\\section{First estimates}\n\\label{sec3}\n\nIn this section, we give upper bounds on\n$$\n\\left\\vert(B_{k+m}-A_{k})\\frac{s_k}{\\vert s_k\\vert}\\right\\vert,\n$$ \nand deduce estimates on \n$$\n\\left\\vert\\frac{(B_{k+m}-A_{*})x}{\\vert x\\vert}\\right\\vert\n$$\nfor a particular set of $x\\in \\R^d$.\n\\begin{propo}\n\\label{mainlemma}\nLet $(A_k)_{k\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}}$ be a sequence of real symmetric matrices in $M_{d}(\\R)$, $d\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$, and $(s_k)$ be any sequence in $\\R^d$. Define $y_k$, $B_k$ and $r_k$ as above. Assume that for some fixed constant $0<c\\leq 1$ and for all $k\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$,\n$$\nr_k^T s_k\\geq c\\vert r_k\\vert\\vert s_k\\vert.\n$$\nThen, for all $l\\geq k+1$, \n$$\n\\vert (A_k-B_l)s_k\\vert\\leq \\left(\\frac{2+c}{c}\\right)^{l-k-1}\\eta_{k,l-1}\\vert s_k\\vert.\n$$\n\\end{propo}\n\n\\textbf{Proof:} We prove this inequality by induction on $l$, with $k\\in \\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$ fixed. For $l=k+1$, we know that $B_{k+1}s_k=A_ks_k=y_k$, hence\n$$\n\\vert (A_{k}-B_{k+1})s_k\\vert=0.\n$$\nWe will use the notation\n$$\nIH(l):=\\left(\\frac{2+c}{c}\\right)^{l-k-1}\\eta_{k,l-1}\\vert s_k\\vert,\n$$ \nwhere $IH$ stands for Induction Hypothesis. Now, assume the result to be true for some $l\\geq k+1$, i.e.\n\\begin{equation}\n\\label{IH}\n\\vert (A_k-B_l)s_k\\vert\\leq \\left(\\frac{2+c}{c}\\right)^{l-k-1}\\eta_{k,l-1}\\vert s_k\\vert=IH(l).\n\\end{equation}\nLet us prove that\n$$\n\\vert (A_k-B_{l+1})s_k\\vert\\leq \\left(\\frac{2+c}{c}\\right)^{l-k}\\eta_{k,l}\\vert s_k\\vert=IH(l+1).\n$$\nNote that\n\\begin{equation}\n\\label{akbl}\n\\begin{aligned}\n\\vert (A_k-B_{l+1})s_k\\vert&=\\vert A_ks_k-(B_l+\\frac{r_lr_l^T}{r_l^Ts_{l}})s_k\\vert\\\\\n&=\\vert A_ks_k-B_ls_k-\\frac{r_lr_l^Ts_k}{r_l^Ts_{l}}\\vert\\\\\n&\\leq \\vert (A_k-B_l)s_k\\vert+\\frac{\\vert r_l\\vert\\vert r_l^Ts_k\\vert}{c\\vert r_l\\vert\\vert s_l\\vert}\\\\\n&\\leq IH(l)+\\frac{\\vert r_l^Ts_k\\vert}{c\\vert s_l\\vert}.\n\\end{aligned}\n\\end{equation}\nLet us find a bound for  $\\frac{\\vert r_l^Ts_k\\vert}{c\\vert s_l\\vert}$, the second term of the right-hand side. First we have\n$$\n\\begin{aligned}\n\\vert r_l^Ts_k\\vert&=\\vert y_l^Ts_k-s_l^TB_l s_k\\vert\\\\ \n&\\leq \\vert y_l^Ts_k-s_l^Ty_k\\vert+\\vert s_l^T(y_k-B_ls_k) \\vert\\\\\n&=\\vert y_l^Ts_k-s_l^Ty_k\\vert+\\vert s_l^T(A_k-B_l)s_k \\vert\\\\ \n&\\leq \\vert y_l^Ts_k-s_l^Ty_k\\vert+\\vert s_l\\vert IH(l).\n\\end{aligned}\n$$\nHowever, since $A_l$ is symmetric and $y_l=A_ls_l$,\n$$\n\\vert y_l^Ts_k-s_l^Ty_k\\vert=\\vert s_l^T(A_l-A_k)s_k\\vert\\leq \\eta_{k,l} \\vert s_l\\vert\\vert s_k\\vert,\n$$\nfrom which we deduce\n$$\n\\vert r_l^Ts_k\\vert\\leq \\eta_{k,l} \\vert s_l\\vert\\vert s_k\\vert+IH(l)\\vert s_l\\vert.\n$$\nGoing back to Inequality \\eqref{akbl}, we get\n$$\n\\begin{aligned}\n\\vert (A_k-B_{l+1})s_k\\vert&\\leq IH(l)+\\frac{\\vert r_l^Ts_k\\vert}{c\\vert s_l\\vert}\\\\ \n&\\leq IH(l)+\\frac{1}{c} \\eta_{k,l} \\vert s_k\\vert+\\frac{1}{c} IH(l)\\\\\n&=(1+\\frac{1}{c}) IH(l)+\\frac{1}{c}\\eta_{k,l}\\vert s_k\\vert\\\\\n&=\\frac{1+c}{c}\\left(\\frac{2+c}{c}\\right)^{l-k-1}\\eta_{k,l-1}\\vert s_k\\vert+\\frac{1}{c}\\eta_{k,l}\\vert s_k\\vert \\\\\n&\\leq \\left(\\frac{2+c}{c}\\right)^{l-k}\\eta_{k,l}\\vert s_k\\vert=HI(l+1),\n\\end{aligned}\n$$\nwhere the last inequality comes from the simple fact that $\\eta_{k,l-1}\\leq\\eta_{k,l}.\\ \\square$\n\n\\medskip\n\nThis proposition shows that if $A_k,\\ A_{k+1},\\dots,\\ A_l$ are not too far away from each other (i.e. if $\\eta_{k,l}$ is small), then $B_ls_k$ stays quantifiably close to $A_{k}s_k$.\n\n\\medskip\n\nNow, note that $\\Vert A_*-A_k\\Vert\\leq \\eta_{k,*}$, and \n$\n\\eta_{k,*}\n$\ndecreases to $0$ as $k$ goes to infinity. Keeping the same assumptions, we obtain the following result.\n\n\\begin{cor}\n\\label{aprox}\nTake $m,k\\in \\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$, and let $x\\in \\R^d$ be in the span of $s_k,\\dots,s_{k+m}$. If\n$$\n\\frac{x}{\\vert x\\vert}=\\sum_{i=0}^m \\lambda_i\\frac{s_{k+i}}{\\vert s_{k+i}\\vert},\\quad \\lambda_0,\\dots,\\lambda_m\\in\\R,\n$$\nthen\n$$\n\\frac{\\vert B_{k+m}x-A_*x\\vert}{\\vert x\\vert}\\leq \\eta_{k,*}\\left(1+\\left(\\frac{2+c}{c}\\right)^{m+1}\\right)\\sum_0^m\\vert\\lambda_i\\vert.\n$$\n\\end{cor}\n\\textbf{Proof:} First, it follows from Lemma \\ref{mainlemma} that\n$$\n\\begin{aligned}\n\\frac{\\vert B_{k+m}x-A_{*}x\\vert}{\\vert x\\vert}\n&\\leq \\sum_{i=0}^m \\frac{\\vert \\lambda_{k+i}\\vert}{\\vert s_{k+i}\\vert}\\vert  B_{k+m}s_{k+i}-A_{*}s_{k+i}\\vert\\\\\n&\\leq \\sum_{i=0}^m \\frac{\\vert \\lambda_{k+i}\\vert}{\\vert s_{k+i}\\vert}\\Big(\\vert B_{k+m}s_{k+i}-A_{k+i}s_{k+i}\\vert+\\vert A_{k+m}s_{*}-A_{k+i}s_{k+i}\\vert\\Big)\\\\\n&\\leq \\sum_{i=0}^m \\vert \\lambda_i\\vert\\left(\\left(\\frac{2+c}{c}\\right)^{i+1}\\eta_{k,k+m-1}+\\eta_{k,*}\\right).\n\\end{aligned}\n$$\n\nNow, if we take\n$$\nC(m)=\\left(1+\\left(\\frac{2+c}{c}\\right)^{m+1}\\right)\n$$\nand use the fact that $\\eta_{k,k+m}\\leq \\eta_{k,*}$, then we get\n$$\n\\frac{\\vert B_{k+m}x-A_*x\\vert}{\\vert x\\vert}\n\\leq \\eta_{k,*}C(m)\\sum_0^m\\vert\\lambda_i\\vert.\n$$\nThe result follows. $\\square$\n\n\\medskip\n\nIn particular, if we can let $k$ go to infinity while keeping\n$\n\\displaystyle\\sum_{i=0}^m \\vert \\lambda_i\\vert\n$\nbounded, then we obtain $B_{k+m}x\\rightarrow A_*x$. Thus, if we can do it for all $x\\in \\R^d$, we will have proved that $B_k\\rightarrow A_*$.\n\n\\medskip\n\nIn other words, we need every normalized vector $x\\in\\R^d$ to be a uniformly bounded linear combination of $s_k,\\dots,s_{k+m}$ as $k$ goes to infinity. In the next section of this paper, we will define a third notion of uniform linear independence of a sequence directly related to to this property and prove that it is equivalent to the previous definitions.\n\n\\section{Uniform $m$-span of a sequence and applications}\n\n\\label{sec4}\n\nIn order to investigate the subspace on which $B_k\\rightarrow A_*$, we need a notion that is more precise  than uniform linear independence.\n\n\\begin{defi}\n\\label{usm}\nLet $s=(s_k)_{k\\geq 0}$ be a sequence in $\\R^d$, and let $m\\in \\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$. We say that a vector $x$ in $\\R^d$ is uniformly in the $m-$span of $s$ if for some fixed $\\gamma_x>0$,\n\\begin{equation}\n\\label{unif}\n\\forall k\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P},\\quad \\exists\\lambda_0,\\dots,\\lambda_m\\in\\R\\quad\\frac{x}{\\vert x\\vert}=\\sum_{i=0}^m \\lambda_i\\frac{s_{k+i}}{\\vert s_{k+i}\\vert}\\quad \\text{and}\\quad \\sum_{i=0}^m \\vert \\lambda_i\\vert\\leq \\gamma_x.\n\\end{equation}\nWe denote by $US_m(s)$ the set of all such vectors. \n\n$US_m(s)$ is a vector sub-space of $\\R^n$. Moreover, there exists a constant $\\gamma>0$ such that Property \\eqref{unif} holds for all $x\\in US_m(s)$ with $\\gamma_x=\\gamma$, i.e.\n\\begin{equation}\n\\label{gsm}\n\\exists\\gamma>0,\\quad\\forall k\\in\\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P},\\ x\\in US_m(s),\\quad \\exists\\lambda_0,\\dots,\\lambda_m\\in\\R,\\quad\\frac{x}{\\vert x\\vert}=\\sum_{i=0}^m \\lambda_i\\frac{s_{k+i}}{\\vert s_{k+i}\\vert}\\quad \\text{and}\\quad \\sum_{i=0}^m \\vert \\lambda_i\\vert\\leq \\gamma.\n\\end{equation}\n\\end{defi}\n\nTo prove the existence of $\\gamma$ in \\eqref{gsm}, it suffices to consider an orthonormal basis $(x_i)_i$ of $US_m(s)$, associated with some constants\n $(\\gamma_{x_i})_{1\\leq i\\leq d},$ in Property \\eqref{unif}. Then we can just take $\\gamma=\\gamma_{x_1}+\\dots+\\gamma_{x_d}$.\n \n\\medskip\n\\noindent\n\\textbf{Remark:} There holds \n$\n\\displaystyle US_m(s)\\subset\\bigcap_{k=0}^{\\infty}\\text{span}(s_k,\\dots,s_{k+m}).\n$\n\n\\medskip\n\\noindent\n\\textbf{Example:} Define the sequence $s=(s_k)$ by \n$$\ns_k=\\left\\lbrace \\begin{aligned}&e_{k[d]}&\\text{when}&\\quad k[d]\\neq n-1,\\\\\n&e_0+\\frac{1}{k}e_{d-1}&\\text{when}&\\quad k[d]= n-1,\n\\end{aligned}\\right.\n$$ \nwhere $k[d]$ is the remainder of the Euclidean division of $k$ by $d$. Then\n$$\nUS_m(s)=\\left\\lbrace\\begin{aligned}&\\lbrace 0\\rbrace\\quad \\text{if}\\quad 0\\leq m\\leq n-2\\\\\n&\\text{span}(e_0,\\dots,e_{d-2})\\ \\text{otherwise}.\n\\end{aligned}\\right. \n$$\n\nUsing this definition, a simple application of Corollary \\ref{aprox} gives the following result.\n\n\\begin{propo}\n\\label{mprop}\nLet $(A_k),\\ (s_k),\\ (y_k),\\ (r_k)$ and $(B_k)$ be defined as in Section \\ref{sec1}, assuming that $(A_k)$ has a limit $A_*$ and that $\\vert r_k^T s_k\\vert\\geq c\\vert r_k\\vert\\vert s_k\\vert$ for some fixed constant $c>0$.\n\n\\noindent\nThen, for every $m\\in \\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$\n\\begin{equation}\n\\label{res}\n\\sup_{x\\in US_m(\\gamma)}\\frac{\\vert B_{k+m}x-A_*x\\vert}{\\vert x\\vert}\\leq C(m)\\gamma\\eta_{k,*},\n\\end{equation}\nwhere $\\gamma$ is taken from \\eqref{gsm} and\n$$\nC(m)=\\left(1+\\left(\\frac{2+c}{c}\\right)^{m+1}\\right).\n$$\n\\end{propo}\n\nFinally the main result follows by combining this proposition with the following lemma.\n\n\\begin{lem}\n\\label{uli3}\nLet $s=(s_k)_{k\\geq 0}$ be a sequence in $\\R^d$, and let $m\\in \\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$. Then $s$ is $(m,\\beta)$-uniformly linearly independent if and only if $US_m(s)=\\R^d$. Moreover, we can take\n$\n\\gamma=\\frac{\\sqrt{d}}{\\beta}\n$ \nin \\eqref{gsm}.\n\\end{lem}\n\\textbf{Proof:} Let $v_1,\\dots,v_d$ be linearly independent elements of $\\R^d$ and define the invertible matrix\n$$\nV=\\left(\\frac{v_1}{\\vert v_1\\vert},\\dots,\\frac{v_d}{\\vert v_d\\vert}\\right).\n$$\nLet $\\Lambda=(\\lambda_1,\\dots,\\lambda_d)\\in \\R^d$, and define $x\\in\\R^d$ a normalized vector such that\n$$\nx=\\sum_{i=1}^d\\lambda_i v_i=V\\Lambda.\n$$\nThen\n$$\n\\sum_{i=1}^d\\vert\\lambda_i\\vert\\leq\\sqrt{d}\\vert\\Lambda\\vert=\\sqrt{d}\\vert\nV^{-1}x\\vert\\leq \\frac{\\sqrt{d}}{\\vert\\lambda(V)\\vert}.\n$$\nThis proves that if a sequence $s=(s_k)$ in $\\R^d$ is $(m,\\beta)$-uniformly linearly independent, then $US_m(s)=\\R^d$ and we can take $\\gamma_m(s)=\\frac{\\sqrt{d}}{\\beta}$.\n\n\\medskip\n\nOn the other hand, take $x\\in\\R^d$ a normalized such that \n$$\nV^{-1T}V^{-1}x=\\frac{1}{\\lambda(V)^2}x.\n$$\nThen, if denoting $(\\lambda_1,\\dots,\\lambda_d)=\\Lambda=V^{-1}x$,\n$$\n\\frac{1}{\\vert\\lambda(V)\\vert}=\\vert\\lambda(V)\\vert\\ \\frac{1}{\\vert\\lambda(V)\\vert^2}=\\vert\\lambda(V)\\vert\\vert V^{-1T}V^{-1}x\\vert=\\vert\\lambda(V)\\vert\\vert V^{-1T}\\Lambda\\vert\\leq \\vert\\Lambda\\vert\\leq \\sum_{i=1}^d\\vert\\lambda_i\\vert,\n$$\nwhich proves the converse. $\\square$\n\n\\medskip\n\nOur main result is proved.\n\n\n\\section{Examples of applications and numerical simulations}\n\nIn this section, after running numerical simulations  of the algorithm on random symmetric matrices, we check that the inverse of a sequence of matrices can indeed be approximated. Then we give an application for computing constrained geodesics between two fixed points in Riemannian manifolds.\n\nAll simulations were done using Matlab.\n\n\n\\subsection{Approximation of a sequence of matrices}\n\nHere we test the algorithm on random symmetric matrices with coefficients generated by a normalized Gaussian law. Let $d\\in \\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}^*$, which will denote the size of the matrices.\n\nFirst we define a square symmetric matrix $A_*=\\frac{1}{2}(M+M^T)$, where the entries of the $d\\times d$ matrix $M$ were chosen at random using the normalized Gaussian law. Then, fix $0<\\lambda<1$, and define the sequence $(A_k)$ of symmetric matrices by perturbating $A_*$ as follows\n$$\nA_k=A_*+\\frac{\\lambda^k}{2}(M_k+M_k^T),\n$$\nwhere $M_k$ is a matrix with random coefficients taken uniformly in $[0,1]$.\n\nObviously, $A_k\\rightarrow A_*$ linearly as $k\\rightarrow\\infty$.\n\nNow, we define the sequence $(B_k)$ thanks to the symmetric rank-one algorithm, starting with $B_0=I_d$, and the sequence $(s_k)$ by the formula\n$$\ns_k=e_{k\\ \\text{mod}\\ d},\\quad k\\in n,\n$$\nwhere $(e_0,\\dots,e_{d-1})$ is the canonical basis of $\\R^d$.\n\n\\medskip\n\nUsing the classical norm given by the Euclidean product $\\left<X,Y\\right>=tr(X^TY)$ on the space of square matrices of size $n$, we give in the following table the distance between $B_k$ and $A_*$. We took $d=10,$ and several values of $\\lambda$ for a various number of steps.\n\n\\bigskip\n\n\n\\centerline{\\begin{tabular}{|c|c|c|c|c|}\n\\hline\n\\text{Number of steps}  & 10 & 20 & 50 & 100\\\\\n\\hline \n$\\lambda=$0.9 & 4 & 2 & 0.1 &  0.005 \\\\ \n\\hline \n$\\lambda=$0.5  & 1 & 1e-3 & 1e-12 & 0 \\\\ \n\\hline \n$\\lambda=$0.1  & 0.02 & 2e-12 & 0 & 0 \\\\ \n\\hline \n\\end{tabular} }\n\n\\bigskip\n\nIn the case of the usual quasi-Newton method (\\cite{ORBOOK}, the goal is to approximate the inverse of the Hessian $H(f)$ of some function $f$ on $\\R^d$. For this, we get a sequence of points $x_k$ converging to the minimum $x_*$, and we define\n$$\n\\begin{aligned}\ns_k&:=x_{k+1}-x_k,\\\\ \nA_k&:=\\int_{0}^1 H(f)_{x_k+ts_k}dt,\\\\\n y_j&:=A_ks_k=\\nabla g(x_{k+1})-\\nabla g(x_k).\n \\end{aligned}\n$$\nThen $\\lim_{k\\rightarrow\\infty}A_k= H(x_*).$\n\\smallskip\n\nThis is a perfect example of a situation where it is easy to compute $A_k s_k$ for some particular $s_k$, but where it might be much harder to compute the actual $A_k$ (or just $H(x_k)$ for that matter). A large number of numerical simulations showing the efficiency of the symmetric rank-one algorithm can be found in \\cite{CGT}.\n\n\\subsection{Approximation of the inverse of a sequence} \n \nAs mentioned in the introduction, another application is the computation of the inverse $A_*^{-1}$ of the limit $A_*$, provided  $A_*$ is invertible.\n\n\\smallskip\n\nIndeed, consider the following sequences for the symmetric rank-one algorithm\n\\begin{equation}\n\\label{skm1}\ns_k:=A_ke_{k\\ \\text{mod}\\ d},\\ y_k=A_k^{-1}s_k=e_{k\\ \\text{mod}\\ d}.\n\\end{equation}\nThen the sequence $(s_j)$ is $(d,\\beta)$-linearly independent for some $\\beta>0$ (at least starting at some $k_0$ large enough). Therefore, the sequence $B_k$ will converge to $A_*^{-1}$. This convergence might be a bit slow depending on the dimension $d$ and the rate of convergence of $A_k$, but $B_k$ is much easier to compute than $A_k^{-1}$.\n\nThis can be useful when solving approximately converging sequences of linear equations, as we will show in the next section.\n\n\\medskip\n\nIn the following numerical simulation, we used the same sequence $(A_k)$ with random coefficients as in the previous section, with $(A_k)$ converging linearly to a random matrix $A_*$ with rate $\\lambda=0.5$. We then computed the distance between $B_k$ and $A_*^{-1}$. We also added an extra test. Indeed, the form of the sequence $s_k$ in \\eqref{skm1} has no reason to be particularly good (i.e. uniformly linearly independent with a nice constant). Therefore, we applied the algorithm by taking a random vector $y_k$ with coefficients taken along a normal Gaussian law at each step and $s_k=A_ky_k$.\n\n\\bigskip\n\n\\centerline{\\begin{tabular}{|c|c|c|c|c|}\n\\hline\n\\text{Number of steps}  & 10 & 20 & 50 & 100\\\\\n\\hline\n$y_k=e_{k\\ \\text{mod}\\ d}$ & 0.5 (300) & 0.001 (0.2) & 1e-7 (1e-4) & 1e-13 (1e-11) \\\\ \n\\hline \n$y_k$ random  & 1 (500) & 0.01 (0.5) & 1e-6 (1e-3) & 1e-13 (1e-10) \\\\ \n\\hline \n\\end{tabular}}\n\n\\bigskip\n\nFor each case, we performed twenty experiments. Each entry in the previous table gives the mean value of the distance between $B_k$ and $A_*^{-1}$, with the highest value obtained in parentheses. This number can be significantly larger than the mean because of the randomness of $A_*$, which can cause it to be almost singular, leading the algorithm to behave badly as the $s_k$ are less uniformly linearly independent with this method.\n\n\\medskip\n\nThis experiment shows that taking $y_k$ random is not as efficient as taking $y_k$ to periodically be equal to the canonical basis of $\\R^d$.\n\n\\subsection{An application: constrained optimisation}\n\nConsider the following control system on $\\R^d$\n$$\n\\dot{x}=K_{x(t)}u(t),\\quad u\\in \\R^d,\n$$\nwith $K_x$ a semi-positive symmetric matrix with coefficients of class $\\mathcal{C}^2$. This corresponds to a sub-Riemannian control system where $u$ is the momentum of the trajectory and $K_x$ the co-metric at $x$ (\\cite{MBOOK}). This is a very natural formulation for problems of shape analysis, see \\cite{ATY}. \n\n\\medskip\n\nTake $C\\in M_{l,d}(\\R)$ such that the $l\\times l$ matrix \n$$\nA_x=CK_xC^T\n$$ \nis invertible for every $x\\in \\R^d$, and take an initial point $x_0\\in \\R^d$ such that $Cx_0=0$.\n\n\\medskip\n\nWe consider the optimal control problem of minimizing \n$$\nL(u)=\\frac{1}{2}\\int_0^1u(t)^TK_{x(t)}u(t)dt+g(x(1)),\\quad \\text{where}\\quad \\dot{x}=K_{x(t)}u(t),\n$$\nover all possible $u\\in L^2([0,1],\\R^d)$ such that\n$$\nCK_{x(t)}u(t)=0\\quad a.e.\\ t\\in[0,1].\n$$\nThis is the same as minimizing $L(u)$ over trajectories that stay in the sub-space $\\ker(C)$. According to the Pontryagin Maximum Principle (\\cite{ATY,TBOOK}), if $u$ is optimal for this constrained problem, then there exists $p\\in W^{1,2}([0,1],\\R^d)$ such that $p(1)+dg_{x(1}=0$, and\n\\begin{equation}\n\\label{geodeq}\n\\left\\lbrace\n\\begin{aligned}\n\\dot{x}&=K_x\\left(p-C'A_x^{-1}CK_xp\\right),\\\\\n\\dot{p}&=-\\frac{1}{2}\\left(p-C'A_x^{-1}CK_xp\\right)^TK_x\\left(p-C'A_x^{-1}CK_xp\\right).\n\\end{aligned}\\right.\n\\end{equation}\nfor almost every $t\\in[0,1]$. Moreover, \n\\begin{equation}\n\\label{geocost}\nL(u)=\\tilde{L}(p(0))=\\frac{1}{2}\\left(p(0)-C'A_{x(0)}^{-1}CK_{x(0)}p(0)\\right)^TK_{x(0)}\\left(p(0)-C'A_{x(0)}^{-1}CK_{x(0)}p(0)\\right)+g(x(1)).\n\\end{equation}\nSince \\eqref{geodeq} is an ordinary differential equation, and since the minimization of $L$ reduces to the minimization of $\\tilde{L}$ with respect to the initial momentum $p_0=p(0)$. Then, the computation the gradient of $\\tilde{L}$ requires solving an adjoint equation with coefficients depending on the derivatives of the right-hand side of \\eqref{geodeq}. This is described in more details in \\cite{ATY}.\n\n\\medskip\n\nOne of the most time-consuming aspects of this method is the computation, at each time step, of the inverse of $A_x$. Therefore, we applied the Quasi-Newton Algorithm as follows. \n\nFor any $k\\in \\mathbb{N}}\\renewcommand{\\P}{\\mathbb{P}$, define $y_k=e_{k\\ \\text{mod}\\ d}$. We start with the initial momentum $p_0=0$, and let $B_0(t)=Id_l$ for all $t\\in[0,1]$. Then, assuming we have constructed an initial momentum $p_k$ and a family of matrices $B_k(t),\\ t\\in[0,1]$, we use \\eqref{geodeq} to compute a trajectory $x_k(t)$, replacing $A_{x}^{-1}$ by $B_k{t}$. Finally, at each time $t$, we define \n$$\n\\begin{aligned}\ns_k(t)&=A_{x_k(t)}y_k,\\\\\nr_k(t)&=B_k(t)s_k(t)-y_k,\\\\\nB_{k+1}(t)&=B_k(t)+\\frac{r_k(t)r_k^T(t)}{r_k^T(t)s_k(t)}.\n\\end{aligned}\n$$\nWe can then compute the gradient of $\\tilde{L}$ with an adjoint equation, where any derivative \n$$\n\\partial_x(A_{x_k(t)})^{-1}=-A_{x_k(t)}^{-1}\\partial_xA_{x_k(t)}A_{x_k(t)}^{-1}\n$$ \nis replaced by $-B_k(t)\\partial_xA_{x_k(t)}B_k(t)$. This allows the minimization of $\\tilde{L}$ using gradient descent or a regular quasi-Newton algorithm.\n\n\\medskip\n\nAs long as the algorithm gives a converging sequence of initial momenta $p_k$, the trajectories $x_k(t)$ will also converge to a trajectory $x_*(t)$, making each $A_{x_k(t)}$, with $t\\in[0,1]$ fixed, a converging sequence, with invertible limit $A_*(t)$. Therefore, each $B_k(t)$, $t\\in[0,1]$ fixed, converges to $A_*(t)$ as $k\\rightarrow \\infty$. In other words, as $k\\rightarrow\\infty$, we are indeed computing the true gradient of $\\tilde{L}$.\n\n\\newpage\n\n\\bibliographystyle{plain}\n", 0.7752088678597158]]}